# Multi-Model Feature Selection Configuration
# Combines importance from multiple model families for robust feature ranking

# Global settings (Single Source of Truth)
global:
  # Random state seed - loaded from pipeline.determinism.base_seed at runtime
  # All model configs will automatically use this value unless explicitly overridden
  # This ensures consistency across all models and aligns with the determinism system
  # Set to null to use pipeline.determinism.base_seed (default: 42)
  # Or set to a specific value (e.g., 42, 1337) to override globally
  seed: null

# Model families configuration
model_families:
  # ============================================================================
  # TREE-BASED MODELS (fast, native importance)
  # ============================================================================
  
  lightgbm:
    enabled: true
    importance_method: "native"  # Uses gain importance
    weight: 1.0
    config:
      objective: "regression_l1"
      metric: "mae"
      boosting_type: "gbdt"
      n_estimators: 300
      learning_rate: 0.05
      num_leaves: 31
      max_depth: -1
      min_child_samples: 20
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.1
      reg_lambda: 0.1
      verbose: -1
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
      device: "cpu"  # Set to "cuda" or "gpu" if available
  
  xgboost:
    enabled: true
    importance_method: "native"  # Uses gain importance
    weight: 1.0
    config:
      objective: "reg:squarederror"
      eval_metric: "mae"
      n_estimators: 300
      learning_rate: 0.05
      max_depth: 6
      min_child_weight: 3
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.1
      reg_lambda: 0.1
      verbosity: 0
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
      tree_method: "auto"  # Set to "gpu_hist" if CUDA available
  
  random_forest:
    enabled: true
    importance_method: "native"  # Uses gini/entropy importance
    weight: 0.8  # Slightly lower weight (can be correlated with other trees)
    config:
      n_estimators: 200
      max_depth: 15
      max_features: "sqrt"
      min_samples_split: 20
      min_samples_leaf: 10
      bootstrap: true
      n_jobs: 4
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  histogram_gradient_boosting:
    enabled: false  # Disable by default (similar to LightGBM)
    importance_method: "native"
    weight: 0.9
    config:
      max_iter: 300
      max_depth: 8
      learning_rate: 0.05
      max_bins: 255
      l2_regularization: 0.0001
      early_stopping: true
      validation_fraction: 0.1
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  # ============================================================================
  # NEURAL NETWORKS (slower, permutation/SHAP importance)
  # ============================================================================
  
  neural_network:
    enabled: false
    importance_method: "permutation"  # Permutation importance
    weight: 1.2  # Higher weight (different architecture family)
    config:
      hidden_layer_sizes: [128, 64]
      activation: "relu"
      solver: "adam"
      alpha: 0.0001
      batch_size: "auto"
      learning_rate_init: 0.001
      max_iter: 300
      early_stopping: true
      validation_fraction: 0.1
      n_iter_no_change: 10
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  # ============================================================================
  # SPECIALIZED MODELS (optional, for specific use cases)
  # ============================================================================
  
  ridge:
    enabled: true  # ✅ ENABLED - Linear baseline (regression only)
    importance_method: "native"  # Uses absolute coefficients
    weight: 0.7
    config:
      alpha: 1.0
      fit_intercept: true
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  elastic_net:
    enabled: true  # ✅ ENABLED - Sparse linear model (regression only)
    importance_method: "native"
    weight: 0.8
    config:
      alpha: 1.0
      l1_ratio: 0.5
      max_iter: 1000
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  # ============================================================================
  # ADDITIONAL MODELS (Recommended for production)
  # ============================================================================
  
  catboost:
    enabled: true  # ✅ ENABLED - Diverse tree-based selection
    importance_method: "native"  # Uses PredictionValuesChange
    weight: 1.0
    config:
      iterations: 300
      learning_rate: 0.05
      depth: 6  # Keep ≤ 8 to avoid exponential complexity (2^d)
      loss_function: "RMSE"
      verbose: false
      metric_period: 50  # Calculate metrics every 50 trees (reduces evaluation overhead)
      # random_seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
      # Note: If training is slow (>20min for 50k samples), check for:
      #   - Text features (add text_features=['col_name'] if present)
      #   - High cardinality categoricals (drop ID columns like User_ID)
      #   - Depth > 8 (reduce to 6-8)
  
  lasso:
    enabled: true  # ✅ ENABLED - Explicit sparse feature selection
    importance_method: "native"  # Uses abs(coef_)
    weight: 0.9
    config:
      alpha: 0.1
      max_iter: 1000
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  mutual_information:
    enabled: false  # ✅ ENABLED - Information-theoretic baseline
    importance_method: "native"  # Direct calculation (no model)
    weight: 0.8
    config:
      discrete_features: "auto"
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  # ============================================================================
  # STATISTICAL & WRAPPER METHODS
  # ============================================================================
  
  univariate_selection:
    enabled: false # ✅ ENABLED - Statistical F-test baseline
    importance_method: "native"  # F-statistics (f_regression/f_classif)
    weight: 0.7
    config:
      # Uses f_regression for regression, f_classif for classification
      # No additional config needed
  
  rfe:
    enabled: false  # ✅ ENABLED - Recursive feature elimination
    importance_method: "native"  # Ranking-based (1/rank)
    weight: 0.8
    config:
      n_features_to_select: 50  # Number of features to select
      step: 5  # Number of features to remove per iteration
  
  boruta:
    enabled: false  # ✅ ENABLED - All-relevant feature selection (slower)
    importance_method: "native"  # Selection-based (selected/tentative/rejected)
    weight: 1.0
    config:
      max_iter: 100  # Maximum iterations
      max_time_minutes: 20  # Time budget for Boruta fit in TARGET_RANKING stage (default: 10 minutes)
      max_features_threshold: 200  # Skip Boruta if n_features > this (default: 200)
      max_samples_threshold: 200000  # Skip Boruta if n_samples > this and subsampling disabled (default: 20000)
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  stability_selection:
    enabled: false  # ✅ ENABLED - Bootstrap-based stable feature selection (slower)
    importance_method: "native"  # Fraction of times selected across bootstraps
    weight: 0.9
    config:
      n_bootstrap: 50  # Number of bootstrap iterations (reduced for speed)
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
      Cs: 10  # Number of C values to try for LogisticRegressionCV
      cv: 3  # CV folds for LassoCV/LogisticRegressionCV
      n_jobs: 1  # Parallel jobs for CV

  # ============================================================================
  # CLASSIFICATION-SPECIFIC MODELS (binary/multiclass targets only)
  # ============================================================================
  
  logistic_regression:
    enabled: true  # ✅ ENABLED - Classification baseline (binary/multiclass only)
    importance_method: "native"  # Uses abs(coef_)
    weight: 0.8
    config:
      solver: "lbfgs"
      max_iter: 1000
      C: 1.0
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)
  
  ftrl_proximal:
    enabled: false  # ✅ ENABLED - Online learning for binary classification
    importance_method: "native"  # Uses abs(coef_)
    weight: 0.7
    config:
      alpha: 0.1
      l1_ratio: 0.5
      max_iter: 1000
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)

  # ============================================================================
  # PROBABILISTIC MODELS (uncertainty estimation)
  # ============================================================================
  
  ngboost:
    enabled: true  # ✅ ENABLED - Probabilistic gradient boosting
    importance_method: "native"  # Uses feature_importances_ from base learner
    weight: 0.9
    config:
      n_estimators: 200
      learning_rate: 0.05
      minibatch_frac: 0.8
      # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)

# Aggregation strategies
aggregation:
  # How to aggregate importance across symbols (for single model family)
  per_symbol_method: "mean"  # mean, median, sum
  
  # How to combine across model families
  # cross_model_method: auto-injected from defaults (weighted_mean)
  
  # Require feature to be important in at least N models
  # require_min_models: auto-injected from defaults (2)
  
  # Consensus threshold (0-1): fraction of models that must agree
  # consensus_threshold: auto-injected from defaults (0.5)

# Sampling and performance
sampling:
  max_samples_per_symbol: 50000  # Limit rows per symbol (for speed)
  # validation_split: auto-injected from defaults (0.2)
  # seed: auto-injected from global.seed (or pipeline.determinism.base_seed)

# SHAP configuration (if using SHAP method)
shap:
  max_samples: 1000  # Max samples for SHAP calculation
  use_tree_explainer: true  # Use TreeExplainer when possible (fast)
  kernel_explainer_background: 100  # Background samples for KernelExplainer

# Permutation importance configuration
permutation:
  n_repeats: 5  # Number of permutation repeats
  seed: null  # null = inherit from pipeline.determinism.base_seed (default: 42)

# Cross-validation configuration
cross_validation:
  # folds: auto-injected from defaults (3)
  # n_jobs: auto-injected from defaults (1)

# Output configuration
output:
  # save_per_family_rankings: auto-injected from defaults (true)
  # save_agreement_matrix: auto-injected from defaults (true)
  # save_metadata: auto-injected from defaults (true)
  # include_model_scores: auto-injected from defaults (true)

# Computational budgets (for large-scale runs)
compute:
  max_symbols: null  # Limit symbols (null = all)
  max_features: null  # Limit features per model (null = all)
  parallel_symbols: false  # Process symbols in parallel (risky with GPU)
  # use_gpu: auto-injected from defaults (false)

# Presets for quick switching
presets:
  fast:
    # Quick validation on 3-5 symbols
    model_families:
      lightgbm:
        enabled: true
        config:
          n_estimators: 100
      xgboost:
        enabled: false
      random_forest:
        enabled: false
      neural_network:
        enabled: false
    sampling:
      max_samples_per_symbol: 10000
  
  balanced:
    # Default balanced approach
    model_families:
      lightgbm:
        enabled: true
      xgboost:
        enabled: true
      random_forest:
        enabled: true
      neural_network:
        enabled: true
    sampling:
      max_samples_per_symbol: 50000
  
  comprehensive:
    # Maximum robustness (slow)
    model_families:
      lightgbm:
        enabled: true
      xgboost:
        enabled: true
      random_forest:
        enabled: true
      histogram_gradient_boosting:
        enabled: true
      neural_network:
        enabled: true
      ridge:
        enabled: true
    sampling:
      max_samples_per_symbol: 100000
    aggregation:
      require_min_models: 3

# Active preset (uncomment to use)
# active_preset: "balanced"


# MLP Model Configuration
# Multi-Layer Perceptron (feedforward neural network)

model_family: "MLP"
description: "Multi-layer perceptron for tabular data"

hyperparameters:
  # Training
  epochs: 50
  batch_size: 512
  # patience: auto-injected from defaults (10)
  learning_rate: 0.001
  
  # Architecture
  hidden_layers: [256, 128, 64]
  # activation: auto-injected from defaults (relu)
  # dropout: auto-injected from defaults (0.2)
  batch_normalization: true

# Variants
variants:
  small:
    hidden_layers: [128, 64]
    dropout: 0.1
    epochs: 30
    
  medium:
    hidden_layers: [256, 128, 64]
    dropout: 0.2
    epochs: 50
    
  large:
    hidden_layers: [512, 256, 128, 64]
    dropout: 0.3
    epochs: 100


# Intelligent Training Pipeline Configuration
# This config allows running the full pipeline with minimal command-line arguments

# Data configuration
data:
  # Default data directory (can be overridden via --data-dir)
  # NOTE: Updated to use versioned labels (v2) with corrected horizon unit conversion
  # Old: data/data_labeled/interval=5m (had horizon unit bug)
  # New: data/data_labeled_v2/interval=5m (fixed: horizon_minutes correctly converted to bars)
  data_dir: "data/data_labeled_v2/interval=5m"
  
  # Default symbols (can be overridden via --symbols)
  symbols:
    - AAPL
    - MSFT
    - GOOGL
    - TSLA
    - NVDA
  
  # Data limits
  max_rows_per_symbol: null  # null = no limit, or set to integer (e.g., 50000)
  max_rows_train: null  # null = no limit for training
  max_cs_samples: 1000  # Max cross-sectional samples per timestamp
  min_cs: 10  # Minimum cross-sectional samples per timestamp

# Target selection
targets:
  # Automatically rank and select targets
  auto_targets: true
  
  # Number of top targets to select (if auto_targets=true)
  top_n_targets: 10
  
  # Limit number of targets to evaluate during ranking (for faster testing)
  # null = evaluate all targets
  max_targets_to_evaluate: null
  
  # Manual target list (overrides auto_targets if provided)
  # Leave empty [] to use auto_targets, or specify: ["fwd_ret_5m", "fwd_ret_15m"]
  manual_targets: []

# Feature selection
features:
  # Automatically select features per target
  auto_features: true
  
  # Number of top features to select per target (if auto_features=true)
  top_m_features: 100
  
  # Manual feature list (overrides auto_features if provided)

# Target-conditional feature exclusions
# Generates per-target exclusion lists based on target horizon and semantics
# This implements "Target-Conditional Feature Selection" - tailoring features to target physics
target_conditional_exclusions:
  enabled: true  # Enable target-conditional exclusions
  horizon_safety_multiplier: 4.0  # Exclude features with lookback > horizon * multiplier (e.g., 60m target -> exclude >240m features)
  enable_semantic_rules: true  # Enable semantic rules (e.g., exclude repainting indicators for peak/valley targets)
  # Exclusion lists are saved to: RESULTS/{cohort}/{run}/feature_exclusions/{target}_exclusions.yaml

# Model families to train
# Options: lightgbm, xgboost, random_forest, catboost, neural_network, 
#          lasso, mutual_information, univariate_selection, etc.
model_families:
  - lightgbm
  - xgboost
  - random_forest
  - catboost
  - neural_network
  - lasso
  - mutual_information
  - univariate_selection

# Training strategy
strategy: "single_task"  # Options: "single_task", "multi_task", "multi_horizon_bundle", "ensemble"

# Strategy-specific configurations
strategy_configs:
  multi_task:
    shared_dim: 128
    head_dims: {}
    loss_weights: {}
    batch_size: 32
    learning_rate: 0.001
    n_epochs: 100
  
  cascade:
    gate_threshold: 0.5
    # Probability threshold for gating decisions
    calibration_method: "isotonic"  # "isotonic" or "platt"
    gating_rules:
      will_peak_5m:
        action: "reduce"  # "reduce", "boost", or "hold"
        factor: 0.5  # Multiplier for action
      will_valley_5m:
        action: "boost"
        factor: 1.2

  # Multi-horizon bundle training configuration
  # Trains shared encoder + per-horizon heads for related targets across horizons
  # Example: fwd_ret_5m, fwd_ret_15m, fwd_ret_60m grouped into one bundle
  multi_horizon_bundle:
    enabled: false  # Enable multi-horizon bundle training
    auto_discover: true  # Auto-discover bundles from target names

    # Bundle discovery settings
    min_horizons: 2  # Minimum horizons required to form a bundle
    max_horizons: 5  # Maximum horizons per bundle
    min_diversity: 0.3  # Minimum diversity score (1 - mean correlation)
    top_n_bundles: 3  # Number of top bundles to train

    # Ranking weights
    diversity_weight: 0.3  # Weight for diversity in bundle ranking
    predictability_weight: 0.7  # Weight for predictability in bundle ranking

    # Model architecture (TensorFlow multi-head)
    shared_layers: [256, 128]  # Shared encoder layer sizes
    head_layers: [64]  # Per-horizon head layer sizes
    dropout: 0.2  # Dropout rate
    batch_norm: true  # Use batch normalization

    # Training settings
    backend: "tensorflow"  # Only tensorflow supported currently
    epochs: 100  # Max training epochs
    batch_size: 256  # Training batch size
    patience: 10  # Early stopping patience
    lr: 0.001  # Learning rate

    # Loss weighting strategy
    # Options: "equal" (all targets equal), "horizon_decay" (decay from primary horizon)
    loss_weighting: "equal"
    horizon_decay_half_life_minutes: 30  # For horizon_decay weighting

# Output configuration
output:
  # Default output directory (can be overridden via --output-dir)
  output_dir: "intelligent_output"
  
  # Cache directory (default: output_dir/cache)
  # null = auto-generate from output_dir
  cache_dir: null

# Cache control
cache:
  # Use cache for rankings/selections
  use_cache: true
  
  # Force refresh of cached rankings/selections
  force_refresh: false
  
  # Never refresh cache (use existing only)
  no_refresh_cache: false

# Advanced options
advanced:
  # Run leakage diagnostics after feature selection
  run_leakage_diagnostics: false
  
  # Dual-view target ranking (CROSS_SECTIONAL + SYMBOL_SPECIFIC)
  enable_dual_view_ranking: true
  
  # Enable LOSO (Leave-One-Symbol-Out) evaluation (optional, high value)
  enable_loso: false

# Decision application (regression/trend-based auto-config)
decisions:
  # Application mode: "off" (assist mode only), "dry_run" (show patch), "apply" (auto-apply)
  apply_mode: "off"  # Options: "off", "dry_run", "apply"

  # Minimum decision level to apply (0=no action, 1=warning, 2=recommendation, 3=action)
  min_level_to_apply: 2
  
  # Bayesian patch policy (Thompson sampling over discrete patch templates)
  use_bayesian: false  # Enable Bayesian learning from past outcomes
  bayesian:
    # Minimum runs before recommending patches
    min_runs_for_learning: 5
    # Minimum P(improve) to auto-apply
    p_improve_threshold: 0.8
    # Minimum expected gain to recommend
    min_expected_gain: 0.01
    # Reward metric to optimize
    reward_metric: "cs_auc"
    # Recency decay factor (higher = more weight on recent runs)
    recency_decay: 0.95
    # Decision level thresholds (all config-driven, no hardcoded values)
    level_3_threshold: 0.8   # P(improve) for auto-apply (level 3)
    level_3_gain: 0.01        # Expected gain for auto-apply (level 3)
    level_2_threshold: 0.6    # P(improve) for recommend (level 2)
    level_2_gain: 0.005       # Expected gain for recommend (level 2)
    level_1_threshold: 0.4    # P(improve) for warning (level 1)
    # Baseline window size (number of recent runs for baseline computation)
    baseline_window: 10
    # Patch templates (optional - uses defaults if not specified)
    # templates:
    #   - name: "cap_features_10pct"
    #     action_code: "cap_features"
    #     reduction_pct: 10
    #     description: "Reduce max_features by 10%"

# Training configuration
training:
  # Cross-validation settings
  folds: 3  # Number of CV folds (default: 3)
  cv_n_jobs: 1  # Parallel jobs for CV (1 = sequential, -1 = all cores). Set to 1 for GPU training to avoid outer parallelism conflicts
  
  # Degenerate folds policy (for rare/imbalanced targets)
  cv_degenerate_fallback: "reduce_folds"  # Options: "reduce_folds" | "skip_cv" | "different_splitter"
  cv_mifolds: 2  # Minimum folds before skipping CV (if reduce_folds policy)
  
  # CatBoost-specific settings
  catboost:
    metric_period: 50  # Calculate metrics every N trees (reduces evaluation overhead). Default: 50

# Test mode configuration (auto-detected if 'test' in output_dir name)
test:
  # Override settings for test runs
  max_targets_to_evaluate: 23  # Limit targets for faster testing
  top_n_targets: 23
  top_m_features: 50

# Safety and Numerical Stability Configuration
# Settings for numerical guards, clipping, and safety thresholds

safety:
  # Output Layout Configuration (view+universe scoped paths)
  # Controls strict validation of output paths to prevent scope violations
  output_layout:
    # If true, hard error when cohort metadata is missing required fields (view, universe_sig, target)
    # If false, warn and fall back to legacy path construction
    # Default: true (enforce scope invariants)
    strict_scope_partitioning: true

  # Feature Clipping
  feature_clipping:
    enabled: true
    clip_value: 1000.0  # Clip features to [-1000, 1000]
    log_clipped: false  # Log when features are clipped
  
  # Target Capping
  target_capping:
    enabled: true
    cap_sigma: 15.0  # Cap targets using 15 MAD (median absolute deviation)
    log_capped: false  # Log when targets are capped
  
  # Numerical Stability
  numerical:
    safe_exp_bounds:
      lo: -40.0  # Lower bound for safe exponential
      hi: 40.0  # Upper bound for safe exponential
    
    numpy_error_handling:
      over: "warn"  # Overflow handling: "warn", "raise", "ignore"
      invalid: "warn"  # Invalid value handling
      divide: "warn"  # Division by zero handling
      under: "ignore"  # Underflow handling
  
  # Gradient Clipping
  gradient_clipping:
    enabled: true
    clipnorm: 1.0  # Gradient norm clipping (TensorFlow/Keras)
    max_norm: 1.0  # Maximum gradient norm (PyTorch)
  
  # Model Output Validation
  validation:
    check_predictions: true  # Check predictions for NaN/Inf
    check_gradients: false  # Check gradients for NaN/Inf (expensive)
    log_anomalies: true  # Log when anomalies are detected
  
  # Safety Guards
  guards:
    min_history_bars: 120  # Minimum history bars before feature use
    max_na_fraction: 0.05  # Maximum NaN fraction (5%)
    hold_on_nan: true  # Return HOLD action on NaN in predictions
  
  # Leakage Detection & Auto-Fix Thresholds
  leakage_detection:
    # Logging mode for lookback computation
    # - "summary": One-line summaries per stage (default, less verbose)
    # - "debug": Per-feature inference traces, pattern matches, recompute details (verbose)
    log_mode: "summary"  # "summary" | "debug"
    
    # Leakage policy: how to handle purge/embargo violations
    # - "strict": Hard-stop on violations (raise exception, block training)
    # - "drop": Drop violating features, recompute budget, ensure pass (default for dev/testing)
    # - "warn": Log violations but continue (NOT recommended for production)
    policy: "drop"  # Default: drop (auto-remove violating features for dev/testing)
    # Set to "strict" for production (fail-fast on violations)
    
    # Over-budget action: what to do when features exceed the lookback budget
    # - "drop": Gatekeeper removes violating features (default for dev/testing)
    # - "hard_stop": Fail the run if any violating feature exists (recommended for prod)
    # - "warn": Allow but log violations (useful only for debugging)
    over_budget_action: "drop"  # Default: drop (auto-remove violating features)
    
    # Lookback budget computation
    # New structured format (recommended):
    lookback_budget:
      mode: auto  # "auto" | "fixed"
      auto_rule: k_times_horizon  # Only rule for now
      k: 10.0  # Multiplier for k_times_horizon (cap = k * horizon)
      min_minutes: 240.0  # Floor for auto rules
      max_minutes: 28800.0  # Optional maximum cap (null to disable, 28800 = 20 days)
    
    # Legacy format (backward compatible, but deprecated):
    # lookback_budget_minutes: "auto"  # "auto" or number (e.g., 240)
    # NOTE: If both formats exist, new format takes precedence
    
    # Buffer added to lookback budget for safety margin
    lookback_buffer_minutes: 5.0  # Default: 5 minutes safety buffer
    
    # Auto-fixer triggers when scores exceed these thresholds
    auto_fix_thresholds:
      cv_score: 0.99  # Cross-validation score threshold (0.99 = 99%)
      training_accuracy: 0.999  # Training accuracy threshold (0.999 = 99.9%)
      training_r2: 0.999  # Training R² threshold for regression (0.999 = 99.9%)
      perfect_correlation: 0.999  # Perfect correlation threshold (0.999 = 99.9%)
    
    # Minimum confidence for auto-fixer to apply fixes (0.0 to 1.0)
    auto_fix_min_confidence: 0.8  # Only auto-fix leaks with >= 80% confidence
    
    # Maximum number of features to auto-fix per run (prevents overly aggressive fixes)
    auto_fix_max_features_per_run: 20  # Limit auto-fixes to top N most confident detections
    
    # Enable/disable auto-fixer
    auto_fix_enabled: true  # Set to false to disable automatic leakage fixing
    
    # Auto-rerun after fixing leaks
    auto_rerun:
      enabled: true  # Enable automatic rerun of targets after auto-fix
      max_reruns: 3  # Maximum number of reruns per target (default: 3)
      rerun_on_perfect_train_acc: true  # Rerun if perfect training accuracy detected
      rerun_on_high_auc_only: false  # Rerun on high AUC alone (default: false, only rerun on perfect train acc)
    
    # Pre-training leak scan thresholds
    pre_scan:
      min_match: 0.999  # Minimum match ratio for binary classification (99.9%)
      min_corr: 0.999  # Minimum correlation for regression (99.9%)
      min_valid_pairs: 10  # Minimum valid pairs needed for correlation check
    
    # Auto-enable features with empty allowed_horizons via family defaults (self-healing of configuration drift)
    # When true: Treats allowed_horizons=[] as None for eligible features (enables inheritance)
    # When false: allowed_horizons=[] remains explicitly disabled (default, safe)
    auto_enable_family_features_with_empty_allowed_horizons: false  # Default: false (opt-in)
    
    # Thresholds for warning when too many features are auto-enabled (suggests registry cleanup needed)
    # If auto-enabled count exceeds either threshold, emit WARNING suggesting registry cleanup
    # Policy: warn if enabled_count > min(threshold_count, ceil(total * percent/100))
    auto_enable_family_features_threshold_count: 50  # Absolute count threshold
    auto_enable_family_features_threshold_percent: 5.0  # Percentage of total registry (5%)
    
    # Feature count requirements for ranking
    ranking:
      min_features_required: 2  # Minimum features needed after filtering (for ranking)
      min_features_for_model: 3  # Minimum features needed for meaningful model training
      min_features_after_leak_removal: 2  # Minimum features after removing leaky features
      # Maximum feature lookback cap for ranking mode (prevents 1440m purge/embargo inflation)
      # Set to null to disable cap (use actual feature lookback)
      # Set to number (e.g., 240) to cap lookback at 4 hours (excludes 1D+ lookback features)
      ranking_mode_max_lookback_minutes: null  # null = no cap, or set to e.g., 240 for 4-hour cap
    
    # Small-panel leniency (for runs with few symbols)
    # When n_symbols < min_symbols_threshold, downgrade leakage BLOCKED to SUSPECT
    # This allows dominance quarantine to attempt recovery before blocking target/view
    small_panel:
      enabled: true
      min_symbols_threshold: 10  # Apply leniency when n_symbols < this
      downgrade_block_to_suspect: true  # Downgrade BLOCKED to SUSPECT for small panels
      log_warning: true  # Log when leniency is applied
    
    # Feature Importance Stability Tracking
    feature_importance:
      # Automatically analyze stability after saving snapshots
      auto_analyze_stability: true  # Set to false to disable automatic analysis
      
      # Stability thresholds for warnings
      stability_thresholds:
        min_top_k_overlap: 0.7  # Warn if top-K overlap < 0.7 (Jaccard similarity)
        min_kendall_tau: 0.6     # Warn if Kendall tau < 0.6 (rank correlation)
        top_k: 20                # Number of top features to analyze (default: 20)
        min_snapshots: 2         # Minimum snapshots required for analysis (default: 2)
      
      # Expensive Importance Computation Policy (SST)
      # Controls when to skip expensive PredictionValuesChange and use fallback
      importance_max_wall_minutes: 30  # Maximum wall time for expensive importance computation (timeout)
      overfit_train_acc_threshold: 0.99  # Skip expensive importance if train_acc >= this (default: 0.99 = 99%)
      overfit_train_val_gap_threshold: 0.20  # Skip expensive importance if (train_acc - cv_acc) >= this (default: 0.20)
      importance_fallback: "gain"  # Fallback method when skipping: "gain" | "split" | "none"
      importance_skip_on_overfit: true  # Enable skipping expensive importance on overfitting detection
      pvc_feature_count_cap: 250  # Optional: skip PVC if n_features > this (null to disable)
    
    # Dominance Quarantine: Auto-suspect → confirm → quarantine workflow
    # Detects features with dominant importance (potential leakage), confirms via rerun,
    # and only escalates to blocking target/view if leakage persists after quarantine
    dominance_quarantine:
      enabled: true
      
      # Suspect trigger (cheap heuristic)
      soft:
        top1_share: 0.30          # 30%+ of total importance
        top1_over_top2: 3.0        # 3× the next feature
        hard_top1_share: 0.40      # immediate suspect if 40%+
        max_features: 3            # quarantine top-N suspects at once
      
      # Confirm logic (rerun once with suspects removed)
      confirm:
        enabled: true
        rerun_once: true
        min_samples: 500           # avoid "confirmed" on tiny sets
        min_symbols: 3             # avoid "confirmed" on SINGLE_SYMBOL_TS
        mean_score_drop_abs: 0.15  # confirmed if removing suspects drops mean_score by ≥ 0.15
        mean_score_drop_rel: 0.25  # OR relative drop ≥ 25%
      
      # Escalation policy
      escalation:
        on_confirmed: "quarantine_feature"      # persist feature exclusion
        on_persisting_leak: "block_target_view"  # only if still leaks after confirm
  
  # Purge/embargo configuration
  temporal:
    purge_include_feature_lookback: true  # If true, purge = max(horizon+buffer, feature_lookback_max * 1.01)
    # If false, purge = horizon+buffer only (assumes features are strictly causal, only use past data)
    # When enabled, automatically increases purge to cover longest feature lookback window + 1% safety buffer
    default_purge_minutes: 85.0  # Default purge if horizon cannot be determined (SST: Single Source of Truth)
  
  # Cross-validation purge/embargo settings
  cv:
    # Purge minutes: "auto" (compute from feature lookback + buffer) or explicit number
    purge_minutes: "auto"  # Default: auto (compute from actual feature set)
    
    # Embargo minutes: "auto" (compute from horizon + extra bars) or explicit number
    embargo_minutes: "auto"  # Default: auto (compute from horizon)
    
    # Extra bars added to horizon for embargo safety margin
    embargo_extra_bars: 5  # Default: 5 bars (e.g., 25 minutes for 5m bars)
    # NUCLEAR TEST COMPLETE: Tested with 1500.0 (24h purge) - score dropped from 0.99 to 0.763
    # This confirms feature leak is fixed, but 0.763 is still suspicious (target repainting likely)
    # Reverted to 85.0 for normal operation - Final Gatekeeper handles feature leaks autonomously
    # Set purge_include_feature_lookback to false if feature lookback is killing sample efficiency 
    # and you're confident features are strictly causal (only use past data)
  
  # Active Sanitization (Ghost Buster)
  # Automatically quarantines features with excessive lookback before training starts
  # This prevents "ghost feature" discrepancies where audit and auto-fix see different lookback values
  active_sanitization:
    enabled: true  # Enable active sanitization (automatically quarantine problematic features)
    mode: budget_cap  # budget_cap | purge_allowance | fixed
    # - budget_cap: Use lookback_budget_minutes cap (if set) as threshold
    # - purge_allowance: Use purge-derived allowance (purge - buffer) as threshold (requires purge context)
    # - fixed: Use fixed_threshold_minutes as threshold (legacy behavior)
    fixed_threshold_minutes: 240.0  # Only used if mode=fixed (default: 4 hours)
    # Features with lookback > threshold will be automatically quarantined
    
    # Pattern-based quarantine (more aggressive - quarantines by naming patterns)
    pattern_quarantine:
      enabled: false  # Default: disabled (more aggressive, use with caution)
      patterns: []  # List of regex patterns to match (e.g., [".*_1d$", ".*daily.*"])
      # If enabled, features matching these patterns will be quarantined regardless of computed lookback
    
    # Look-ahead bias fixes (new section)
    # These fixes address implementation bugs that cause look-ahead bias:
    # 1. Rolling windows including current bar (should exclude with shift(1))
    # 2. Global normalization before train/test split (should normalize inside CV)
    # 3. pct_change() potentially including current bar (needs verification)
    lookahead_bias_fixes:
      # Fix #1: Exclude current bar from rolling windows
      # When enabled, adds .shift(1) before rolling operations (rolling_mean, rolling_std, etc.)
      # This prevents features from including the current bar's price, which leaks current information
      # Default: false (maintains current behavior for backward compatibility)
      exclude_current_bar_from_rolling: false
      
      # Fix #2: Normalize inside CV loops (fit on train, transform test)
      # When enabled, moves scaler/imputer fitting inside CV loops
      # This prevents leaking future statistics into training (global normalization leak)
      # Default: false (maintains current behavior for backward compatibility)
      # NOTE: This fix requires refactoring call sites to pass train/test separately
      normalize_inside_cv: false
      
      # Fix #3: Verify pct_change excludes current bar
      # When enabled, uses explicit shift for pct_change calculations to ensure current bar is excluded
      # Default: false (maintains current behavior for backward compatibility)
      verify_pct_change_shift: false
      
      # Migration mode: controls how fixes are applied and validated
      # - "off": All fixes disabled (current behavior, default)
      # - "test": Fixes enabled, log differences but don't fail (for validation)
      # - "warn": Fixes enabled, warn on discrepancies (for gradual rollout)
      # - "enforce": Fixes enabled, fail on discrepancies (production mode)
      migration_mode: "off"  # Default: off (maintains current behavior)
  
  # Reproducibility Tracking Thresholds
  reproducibility:
    enabled: true
    # Cohort-aware reproducibility: organize runs by sample size, symbols, date range, config
    # DEFAULT: true (cohort-aware mode is the default and recommended)
    # Set to false only if you need the legacy flat-file structure
    cohort_aware: true  # Enable cohort-aware tracking (organizes by cohort, only compares within cohorts)
    n_ratio_threshold: 0.90  # Min ratio (min/max) for comparability (0.90 = 90% overlap required)
    cohort_config_keys:      # Keys to include in cohort hash (identifies different cohorts)
      - min_cs
      - max_cs_samples
      - leakage_filter_version
      - universe_sig
    # Tolerance bands: STABLE (within noise) / DRIFTING (small changes) / DIVERGED (real issues)
    thresholds:
      # ROC-AUC / R² / primary score thresholds
      roc_auc:
        abs: 0.005      # 0.5 AUC points absolute difference (effect size threshold)
        rel: 0.02       # 2% relative difference (effect size threshold)
        z_score: 2.0    # z-score threshold for DIVERGED (requires ~95% confidence, i.e., statistically significant)
      # Composite score thresholds
      composite:
        abs: 0.02       # 0.02 in composite space (effect size threshold)
        rel: 0.05       # 5% relative difference (effect size threshold)
        z_score: 2.0    # z-score threshold for DIVERGED (requires ~95% confidence)
      # Importance thresholds
      importance:
        abs: 0.05       # 0.05 absolute difference (effect size threshold)
        rel: 0.20       # 20% relative difference (importance is more variable)
        z_score: 2.5    # z-score threshold for DIVERGED (importance is more variable, so require higher confidence)
    # Classification rules: differences must be within BOTH abs AND rel thresholds for STABLE
    # DRIFTING: within 2x thresholds, DIVERGED: exceeds 2x thresholds
    use_z_score: true   # If true, use z-score (|Δ| / σ) when std_score is available
  
  # Leakage warning thresholds (for detect_leakage function)
    warning_thresholds:
      classification:
        high: 0.90  # ROC-AUC/Accuracy > 0.90 is suspicious
        very_high: 0.95  # ROC-AUC/Accuracy > 0.95 is extremely suspicious
      regression:
        forward_return:
          high: 0.50  # R² > 0.50 is suspicious for forward returns
          very_high: 0.60  # R² > 0.60 is extremely suspicious
        barrier:
          high: 0.70  # R² > 0.70 is suspicious for barrier targets
          very_high: 0.80  # R² > 0.80 is extremely suspicious
    
    # Model-specific leakage alert thresholds
    model_alerts:
      suspicious_score: 0.99  # Score >= 0.99 triggers leakage alert (for LightGBM, RF, etc.)
    
    # Importance thresholds
    importance:
      single_feature_threshold: 0.50  # Flag if single feature has >50% importance
      high_importance_threshold: 0.30  # Threshold for high importance detection
    
    # Model evaluation thresholds
    model_evaluation:
      # Binary classification threshold
      binary_classification_threshold: 0.5  # Probability threshold for binary classification
      # Feature count threshold for pruning (only prune if feature count exceeds this)
      feature_count_pruning_threshold: 100
      # Epsilon for threshold comparisons (prevents false positives from rounding)
      comparison_epsilon: 1e-6
      # Consistency check thresholds
      composite_score_high_threshold: 0.5  # High composite score threshold
      regression_score_low_threshold: 0.2  # Low score threshold for regression
      classification_score_low_threshold: 0.6  # Low score threshold for classification
      importance_high_threshold: 0.7  # High importance threshold
      regression_score_very_low_threshold: 0.1  # Very low score threshold for regression
      classification_score_very_low_threshold: 0.5  # Very low score threshold for classification
      # NOTE: feature_count_pruning_threshold defined at line 331 (avoid duplicate key)
      # Interval detection tolerance
      interval_detection_tolerance: 0.2  # 20% tolerance for interval detection
    
    # Leakage Sentinel Thresholds
    leakage_sentinels:
      shifted_target_threshold: 0.5  # If model score > this on shifted target, flag as leaky
      symbol_holdout_train_threshold: 0.9  # If train score > this, flag for investigation
      symbol_holdout_test_threshold: 0.3  # If test score < this (with high train), flag as leaky
      randomized_time_threshold: 0.5  # If model score > this on time-shuffled data, flag as leaky
    
    # Auto-Fixer Settings
    auto_fixer:
      perfect_score_threshold: 0.99  # Train score >= this indicates leakage
      min_confidence: 0.7  # Minimum confidence to auto-fix (0.0 to 1.0)
      max_backups_per_target: 20  # Maximum backups to keep per target
      symbol_holdout_test_size: 0.2  # Test size for symbol holdout split
    
  # Metrics System (Cube-based with explicit dimensions and rollups)
  metrics:
    enabled: true  # Enable metrics collection
    
    # Granularity levels (what dimensions to track)
    levels:
      run: true              # Per-run aggregates
      view: true             # Per-view aggregates (CROSS_SECTIONAL, SYMBOL_SPECIFIC)
      target: true            # Per-target aggregates
      symbol: true            # Per-symbol aggregates
      target_symbol: false   # Per-target-per-symbol (expensive, opt-in)
    
    # Baseline configuration (for drift comparisons)
    baselines:
      previous_run: true      # Compare to previous run
      rolling_window_k: 10    # Rolling window size for baseline (k runs)
      last_good_run: true     # Compare to last "good" run (if marked)
    
    # Drift detection configuration
    drift:
      feature_set_mode: "topk_importance"  # "topk_importance" | "fixed_list"
      topk: 50                            # Top-K features to track for drift (if mode=topk_importance)
      psi_threshold: 0.2                  # Population Stability Index threshold
      ks_threshold: 0.1                   # Kolmogorov-Smirnov test threshold
    
    # Rollup configuration (which aggregations to generate)
    rollups:
      per_symbol: true           # Per-symbol rollups (groupby run_id, view, symbol)
      per_cross_sectional: true   # Per-cross-sectional rollups (groupby run_id, view, universe_sig)
      per_target: true            # Per-target rollups (groupby run_id, target, view)
      per_run: true               # Per-run rollups (groupby run_id)


# Intelligent Training Pipeline Configuration
# This config allows running the full pipeline with minimal command-line arguments

# Data configuration
data:
  # Default data directory (can be overridden via --data-dir)
  # NOTE: Updated to use versioned labels (v2) with corrected horizon unit conversion
  # Old: data/data_labeled/interval=5m (had horizon unit bug)
  # New: data/data_labeled_v2/interval=5m (fixed: horizon_minutes correctly converted to bars)
  data_dir: "data/data_labeled_v2/interval=5m"
  
  # Default symbols (can be overridden via --symbols)
  symbols:
    - AAPL
    - MSFT
    - GOOGL
    - TSLA
    - NVDA
  
  # Data limits
  max_rows_per_symbol: null  # null = no limit, or set to integer (e.g., 50000)
  max_rows_train: null  # null = no limit for training
  max_cs_samples: 1000  # Max cross-sectional samples per timestamp
  min_cs: 10  # Minimum cross-sectional samples per timestamp

# Target selection
targets:
  # Automatically rank and select targets
  auto_targets: true
  
  # Number of top targets to select (if auto_targets=true)
  top_n_targets: 10
  
  # Limit number of targets to evaluate during ranking (for faster testing)
  # null = evaluate all targets
  max_targets_to_evaluate: null
  
  # Manual target list (overrides auto_targets if provided)
  # Leave empty [] to use auto_targets, or specify: ["fwd_ret_5m", "fwd_ret_15m"]
  manual_targets: []

# Feature selection
features:
  # Automatically select features per target
  auto_features: true
  
  # Number of top features to select per target (if auto_features=true)
  top_m_features: 50
  
  # Manual feature list (overrides auto_features if provided)

# Target-conditional feature exclusions
# Generates per-target exclusion lists based on target horizon and semantics
# This implements "Target-Conditional Feature Selection" - tailoring features to target physics
target_conditional_exclusions:
  enabled: true  # Enable target-conditional exclusions
  horizon_safety_multiplier: 4.0  # Exclude features with lookback > horizon * multiplier (e.g., 60m target -> exclude >240m features)
  enable_semantic_rules: true  # Enable semantic rules (e.g., exclude repainting indicators for peak/valley targets)
  # Exclusion lists are saved to: RESULTS/{cohort}/{run}/feature_exclusions/{target}_exclusions.yaml

# Model families to train
# Options: lightgbm, xgboost, random_forest, catboost, neural_network, 
#          lasso, mutual_information, univariate_selection, etc.
model_families:
  - lightgbm
  - xgboost
  - random_forest
  - catboost
  - neural_network
  - lasso
  - mutual_information
  - univariate_selection

# Training strategy
strategy: "single_task"  # Options: "single_task", "multi_task", "ensemble"

# Output configuration
output:
  # Default output directory (can be overridden via --output-dir)
  output_dir: "intelligent_output"
  
  # Cache directory (default: output_dir/cache)
  # null = auto-generate from output_dir
  cache_dir: null

# Cache control
cache:
  # Use cache for rankings/selections
  use_cache: true
  
  # Force refresh of cached rankings/selections
  force_refresh: false
  
  # Never refresh cache (use existing only)
  no_refresh_cache: false

# Advanced options
advanced:
  # Run leakage diagnostics after feature selection
  run_leakage_diagnostics: false
  
  # Dual-view target ranking (CROSS_SECTIONAL + SYMBOL_SPECIFIC)
  enable_dual_view_ranking: true
  
  # Enable LOSO (Leave-One-Symbol-Out) evaluation (optional, high value)
  enable_loso: false

# Decision application (regression/trend-based auto-config)
decisions:
  # Application mode: "off" (assist mode only), "dry_run" (show patch), "apply" (auto-apply)
  apply_mode: "off"  # Options: "off", "dry_run", "apply"

  # Minimum decision level to apply (0=no action, 1=warning, 2=recommendation, 3=action)
  min_level_to_apply: 2
  
  # Bayesian patch policy (Thompson sampling over discrete patch templates)
  use_bayesian: false  # Enable Bayesian learning from past outcomes
  bayesian:
    # Minimum runs before recommending patches
    min_runs_for_learning: 5
    # Minimum P(improve) to auto-apply
    p_improve_threshold: 0.8
    # Minimum expected gain to recommend
    min_expected_gain: 0.01
    # Reward metric to optimize
    reward_metric: "cs_auc"
    # Recency decay factor (higher = more weight on recent runs)
    recency_decay: 0.95
    # Decision level thresholds (all config-driven, no hardcoded values)
    level_3_threshold: 0.8   # P(improve) for auto-apply (level 3)
    level_3_gain: 0.01        # Expected gain for auto-apply (level 3)
    level_2_threshold: 0.6    # P(improve) for recommend (level 2)
    level_2_gain: 0.005       # Expected gain for recommend (level 2)
    level_1_threshold: 0.4    # P(improve) for warning (level 1)
    # Baseline window size (number of recent runs for baseline computation)
    baseline_window: 10
    # Patch templates (optional - uses defaults if not specified)
    # templates:
    #   - name: "cap_features_10pct"
    #     action_code: "cap_features"
    #     reduction_pct: 10
    #     description: "Reduce max_features by 10%"

# Training configuration
training:
  # Cross-validation settings
  folds: 3  # Number of CV folds (SST: canonical name, matches defaults.yaml and intelligent.yaml)
  cv_n_jobs: 1  # Parallel jobs for CV (1 = sequential, -1 = all cores). Set to 1 for GPU training to avoid outer parallelism conflicts
  
  # CatBoost-specific settings
  catboost:
    metric_period: 50  # Calculate metrics every N trees (reduces evaluation overhead). Default: 50

# Test mode configuration (auto-detected if 'test' in output_dir name)
test:
  # Override settings for test runs
  max_targets_to_evaluate: 10  # Limit targets for faster testing
  top_n_targets: 5
  top_m_features: 50

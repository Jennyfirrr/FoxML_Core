# SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial
# Copyright (c) 2025-2026 Fox ML Infrastructure LLC

"""
Single Task Strategy

Option A: Train separate models for each target.
Best for: Tree models, when targets have different scales, avoiding leakage.
"""


import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
import logging
from .base import BaseTrainingStrategy

# Import determinism system for consistent seed
try:
    from TRAINING.common.determinism import BASE_SEED
    _DETERMINISM_AVAILABLE = True
except ImportError:
    _DETERMINISM_AVAILABLE = False
    BASE_SEED = 42

# DETERMINISM_CRITICAL: Target iteration order must be deterministic
from TRAINING.common.utils.determinism_ordering import sorted_items

logger = logging.getLogger(__name__)

class SingleTaskStrategy(BaseTrainingStrategy):
    """Train separate models for each target"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.target_types = {}
        
    def train(self, X: np.ndarray, y_dict: Dict[str, np.ndarray], 
              feature_names: List[str], **kwargs) -> Dict[str, Any]:
        """Train separate models for each target"""
        logger.info("ðŸŽ¯ Training single-task models (Option A)")
        
        self.validate_data(X, y_dict)
        
        results = {}

        # DETERMINISM: Use sorted_items for deterministic target iteration order
        for target, y in sorted_items(y_dict):
            logger.info(f"Training model for target: {target}")
            
            # Determine model type based on target
            target_type = self._determine_target_type(target, y)
            self.target_types[target] = target_type
            
            # Create appropriate model
            model = self._create_model(target_type, target)
            
            # Train model with early stopping (if supported)
            try:
                # Check if model supports early stopping
                if hasattr(model, 'fit') and ('eval_set' in model.fit.__code__.co_varnames or 
                                              'X_valid' in model.fit.__code__.co_varnames):
                    # Split for validation
                    from sklearn.model_selection import train_test_split
                    # Load test_size and seed from config
                    try:
                        from CONFIG.config_loader import get_cfg
                        from TRAINING.common.determinism import BASE_SEED
                        test_size = float(get_cfg("preprocessing.validation.test_size", default=0.2, config_name="preprocessing_config"))
                        split_seed = BASE_SEED if BASE_SEED is not None else 42  # FALLBACK_DEFAULT_OK
                    except Exception:
                        test_size = 0.2  # FALLBACK_DEFAULT_OK
                        split_seed = 42  # FALLBACK_DEFAULT_OK
                    X_train, X_val, y_train, y_val = train_test_split(
                        X, y, test_size=test_size, random_state=split_seed
                    )
                    
                    # Fit with early stopping
                    if 'eval_set' in model.fit.__code__.co_varnames:
                        # LightGBM/XGBoost style
                        import lightgbm as lgb
                        early_stopping_rounds = self.config.get('early_stopping_rounds', 50)
                        model.fit(
                            X_train, y_train,
                            eval_set=[(X_val, y_val)],
                            callbacks=[lgb.early_stopping(early_stopping_rounds, verbose=False)]
                        )
                        logger.info(f"âœ… {target}: Early stopping at iteration {getattr(model, 'best_iteration_', 'N/A')}")
                    else:
                        # Fallback
                        model.fit(X, y)
                else:
                    # Standard fit for models that don't support early stopping
                    model.fit(X, y)
            except Exception as e:
                logger.warning(f"Early stopping failed for {target}, using standard fit: {e}")
                model.fit(X, y)
            
            # Store model and metadata
            self.models[target] = model
            results[target] = {
                'model': model,
                'target_type': target_type,
                'feature_names': feature_names,
                'n_features': X.shape[1],
                'n_samples': len(y),
                'feature_importance': self._get_feature_importance(model)
            }
            
            logger.info(f"âœ… {target} ({target_type}) trained successfully")
            
        return results
    
    def predict(self, X: np.ndarray, **kwargs) -> Dict[str, np.ndarray]:
        """Make predictions using trained models"""
        predictions = {}

        # DETERMINISM: Use sorted_items for deterministic target iteration order
        for target, model in sorted_items(self.models):
            try:
                if self.target_types[target] == 'classification':
                    # For classification, return probabilities
                    if hasattr(model, 'predict_proba'):
                        pred = model.predict_proba(X)[:, 1]  # Probability of positive class
                    else:
                        pred = model.predict(X)
                else:
                    # For regression, return continuous values
                    pred = model.predict(X)
                
                predictions[target] = pred
                
            except Exception as e:
                logger.error(f"Error predicting {target}: {e}")
                predictions[target] = np.zeros(len(X))
        
        return predictions
    
    def get_target_types(self) -> Dict[str, str]:
        """Return target types for each target"""
        return self.target_types.copy()
    
    def _determine_target_type(self, target: str, y: np.ndarray) -> str:
        """Determine if target is regression or classification"""
        
        # Check target name patterns
        if target.startswith('fwd_ret_'):
            return 'regression'
        elif any(target.startswith(prefix) for prefix in 
                ['will_peak', 'will_valley', 'mdd', 'mfe', 'y_will_']):
            return 'classification'
        
        # Check data characteristics
        unique_values = np.unique(y[~np.isnan(y)])
        
        if len(unique_values) <= 2 and all(v in [0, 1] for v in unique_values):
            return 'classification'
        else:
            return 'regression'
    
    def _create_model(self, target_type: str, target: str):
        """Create appropriate model for target type"""
        
        # Get model configuration
        model_config = self.config.get('models', {})
        model_type = model_config.get('type', 'auto')
        
        if model_type == 'auto':
            # Auto-select based on target type
            if target_type == 'classification':
                return self._create_classification_model(target)
            else:
                return self._create_regression_model(target)
        else:
            # Use specified model type
            return self._create_model_by_type(model_type, target_type, target)
    
    def _create_model_with_factory(self, model_type: str, target_type: str, target: str):
        """Create model using the factory system"""
        try:
            from models.factory import ModelFactory
            
            # Use global singleton factory (no re-initialization)
            factory = ModelFactory()
            
            model_config = self.config.get('models', {}).get(model_type, {})
            
            # Remove type from config
            model_config = {k: v for k, v in model_config.items() if k != 'type'}
            
            return factory.create_model(model_type, target_type, model_config)
            
        except Exception as e:
            logger.warning(f"Could not create {model_type} with factory: {e}")
            # Fallback to direct creation
            return self._create_model_by_type(model_type, target_type, target)
    
    def _create_regression_model(self, target: str):
        """Create regression model"""
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import Ridge
        
        model_config = self.config.get('models', {}).get('regression', {})
        model_type = model_config.get('type', 'random_forest')
        
        # Get seed from determinism system
        seed = BASE_SEED if BASE_SEED is not None else 42
        
        if model_type == 'random_forest':
            return RandomForestRegressor(
                n_estimators=model_config.get('n_estimators', 100),
                max_depth=model_config.get('max_depth', None),
                random_state=seed
            )
        elif model_type == 'ridge':
            return Ridge(alpha=model_config.get('alpha', 1.0))
        else:
            # Fallback: try to load default from config
            try:
                from CONFIG.config_loader import get_cfg
                default_n_estimators = int(get_cfg("models.random_forest.n_estimators", default=100, config_name="training_config"))
            except Exception:
                default_n_estimators = 100
            return RandomForestRegressor(n_estimators=default_n_estimators, random_state=seed)
    
    def _create_classification_model(self, target: str):
        """Create classification model"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import LogisticRegression
        
        model_config = self.config.get('models', {}).get('classification', {})
        model_type = model_config.get('type', 'random_forest')
        
        # Get seed from determinism system
        seed = BASE_SEED if BASE_SEED is not None else 42
        
        if model_type == 'random_forest':
            return RandomForestClassifier(
                n_estimators=model_config.get('n_estimators', 100),
                max_depth=model_config.get('max_depth', None),
                random_state=seed
            )
        elif model_type == 'logistic':
            return LogisticRegression(
                C=model_config.get('C', 1.0),
                random_state=seed
            )
        else:
            # Fallback: try to load default from config
            try:
                from CONFIG.config_loader import get_cfg
                default_n_estimators = int(get_cfg("models.random_forest.n_estimators", default=100, config_name="training_config"))
            except Exception:
                default_n_estimators = 100
            return RandomForestClassifier(n_estimators=default_n_estimators, random_state=seed)
    
    def _create_model_by_type(self, model_type: str, target_type: str, target: str):
        """Create model by specific type"""
        # Try factory first for MTF model families
        if model_type in ['LightGBM', 'XGBoost', 'MLP', 'CNN1D', 'LSTM', 'Transformer', 'Ensemble']:
            return self._create_model_with_factory(model_type, target_type, target)
        
        # Fallback to direct creation for other types
        if model_type == 'lightgbm':
            return self._create_lightgbm_model(target_type)
        elif model_type == 'xgboost':
            return self._create_xgboost_model(target_type)
        elif model_type == 'neural_network':
            return self._create_neural_network_model(target_type)
        else:
            # Fallback to default
            if target_type == 'classification':
                return self._create_classification_model(target)
            else:
                return self._create_regression_model(target)
    
    def _create_lightgbm_model(self, target_type: str):
        """Create LightGBM model with Spec 2 high regularization"""
        try:
            import lightgbm as lgb
            
            # Get model config or use Spec 2 defaults
            model_config = self.config.get('models', {}).get('lightgbm', {})
            
            # Spec 2: High Regularization defaults
            params = {
                'num_leaves': model_config.get('num_leaves', 96),  # 64-128 range
                'max_depth': model_config.get('max_depth', 8),  # 7-9 range
                'min_child_weight': model_config.get('min_child_weight', 0.5),
                'learning_rate': model_config.get('learning_rate', 0.03),
                'n_estimators': model_config.get('n_estimators', 1000),
                'subsample': model_config.get('bagging_fraction', 0.75),  # 0.7-0.8 range
                'colsample_bytree': model_config.get('feature_fraction', 0.75),
                'subsample_freq': 1,
                'reg_alpha': model_config.get('lambda_l1', 0.1),
                'reg_lambda': model_config.get('lambda_l2', 0.1),
                'seed': BASE_SEED if BASE_SEED is not None else 42,
                'verbose': -1,
            }
            
            # Remove 'verbose' from model_config if present (to avoid double argument error)
            # verbose is already set in params above
            model_config_clean = {k: v for k, v in model_config.items() if k not in params}
            model_config_clean.pop('verbose', None)  # Explicitly remove verbose if present
            params.update(model_config_clean)  # Merge remaining config params
            
            if target_type == 'classification':
                return lgb.LGBMClassifier(**params)
            else:
                return lgb.LGBMRegressor(**params)
                
        except ImportError:
            logger.warning("LightGBM not available, falling back to RandomForest")
            if target_type == 'classification':
                return self._create_classification_model('fallback')
            else:
                return self._create_regression_model('fallback')
    
    def _create_xgboost_model(self, target_type: str):
        """Create XGBoost model with Spec 2 high regularization"""
        try:
            import xgboost as xgb
            
            # Get model config or use Spec 2 defaults
            model_config = self.config.get('models', {}).get('xgboost', {})
            
            # Spec 2: High Regularization defaults
            params = {
                'max_depth': model_config.get('max_depth', 7),  # 5-8 range
                'min_child_weight': model_config.get('min_child_weight', 0.5),
                'gamma': model_config.get('gamma', 0.3),  # min_split_gain
                'learning_rate': model_config.get('eta', 0.03),
                'n_estimators': model_config.get('n_estimators', 1000),
                'subsample': model_config.get('subsample', 0.75),  # 0.7-0.8 range
                'colsample_bytree': model_config.get('colsample_bytree', 0.75),
                'reg_alpha': model_config.get('reg_alpha', 0.1),
                'reg_lambda': model_config.get('reg_lambda', 0.1),
                'seed': BASE_SEED if BASE_SEED is not None else 42,
                'verbosity': 0,
            }
            
            if target_type == 'classification':
                return xgb.XGBClassifier(**params)
            else:
                return xgb.XGBRegressor(**params)
                
        except ImportError:
            logger.warning("XGBoost not available, falling back to RandomForest")
            if target_type == 'classification':
                return self._create_classification_model('fallback')
            else:
                return self._create_regression_model('fallback')
    
    def _create_neural_network_model(self, target_type: str):
        """Create neural network model"""
        try:
            from sklearn.neural_network import MLPRegressor, MLPClassifier
            
            # Get seed from determinism system
            seed = BASE_SEED if BASE_SEED is not None else 42
            
            if target_type == 'classification':
                return MLPClassifier(
                    hidden_layer_sizes=(100, 50),
                    random_state=seed,
                    max_iter=500
                )
            else:
                return MLPRegressor(
                    hidden_layer_sizes=(100, 50),
                    random_state=seed,
                    max_iter=500
                )
        except ImportError:
            logger.warning("Neural network not available, falling back to RandomForest")
            if target_type == 'classification':
                return self._create_classification_model('fallback')
            else:
                return self._create_regression_model('fallback')
    
    def _get_feature_importance(self, model) -> Optional[np.ndarray]:
        """Get feature importance from model"""
        if hasattr(model, 'feature_importances_'):
            return model.feature_importances_
        elif hasattr(model, 'coef_'):
            return np.abs(model.coef_)
        else:
            return None

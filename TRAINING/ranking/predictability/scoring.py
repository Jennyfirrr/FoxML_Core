# SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial
# Copyright (c) 2025-2026 Fox ML Infrastructure LLC

"""
Target Predictability Ranking

Uses multiple model families to evaluate which of your 63 targets are most predictable.
This helps prioritize compute: train models on high-predictability targets first.

Methodology:
1. For each target, train multiple model families on sample data
2. Calculate predictability scores:
   - Model R² scores (cross-validated)
   - Feature importance magnitude (mean absolute SHAP/importance)
   - Consistency across models (low std = high confidence)
3. Rank targets by composite predictability score
4. Output ranked list with recommendations

Usage:
  # Rank all enabled targets
  python SCRIPTS/rank_target_predictability.py
  
  # Test on specific symbols first
  python SCRIPTS/rank_target_predictability.py --symbols AAPL,MSFT,GOOGL
  
  # Rank specific targets
  python SCRIPTS/rank_target_predictability.py --targets peak_60m,valley_60m,swing_high_15m
"""


import argparse
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional, Union, TYPE_CHECKING
import pandas as pd

if TYPE_CHECKING:
    from TRAINING.common.leakage_auto_fixer import AutoFixInfo
import numpy as np
from dataclasses import dataclass, field
import yaml
import json
from collections import defaultdict
import warnings

# Add project root FIRST (before any scripts.* imports)
# TRAINING/ranking/rank_target_predictability.py -> parents[2] = repo root
_REPO_ROOT = Path(__file__).resolve().parents[2]
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

# Add CONFIG directory to path for centralized config loading
_CONFIG_DIR = _REPO_ROOT / "CONFIG"
if str(_CONFIG_DIR) not in sys.path:
    sys.path.insert(0, str(_CONFIG_DIR))

# Try to import config loader
_CONFIG_AVAILABLE = False
try:
    from config_loader import get_cfg, get_safety_config
    _CONFIG_AVAILABLE = True
except ImportError:
    pass  # Logger not yet initialized, will be set up below

# Import logging config utilities
try:
    from CONFIG.logging_config_utils import get_module_logging_config, get_backend_logging_config
    _LOGGING_CONFIG_AVAILABLE = True
except ImportError:
    _LOGGING_CONFIG_AVAILABLE = False
    # Fallback: create a simple config-like object
    class _DummyLoggingConfig:
        def __init__(self):
            self.gpu_detail = False
            self.cv_detail = False
            self.edu_hints = False
            self.detail = False

# Import checkpoint utility (after path is set)
from TRAINING.orchestration.utils.checkpoint import CheckpointManager

# Import unified task type system
from TRAINING.common.utils.task_types import (
    TaskType, TargetConfig, ModelConfig, 
    is_compatible, create_model_configs_from_yaml
)
from TRAINING.common.utils.task_metrics import evaluate_by_task, compute_composite_score
from TRAINING.ranking.utils.target_validation import validate_target, check_cv_compatibility

# Suppress expected warnings (harmless)
warnings.filterwarnings('ignore', message='X does not have valid feature names')
warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')
warnings.filterwarnings('ignore', category=RuntimeWarning, module='sklearn')
warnings.filterwarnings('ignore', message='invalid value encountered in divide')
warnings.filterwarnings('ignore', message='invalid value encountered in true_divide')

# Setup logging with journald support
from TRAINING.orchestration.utils.logging_setup import setup_logging
logger = setup_logging(
    script_name="rank_target_predictability",
    level=logging.INFO,
    use_journald=True
)

# TargetPredictabilityScore class definition

@dataclass
class TargetPredictabilityScore:
    """Predictability assessment for a single target"""
    target: str
    target_column: str
    task_type: TaskType  # REGRESSION, BINARY_CLASSIFICATION, or MULTICLASS_CLASSIFICATION
    auc: float  # DEPRECATED: Use primary_metric_mean. Mean score (R² for regression, ROC-AUC for binary, accuracy for multiclass)
    std_score: float  # DEPRECATED: Use primary_metric_std. Std of scores
    mean_importance: float  # Mean absolute importance
    consistency: float  # 1 - CV(score) - lower is better
    n_models: int
    model_scores: Dict[str, float]
    composite_score: float = 0.0
    composite_definition: Optional[str] = None  # Formula/version for composite score
    composite_version: Optional[str] = None  # Version identifier for composite calculation
    leakage_flag: str = "OK"  # "OK", "SUSPICIOUS", "HIGH_SCORE", "INCONSISTENT"
    suspicious_features: Dict[str, List[Tuple[str, float]]] = None  # {model: [(feature, imp), ...]}
    fold_timestamps: List[Dict[str, Any]] = None  # List of {fold_idx, train_start, train_end, test_start, test_end} per fold
    fold_scores: Optional[List[float]] = None  # Per-fold scores across all models (for distributional analysis)
    # Auto-fix and rerun tracking
    autofix_info: Optional['AutoFixInfo'] = None  # AutoFixInfo from auto-fixer (if leakage was detected)
    leakage_flags: Dict[str, bool] = None  # Detailed leakage flags: {"perfect_train_acc": bool, "high_auc": bool, etc.}
    status: str = "OK"  # "OK", "SUSPICIOUS_STRONG", "LEAKAGE_UNRESOLVED", "LEAKAGE_UNRESOLVED_MAX_RETRIES"
    attempts: int = 1  # Number of evaluation attempts (for auto-rerun tracking)
    # Canonical metric naming (new, unambiguous naming scheme)
    view: str = "CROSS_SECTIONAL"  # "CROSS_SECTIONAL" or "SYMBOL_SPECIFIC"
    
    # === P0 CORRECTNESS FIELDS (2026-01 snapshot contract unification) ===
    # Primary metric stats (explicit, not inferred from deprecated auc/std_score)
    primary_metric_mean: Optional[float] = None  # Authoritative mean (centered: IC for regression, AUC-excess for classification)
    primary_metric_std: Optional[float] = None  # Authoritative std (falls back to std_score if not set)
    primary_metric_tstat: Optional[float] = None  # t-stat: mean / se - universal skill signal
    primary_se: Optional[float] = None  # Phase 3.1: Standard error (std / sqrt(n)) for SE-based stability
    auto_fix_reason: Optional[str] = None  # Reason why auto-fix was skipped (if applicable)
    
    # Invalid slice tracking (P0: track why cross-sections were excluded)
    n_cs_valid: Optional[int] = None  # Number of valid cross-sections used in aggregation
    n_cs_total: Optional[int] = None  # Total cross-sections before filtering
    invalid_reason_counts: Optional[Dict[str, int]] = None  # {"single_class": 2, "insufficient_samples": 1, ...}
    
    # Classification-specific (P0: centered AUC for proper aggregation)
    auc_mean_raw: Optional[float] = None  # Raw 0-1 AUC mean (classification only)
    auc_excess_mean: Optional[float] = None  # Centered: auc - 0.5 (classification only, for aggregation)
    
    # Schema versioning (P1: track which schema produced this snapshot)
    metrics_schema_version: str = "1.2"  # Bumped to 1.2: added auto_fix_reason field
    scoring_schema_version: str = "1.2"  # Phase 3.2: Bump for eligibility gates and quality formula change (removed stability from quality)
    
    # NEW: Eligibility gates
    valid_for_ranking: bool = True  # Whether this target is eligible for ranking
    invalid_reasons: List[str] = field(default_factory=list)  # List of reasons if not valid (e.g., ["LOW_REGISTRY_COVERAGE", "LOW_N_CS"])
    warnings: List[str] = field(default_factory=list)  # Non-blocking quality flags (e.g., LOW_REGISTRY_COVERAGE when enforce=false)
    run_intent: Optional[str] = None  # NEW: "smoke", "eval", or None - tracks run intent for eligibility
    
    # Phase 3.1: Scoring signature for determinism
    scoring_signature: Optional[str] = None  # SHA256 hash of scoring params
    
    # === DUAL RANKING FIELDS (2026-01 filtering mismatch fix) ===
    # Screen rank: permissive (safe_family + registry) - for discovery
    score_screen: Optional[float] = None  # Composite score using screen features (safe+registry)
    # Strict rank: registry-only (exact training universe) - for promotion decision
    score_strict: Optional[float] = None  # Composite score using strict features (registry-only)
    strict_viability_flag: Optional[bool] = None  # True if score_strict clears promotion threshold
    rank_delta: Optional[int] = None  # rank_screen - rank_strict (positive = screen ranks higher)
    mismatch_telemetry: Optional[Dict[str, Any]] = None  # {
    #   "n_feats_screen": int,  # Number of features in screen evaluation
    #   "n_feats_strict": int,  # Number of features in strict evaluation
    #   "topk_overlap": float,  # Importance overlap (Jaccard similarity of top-k features)
    #   "unknown_feature_count": int,  # Count of features in screen but not in registry
    #   "registry_coverage_rate": float  # n_feats_strict / n_feats_screen
    # }
    
    # Backward compatibility: mean_r2 property
    @property
    def mean_r2(self) -> float:
        """Backward compatibility: returns auc"""
        return self.auc
    
    @property
    def std_r2(self) -> float:
        """Backward compatibility: returns std_score"""
        return self.std_score
    
    @property
    def primary_score(self) -> float:
        """Canonical alias for the primary metric value (replaces ambiguous 'auc')"""
        # Prefer explicit primary_metric_mean if set, otherwise fall back to auc
        if self.primary_metric_mean is not None:
            return self.primary_metric_mean
        return self.auc
    
    @property
    def primary_metric_name(self) -> str:
        """
        Get canonical metric name based on task_type and view.
        
        Examples:
            - REGRESSION + CROSS_SECTIONAL -> "spearman_ic__cs__mean"
            - BINARY_CLASSIFICATION + CROSS_SECTIONAL -> "roc_auc__cs__mean"
        """
        from TRAINING.ranking.predictability.metrics_schema import get_canonical_metric_name
        return get_canonical_metric_name(self.task_type, self.view)
    
    @property
    def std_metric_name(self) -> str:
        """Get canonical name for the std deviation of the primary metric."""
        from TRAINING.ranking.predictability.metrics_schema import get_canonical_metric_name
        return get_canonical_metric_name(self.task_type, self.view, "std")
    
    @property
    def skill01(self) -> float:
        """
        Normalized skill score in [0,1] range for unified routing.
        
        Maps:
        - Regression IC: [-1, 1] → [0, 1] (0.5 * (ic + 1.0))
        - Classification AUC-excess: [-0.5, 0.5] → [0, 1] (0.5 * (auc_excess + 1.0))
        
        Both IC and AUC-excess are centered at 0 (null baseline), so the same
        normalization formula works for both task types.
        
        Returns:
            Normalized skill score in [0, 1] range. Returns 0.0 if primary_metric_mean is None.
        """
        if self.primary_metric_mean is None:
            return 0.0
        # Clamp to valid range: IC ∈ [-1, 1], AUC-excess ∈ [-0.5, 0.5]
        # If primary_metric_mean is outside expected range (e.g., failed target with -999.5),
        # clamp to ensure skill01 ∈ [0, 1]
        normalized = 0.5 * (self.primary_metric_mean + 1.0)
        # Clamp to [0, 1] range (handles edge cases like failed targets)
        return max(0.0, min(1.0, normalized))
    
    @property
    def effective_primary_std(self) -> float:
        """Get effective primary std (prefers explicit, falls back to std_score)."""
        if self.primary_metric_std is not None:
            return self.primary_metric_std
        return self.std_score
    
    @property
    def coverage(self) -> float:
        """Coverage ratio: n_cs_valid / n_cs_total (for composite scoring)."""
        if self.n_cs_valid is not None and self.n_cs_total is not None and self.n_cs_total > 0:
            return self.n_cs_valid / self.n_cs_total
        return 1.0  # Assume full coverage if not tracked
    
    def to_dict(self, filter_task_irrelevant: bool = False) -> Dict[str, Any]:
        """
        Convert to dictionary for JSON serialization.
        
        Args:
            filter_task_irrelevant: If True, exclude fields not relevant to this task type
                (e.g., auc_mean_raw/auc_excess_mean for regression, regression-specific fields for classification)
        """
        # Get canonical metric names (task-aware, view-aware)
        primary_name = self.primary_metric_name
        std_name = self.std_metric_name
        
        # Use explicit primary_metric_mean/std if set, otherwise fall back to auc/std_score
        effective_mean = self.primary_metric_mean if self.primary_metric_mean is not None else self.auc
        effective_std = self.primary_metric_std if self.primary_metric_std is not None else self.std_score
        
        result = {
            'target': self.target,
            'target_column': self.target_column,
            'task_type': self.task_type.name if hasattr(self, 'task_type') else 'REGRESSION',
            'view': getattr(self, 'view', 'CROSS_SECTIONAL'),
            # Schema versions (P1)
            'metrics_schema_version': getattr(self, 'metrics_schema_version', '1.1'),
            'scoring_schema_version': getattr(self, 'scoring_schema_version', '1.2'),
            # Canonical metric names (new, unambiguous) - use effective values
            primary_name: float(effective_mean),
            std_name: float(effective_std),
            # P0: Explicit primary metric stats (authoritative)
            'primary_metric_mean': float(effective_mean),
            'primary_metric_std': float(effective_std),
            # DEPRECATED: legacy fields for backward compatibility
            'auc': float(self.auc),
            'std_score': float(self.std_score),
            'mean_r2': float(self.auc),  # Backward compatibility
            'std_r2': float(self.std_score),  # Backward compatibility
            'mean_importance': float(self.mean_importance),
            'consistency': float(self.consistency),
            'n_models': int(self.n_models),
            'model_scores': {k: float(v) for k, v in sorted(self.model_scores.items())},
            'composite_score': float(self.composite_score),
            'leakage_flag': self.leakage_flag
        }
        
        # P0: Add t-stat if computed
        if self.primary_metric_tstat is not None:
            result['primary_metric_tstat'] = float(self.primary_metric_tstat)
        
        # Phase 3.1: Add primary_se (standard error) for SE-based stability
        if self.primary_se is not None:
            result['primary_se'] = float(self.primary_se)
        
        # P0: Add invalid slice tracking if available
        if self.n_cs_valid is not None:
            result['n_cs_valid'] = int(self.n_cs_valid)
        if self.n_cs_total is not None:
            result['n_cs_total'] = int(self.n_cs_total)
        if self.invalid_reason_counts is not None:
            result['invalid_reason_counts'] = self.invalid_reason_counts
        
        # P0: Add classification-specific centered AUC (only for classification tasks if filtering)
        if filter_task_irrelevant:
            # Only include classification-specific fields if this is a classification task
            try:
                from TRAINING.common.utils.task_types import TaskType
                task_type = getattr(self, 'task_type', None)
                is_classification = task_type and task_type in (
                    TaskType.BINARY_CLASSIFICATION, TaskType.MULTICLASS_CLASSIFICATION
                )
                if is_classification:
                    if self.auc_mean_raw is not None:
                        result['auc_mean_raw'] = float(self.auc_mean_raw)
                    if self.auc_excess_mean is not None:
                        result['auc_excess_mean'] = float(self.auc_excess_mean)
            except ImportError:
                # If TaskType not available, include all fields (backward compatibility)
                if self.auc_mean_raw is not None:
                    result['auc_mean_raw'] = float(self.auc_mean_raw)
                if self.auc_excess_mean is not None:
                    result['auc_excess_mean'] = float(self.auc_excess_mean)
        else:
            # Include all fields (default behavior)
            if self.auc_mean_raw is not None:
                result['auc_mean_raw'] = float(self.auc_mean_raw)
            if self.auc_excess_mean is not None:
                result['auc_excess_mean'] = float(self.auc_excess_mean)
        
        # Phase 3.1: Add scoring signature for determinism
        if self.scoring_signature is not None:
            result['scoring_signature'] = self.scoring_signature
        
        # Eligibility gates (Phase 3.2)
        result['valid_for_ranking'] = bool(getattr(self, 'valid_for_ranking', True))
        if hasattr(self, 'invalid_reasons') and self.invalid_reasons:
            result['invalid_reasons'] = list(self.invalid_reasons)
        if hasattr(self, 'warnings') and self.warnings:
            result['warnings'] = list(self.warnings)
        if hasattr(self, 'run_intent') and self.run_intent is not None:
            result['run_intent'] = str(self.run_intent)
        
        # Dual ranking fields (2026-01 filtering mismatch fix)
        if self.score_screen is not None:
            result['score_screen'] = float(self.score_screen)
        if self.score_strict is not None:
            result['score_strict'] = float(self.score_strict)
        if self.strict_viability_flag is not None:
            result['strict_viability_flag'] = bool(self.strict_viability_flag)
        if self.rank_delta is not None:
            result['rank_delta'] = int(self.rank_delta)
        if self.mismatch_telemetry is not None:
            result['mismatch_telemetry'] = self.mismatch_telemetry
        
        # Add composite score definition and version
        if self.composite_definition is not None:
            result['composite_definition'] = self.composite_definition
        if self.composite_version is not None:
            result['composite_version'] = self.composite_version
        
        # Add fold scores and distributional stats
        if self.fold_scores is not None and len(self.fold_scores) > 0:
            import numpy as np
            valid_scores = [s for s in self.fold_scores if s is not None and not (isinstance(s, float) and np.isnan(s))]
            if valid_scores:
                result['fold_scores'] = [float(s) for s in valid_scores]
                result['min_score'] = float(np.min(valid_scores))
                result['max_score'] = float(np.max(valid_scores))
                result['median_score'] = float(np.median(valid_scores))
        
        # Enhanced leakage reporting
        if self.leakage_flags is not None or self.leakage_flag != "OK":
            leakage_info = {
                'status': self.leakage_flag,
                'checks_run': []
            }
            
            # Determine which checks were run based on available flags
            if self.leakage_flags:
                if 'perfect_train_acc' in self.leakage_flags:
                    leakage_info['checks_run'].append('perfect_train_accuracy')
                if 'high_auc' in self.leakage_flags or 'high_r2' in self.leakage_flags:
                    leakage_info['checks_run'].append('high_cv_score')
                if 'suspicious_flag' in self.leakage_flags:
                    leakage_info['checks_run'].append('suspicious_features')
            
            # Add violations if any
            violations = []
            if self.leakage_flag != "OK":
                violations.append(f"leakage_flag={self.leakage_flag}")
            if self.leakage_flags:
                for check, flag in sorted(self.leakage_flags.items()):
                    if flag and check != 'suspicious_flag':  # suspicious_flag is redundant with leakage_flag
                        violations.append(check)
            
            if violations:
                leakage_info['violations'] = violations
            
            result['leakage'] = leakage_info
        else:
            # Still provide structure even when OK
            result['leakage'] = {
                'status': 'OK',
                'checks_run': ['lookahead', 'target_overlap', 'feature_lookback'],
                'violations': []
            }
        
        if self.fold_timestamps is not None:
            result['fold_timestamps'] = self.fold_timestamps
        return result
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> 'TargetPredictabilityScore':
        """Create from dictionary"""
        # Handle suspicious_features if present
        suspicious = d.pop('suspicious_features', None)
        
        # Backward compatibility: handle old format with mean_r2/std_r2
        if 'mean_r2' in d and 'auc' not in d:
            d['auc'] = d['mean_r2']
        if 'std_r2' in d and 'std_score' not in d:
            d['std_score'] = d['std_r2']
        
        # Handle task_type (may be missing in old checkpoints)
        if 'task_type' not in d:
            # Try to infer from target name or default to REGRESSION
            d['task_type'] = TaskType.REGRESSION
        
        # Convert task_type string to enum if needed
        if isinstance(d.get('task_type'), str):
            d['task_type'] = TaskType[d['task_type']]
        
        obj = cls(**d)
        if suspicious:
            obj.suspicious_features = suspicious
        return obj



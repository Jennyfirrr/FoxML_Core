# SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial
# Copyright (c) 2025-2026 Fox ML Infrastructure LLC

"""
Automated Leakage Detection and Auto-Fix System

Automatically detects leaking features from:
1. Leakage sentinels (shifted target, symbol holdout, randomized time tests)
2. Feature importance analysis (perfect scores, suspicious importance)
3. Importance diff detector (comparing full vs safe feature sets)

Then auto-populates excluded_features.yaml and feature_registry.yaml
and re-runs training until no leakage is detected.
"""

import sys
import logging
import yaml
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass
from datetime import datetime
import json
import re
import hashlib
import fcntl
import os

# Add project root to path
# TRAINING/common/leakage_auto_fixer.py -> parents[2] = repo root
_REPO_ROOT = Path(__file__).resolve().parents[2]
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

from TRAINING.common.leakage_sentinels import LeakageSentinel, SentinelResult
from TRAINING.common.importance_diff_detector import ImportanceDiffDetector
from TRAINING.ranking.utils.leakage_filtering import filter_features_for_target, _load_leakage_config
from TRAINING.common.utils.sst_contract import resolve_target_horizon_minutes
# DETERMINISM: Import deterministic filesystem helpers
from TRAINING.common.utils.determinism_ordering import iterdir_sorted

logger = logging.getLogger(__name__)

# Try to import config loader for path configuration
_CONFIG_AVAILABLE = False
try:
    from config_loader import get_system_config
    _CONFIG_AVAILABLE = True
except ImportError:
    pass


# ============================================================================
# Patch File Utilities (Collision-Proof Naming)
# ============================================================================
# Import from neutral module to avoid import cycles
from TRAINING.common.registry_patch_naming import safe_target_filename, find_patch_file


@dataclass(frozen=True)
class TargetContext:
    """
    Explicit target context for registry updates.
    
    Bundles all target-specific information needed for per-target, per-horizon
    registry patches. Uses SST-driven bar_minutes (not hardcoded).
    """
    target: str
    horizon_minutes: int
    horizon_bars: int
    bar_minutes: int  # SST-driven, not hardcoded
    
    @classmethod
    def from_target(
        cls,
        target: str,
        bar_minutes: float,  # From SST (resolved_config.interval_minutes or detected_interval)
        experiment_config: Optional[Any] = None
    ) -> Optional['TargetContext']:
        """
        Create TargetContext from target name and SST bar_minutes.
        
        Args:
            target: Target column name (e.g., 'fwd_ret_5m')
            bar_minutes: Data bar interval in minutes (SST-driven, from config or detection)
            experiment_config: Optional experiment config (for horizon extraction)
        
        Returns:
            TargetContext if horizon can be resolved and is divisible by bar_minutes, None otherwise
        """
        horizon_minutes = resolve_target_horizon_minutes(target, experiment_config)
        if horizon_minutes is None:
            return None
        
        # STRICT: Enforce divisibility
        if horizon_minutes % bar_minutes != 0:
            logger.warning(
                f"Target {target}: horizon_minutes={horizon_minutes} not divisible by "
                f"bar_minutes={bar_minutes}. Cannot compute horizon_bars. "
                f"Skipping per-horizon exclusion (will use target-level exclusion if needed)."
            )
            return None
        
        horizon_bars = int(horizon_minutes / bar_minutes)
        # ENFORCE: bar_minutes stored as int (not float)
        bar_minutes_int = int(bar_minutes)
        return cls(
            target=target,
            horizon_minutes=int(horizon_minutes),
            horizon_bars=horizon_bars,
            bar_minutes=bar_minutes_int  # Always int
        )


@dataclass
class LeakageDetection:
    """Result of leakage detection for a single feature."""
    feature_name: str
    confidence: float  # 0.0 to 1.0
    reason: str  # Why it's considered a leak
    source: str  # Which detector found it (sentinels, importance, diff)
    suggested_action: str  # 'exact', 'prefix', 'regex', 'registry_reject'


@dataclass
class AutoFixInfo:
    """Information about what was modified by auto-fixer."""
    modified_configs: bool  # True if any configs were modified
    modified_files: List[str]  # List of config files that were modified
    modified_features: List[str]  # List of feature names that were excluded/rejected
    excluded_features_updates: Dict[str, Any]  # Updates to excluded_features.yaml
    feature_registry_updates: Dict[str, Any]  # Updates to feature_registry.yaml
    backup_files: List[str] = None  # List of backup files created


class LeakageAutoFixer:
    """
    Automatically detects and fixes data leakage by:
    1. Running leakage diagnostics
    2. Identifying leaking features
    3. Auto-updating config files
    4. Re-running until clean
    """
    
    def __init__(
        self,
        excluded_features_path: Optional[Path] = None,
        feature_registry_path: Optional[Path] = None,
        backup_configs: bool = False,  # Disabled by default - backups are not needed
        output_dir: Optional[Path] = None  # Optional: if provided, backups go to output_dir/backups/ instead of CONFIG/backups/
    ):
        """
        Initialize auto-fixer.
        
        Args:
            excluded_features_path: Path to excluded_features.yaml (default: CONFIG/excluded_features.yaml)
            feature_registry_path: Path to feature_registry.yaml (default: CONFIG/feature_registry.yaml)
            backup_configs: If True, backup configs before modifying (default: False - backups disabled)
            output_dir: Optional output directory. If provided and backup_configs=True, backups go to output_dir/backups/
        """
        # Load paths from config if available, otherwise use defaults
        if excluded_features_path is None:
            if _CONFIG_AVAILABLE:
                try:
                    system_cfg = get_system_config()
                    config_path = system_cfg.get('system', {}).get('paths', {})
                    excluded_path = config_path.get('excluded_features')
                    if excluded_path:
                        excluded_features_path = Path(excluded_path)
                        if not excluded_features_path.is_absolute():
                            excluded_features_path = _REPO_ROOT / excluded_path
                    else:
                        # Use default: CONFIG/excluded_features.yaml
                        config_dir = config_path.get('config_dir', 'CONFIG')
                        excluded_features_path = _REPO_ROOT / config_dir / "excluded_features.yaml"
                except Exception:
                    # Fallback to default
                    excluded_features_path = _REPO_ROOT / "CONFIG" / "excluded_features.yaml"
            else:
                excluded_features_path = _REPO_ROOT / "CONFIG" / "excluded_features.yaml"
        
        if feature_registry_path is None:
            if _CONFIG_AVAILABLE:
                try:
                    system_cfg = get_system_config()
                    config_path = system_cfg.get('system', {}).get('paths', {})
                    registry_path = config_path.get('feature_registry')
                    if registry_path:
                        feature_registry_path = Path(registry_path)
                        if not feature_registry_path.is_absolute():
                            # CRITICAL: Ensure relative paths include CONFIG prefix (SST pattern)
                            # Config may have "data/feature_registry.yaml" but canonical path is "CONFIG/data/feature_registry.yaml"
                            registry_path_str = str(registry_path)
                            if not registry_path_str.startswith("CONFIG") and "CONFIG/" not in registry_path_str:
                                # Prepend CONFIG/ to relative paths that don't already include it
                                registry_path_str = "CONFIG/" + registry_path_str
                            feature_registry_path = _REPO_ROOT / registry_path_str
                    else:
                        # Use canonical registry (SST)
                        config_dir = config_path.get('config_dir', 'CONFIG')
                        feature_registry_path = _REPO_ROOT / config_dir / "data" / "feature_registry.yaml"
                except Exception:
                    # Fallback to canonical registry (SST)
                    feature_registry_path = _REPO_ROOT / "CONFIG" / "data" / "feature_registry.yaml"
            else:
                feature_registry_path = _REPO_ROOT / "CONFIG" / "data" / "feature_registry.yaml"
        
        self.excluded_features_path = Path(excluded_features_path)
        self.feature_registry_path = Path(feature_registry_path)
        self.backup_configs = backup_configs
        self.output_dir = Path(output_dir) if output_dir is not None else None
        
        # Get backup directory - prefer output_dir if provided (for cohort-organized runs)
        if output_dir is not None:
            # If output_dir is provided, store backups in run directory
            # This allows backups to be organized by cohort when run moves to RESULTS/{cohort_id}/
            self.backup_dir = Path(output_dir) / "backups"
            self._use_run_backups = True
        elif _CONFIG_AVAILABLE:
            try:
                system_cfg = get_system_config()
                config_path = system_cfg.get('system', {}).get('paths', {})
                backup_dir = config_path.get('config_backup_dir')
                if backup_dir:
                    self.backup_dir = Path(backup_dir)
                    if not self.backup_dir.is_absolute():
                        self.backup_dir = _REPO_ROOT / backup_dir
                else:
                    # Default: CONFIG/backups/
                    config_dir = config_path.get('config_dir', 'CONFIG')
                    self.backup_dir = _REPO_ROOT / config_dir / "backups"
                self._use_run_backups = False
            except Exception:
                # Fallback to default
                self.backup_dir = self.excluded_features_path.parent / "backups"
                self._use_run_backups = False
        else:
            # Fallback to default
            self.backup_dir = self.excluded_features_path.parent / "backups"
            self._use_run_backups = False
        
        # Track detected leaks across iterations
        self.detected_leaks: Dict[str, LeakageDetection] = {}
        self.iteration_count = 0
        
        # Cache of already-excluded features (loaded on demand)
        self._excluded_features_cache: Optional[Set[str]] = None
        self._excluded_prefixes_cache: Optional[Set[str]] = None
        
        # Load backup settings from config
        self.max_backups_per_target = self._load_backup_config()
        
        # Log initialization for observability
        logger.info(f"üîß LeakageAutoFixer initialized:")
        logger.info(f"   - Excluded features: {self.excluded_features_path} (exists: {self.excluded_features_path.exists()})")
        logger.info(f"   - Feature registry: {self.feature_registry_path} (exists: {self.feature_registry_path.exists()})")
        if self.backup_configs:
            logger.info(f"   - Backup directory: {self.backup_dir} (exists: {self.backup_dir.exists()})")
            logger.info(f"   - Backup enabled: {self.backup_configs}")
            logger.info(f"   - Max backups per target: {self.max_backups_per_target}")
        else:
            logger.debug(f"   - Backups disabled (backup_configs=False)")
    
    def _load_excluded_features(self) -> Tuple[Set[str], Set[str]]:
        """Load already-excluded features from config files."""
        if self._excluded_features_cache is not None:
            return self._excluded_features_cache, self._excluded_prefixes_cache
        
        excluded_exact = set()
        excluded_prefixes = set()
        
        # Load from excluded_features.yaml
        if self.excluded_features_path.exists():
            try:
                with open(self.excluded_features_path, 'r') as f:
                    config = yaml.safe_load(f) or {}
                    always_exclude = config.get('always_exclude', {})
                    excluded_exact = set(always_exclude.get('exact_patterns', []))
                    excluded_prefixes = set(always_exclude.get('prefix_patterns', []))
            except Exception as e:
                logger.debug(f"Could not load excluded_features.yaml: {e}")
        
        # Load from feature_registry.yaml (rejected features)
        if self.feature_registry_path.exists():
            try:
                with open(self.feature_registry_path, 'r') as f:
                    config = yaml.safe_load(f) or {}
                    features = config.get('features', {})
                    for feat_name, metadata in features.items():
                        if metadata.get('rejected', False):
                            excluded_exact.add(feat_name)
            except Exception as e:
                logger.debug(f"Could not load feature_registry.yaml: {e}")
        
        self._excluded_features_cache = excluded_exact
        self._excluded_prefixes_cache = excluded_prefixes
        return excluded_exact, excluded_prefixes
    
    def _is_already_excluded(self, feature_name: str) -> bool:
        """Check if a feature is already excluded."""
        excluded_exact, excluded_prefixes = self._load_excluded_features()
        
        # Check exact match
        if feature_name in excluded_exact:
            return True
        
        # Check prefix match
        for prefix in excluded_prefixes:
            if feature_name.startswith(prefix):
                return True
        
        return False
    
    def detect_leaking_features(
        self,
        X: Any,  # Feature matrix (pd.DataFrame or np.ndarray)
        y: Any,  # Target (pd.Series or np.ndarray)
        feature_names: List[str],
        target_column: str,
        symbols: Optional[Any] = None,  # pd.Series with symbol labels
        task_type: str = 'classification',  # 'classification' or 'regression'
        data_interval_minutes: int = 5,
        model_importance: Optional[Dict[str, float]] = None,  # feature -> importance
        train_score: Optional[float] = None,  # Perfect score indicates leakage
        test_score: Optional[float] = None
    ) -> List[LeakageDetection]:
        """
        Detect leaking features using multiple methods.
        
        Filters out features that are already excluded to avoid redundant detections.
        
        Returns:
            List of LeakageDetection objects
        """
        # Filter out already-excluded features from detection
        # (These shouldn't be in feature_names if filtering worked, but check anyway)
        excluded_exact, excluded_prefixes = self._load_excluded_features()
        candidate_features = [
            f for f in feature_names 
            if not self._is_already_excluded(f)
        ]
        
        if len(candidate_features) < len(feature_names):
            logger.debug(
                f"Filtered out {len(feature_names) - len(candidate_features)} "
                f"already-excluded features from detection"
            )
        
        detections = []
        
        # Method 1: Perfect scores indicate leakage
        # Load threshold from config
        try:
            from CONFIG.config_loader import get_safety_config
            safety_cfg = get_safety_config()
            # safety_config.yaml has a top-level 'safety' key
            safety_section = safety_cfg.get('safety', {})
            leakage_cfg = safety_section.get('leakage_detection', {})
            auto_fixer_cfg = leakage_cfg.get('auto_fixer', {})
            perfect_score_threshold = float(auto_fixer_cfg.get('perfect_score_threshold', 0.99))
        except Exception:
            perfect_score_threshold = 0.99  # FALLBACK_DEFAULT_OK
        
        train_score_str = f"{train_score:.4f}" if train_score is not None else "None"
        logger.debug(f"Leakage detection: train_score={train_score_str}, "
                    f"threshold={perfect_score_threshold:.4f}, "
                    f"features={len(feature_names)}, "
                    f"importance_keys={len(model_importance) if model_importance else 0}")
        
        if train_score is not None and train_score >= perfect_score_threshold:
            logger.debug(f"Method 1: Perfect score detected ({train_score:.4f} >= 0.99)")
            # High importance features in perfect-score models are suspicious
            if model_importance and len(model_importance) > 0:
                sorted_features = sorted(model_importance.items(), key=lambda x: x[1], reverse=True)
                top_features = sorted_features[:10]  # Top 10 most important
                logger.debug(f"Method 1: Found {len(top_features)} top features from model_importance")
                
                for feat_name, importance in top_features:
                    if feat_name in candidate_features:
                        # CRITICAL FIX: Confidence should reflect perfect score context, not just raw importance
                        # Even low-importance features in perfect-score models are highly suspicious
                        # Base confidence of 0.85 (perfect score = high suspicion), scaled by importance
                        base_confidence = 0.85  # High base confidence for perfect score context
                        importance_boost = min(0.1, importance * 2)  # Boost up to 0.1 based on importance
                        detection_confidence = min(0.95, base_confidence + importance_boost)
                        detections.append(LeakageDetection(
                            feature_name=feat_name,
                            confidence=detection_confidence,
                            reason=f"High importance ({importance:.2%}) in perfect-score model (train_score={train_score:.4f})",
                            source="perfect_score_importance",
                            suggested_action=self._suggest_action(feat_name)
                        ))
                logger.debug(f"Method 1: Created {len(detections)} detections from perfect score")
            else:
                logger.warning(f"Method 1: No model_importance provided or empty (len={len(model_importance) if model_importance else 0})")
                logger.warning(f"   Perfect score ({train_score:.4f}) detected but no feature importances available!")
                logger.warning(f"   This limits detection effectiveness. Computing importances on-the-fly...")
                
                # CRITICAL: When we have perfect score but no importances, we MUST compute them
                # Otherwise we can't identify which features are causing the leakage
                try:
                    import pandas as pd
                    import numpy as np
                    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
                    from sklearn.linear_model import LogisticRegression, LinearRegression
                    
                    # Convert to DataFrame if needed
                    if not isinstance(X, pd.DataFrame):
                        X_df = pd.DataFrame(X, columns=feature_names)
                    else:
                        X_df = X
                    
                    if not isinstance(y, pd.Series):
                        y_series = pd.Series(y)
                    else:
                        y_series = y
                    
                    # EH-002: Get deterministic seed with strict mode enforcement
                    try:
                        from TRAINING.common.determinism import BASE_SEED, stable_seed_from, is_strict_mode
                        target = getattr(self, 'target', target_column)
                        if BASE_SEED is not None:
                            leak_seed = stable_seed_from(['leakage_auto_fixer', target])
                        else:
                            if is_strict_mode():
                                from TRAINING.common.exceptions import ConfigError
                                raise ConfigError(
                                    "BASE_SEED not initialized in strict mode for leakage detection",
                                    config_key="pipeline.determinism.base_seed",
                                    stage="LEAKAGE_DETECTION"
                                )
                            leak_seed = 42  # FALLBACK_DEFAULT_OK
                            logger.warning("EH-002: BASE_SEED is None, using fallback seed=42")
                    except ImportError as e:
                        leak_seed = 42  # FALLBACK_DEFAULT_OK
                        logger.warning(f"EH-002: Using fallback seed=42 due to import error: {e}")
                    
                    # Train a quick model to get feature importances
                    logger.debug(f"   Training quick model for importance extraction (seed={leak_seed})...")
                    if task_type == 'classification':
                        quick_model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=leak_seed, n_jobs=1)
                    else:
                        quick_model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=leak_seed, n_jobs=1)
                    
                    # Use sample if data is too large
                    if len(X_df) > 10000:
                        sample_idx = np.random.RandomState(leak_seed).choice(len(X_df), size=10000, replace=False)
                        X_sample = X_df.iloc[sample_idx]
                        y_sample = y_series.iloc[sample_idx]
                    else:
                        X_sample = X_df
                        y_sample = y_series
                    
                    quick_model.fit(X_sample, y_sample)
                    
                    # Extract importances
                    computed_importance = dict(zip(feature_names, quick_model.feature_importances_))
                    logger.debug(f"   Computed importances for {len(computed_importance)} features")
                    
                    # Now use computed importances
                    sorted_features = sorted(computed_importance.items(), key=lambda x: x[1], reverse=True)
                    top_features = sorted_features[:10]  # Top 10 most important
                    logger.debug(f"Method 1: Found {len(top_features)} top features from computed importance")
                    
                    for feat_name, importance in top_features:
                        if feat_name in candidate_features:
                            # CRITICAL FIX: Confidence should reflect perfect score context, not just raw importance
                            # Even low-importance features in perfect-score models are highly suspicious
                            # Base confidence of 0.85 (perfect score = high suspicion), scaled by importance
                            base_confidence = 0.85  # High base confidence for perfect score context
                            importance_boost = min(0.1, importance * 2)  # Boost up to 0.1 based on importance
                            detection_confidence = min(0.95, base_confidence + importance_boost)
                            detections.append(LeakageDetection(
                                feature_name=feat_name,
                                confidence=detection_confidence,
                                reason=f"High importance ({importance:.2%}) in perfect-score model (train_score={train_score:.4f}, computed on-the-fly)",
                                source="perfect_score_computed_importance",
                                suggested_action=self._suggest_action(feat_name)
                            ))
                    logger.info(f"Method 1: Created {len(detections)} detections from computed importance")
                    
                except Exception as e:
                    logger.warning(f"   Failed to compute importances on-the-fly: {e}")
                    logger.warning("   Falling back to pattern-based detection for all features")
                    # Fallback: check all features for known patterns
                    for feat_name in candidate_features:
                        if self._is_known_leaky_pattern(feat_name):
                            detections.append(LeakageDetection(
                                feature_name=feat_name,
                                confidence=0.9,  # High confidence for known patterns with perfect score
                                reason=f"Known leaky pattern in perfect-score model (train_score={train_score:.4f})",
                                source="perfect_score_pattern",
                                suggested_action=self._suggest_action(feat_name)
                            ))
                    logger.debug(f"Method 1: Pattern-based fallback found {len(detections)} detections")
        
        # Method 2: Leakage sentinels
        try:
            sentinel = LeakageSentinel()
            
            # Convert to DataFrame if needed
            import pandas as pd
            import numpy as np
            
            if not isinstance(X, pd.DataFrame):
                X_df = pd.DataFrame(X, columns=feature_names)
            else:
                X_df = X
            
            if not isinstance(y, pd.Series):
                y_series = pd.Series(y)
            else:
                y_series = y
            
            # Train a simple model for sentinel tests
            # Use sklearn models for fast training
            try:
                from sklearn.linear_model import LogisticRegression, LinearRegression
                from sklearn.model_selection import train_test_split
                
                # EH-002: Get deterministic seed with strict mode enforcement
                try:
                    from TRAINING.common.determinism import BASE_SEED, stable_seed_from, is_strict_mode
                    # Use target/feature-specific seed if available
                    target = getattr(self, 'target', 'leakage_test')
                    if BASE_SEED is not None:
                        leak_seed = stable_seed_from(['leakage_auto_fixer', target])
                    else:
                        if is_strict_mode():
                            from TRAINING.common.exceptions import ConfigError
                            raise ConfigError(
                                "BASE_SEED not initialized in strict mode for leakage testing",
                                config_key="pipeline.determinism.base_seed",
                                stage="LEAKAGE_DETECTION"
                            )
                        leak_seed = 42  # FALLBACK_DEFAULT_OK
                        logger.warning("EH-002: BASE_SEED is None, using fallback seed=42")
                except ImportError as e:
                    leak_seed = 42  # FALLBACK_DEFAULT_OK
                    logger.warning(f"EH-002: Using fallback seed=42 due to import error: {e}")
                
                if task_type == 'classification':
                    simple_model = LogisticRegression(random_state=leak_seed, solver='liblinear', max_iter=100)
                else:
                    simple_model = LinearRegression()
                
                # Train on subset for speed
                if len(X_df) > 10000:
                    X_sample, _, y_sample, _ = train_test_split(X_df, y_series, train_size=10000, random_state=leak_seed, stratify=y_series if task_type == 'classification' else None)
                else:
                    X_sample, y_sample = X_df, y_series
                
                simple_model.fit(X_sample, y_sample)
                
                # Shifted target test (requires model)
                shifted_result = sentinel.shifted_target_test(
                    simple_model, X_sample.values, y_sample.values, horizon=1
                )
                if not shifted_result.passed and shifted_result.score > 0.7:
                    # High score on shifted target = features encode future info
                    # Mark top features as suspicious
                    if model_importance:
                        top_suspicious = sorted(model_importance.items(), key=lambda x: x[1], reverse=True)[:5]
                        for feat_name, importance in top_suspicious:
                            if feat_name in candidate_features:
                                detections.append(LeakageDetection(
                                    feature_name=feat_name,
                                    confidence=0.8,
                                    reason=f"High importance in shifted-target test failure (score={shifted_result.score:.3f})",
                                    source="shifted_target_test",
                                    suggested_action=self._suggest_action(feat_name)
                                ))
                
                # Symbol holdout test (requires train/test split by symbol)
                if symbols is not None and len(symbols.unique()) >= 2:
                    try:
                        from sklearn.model_selection import train_test_split as sk_train_test_split
                        unique_symbols = symbols.unique()
                        # Use symbol-specific seed for holdout test
                        holdout_seed = stable_seed_from(['leakage_holdout', target]) if BASE_SEED is not None else leak_seed
                        # Load test_size from config
                        try:
                            from CONFIG.config_loader import get_safety_config
                            safety_cfg = get_safety_config()
                            # safety_config.yaml has a top-level 'safety' key
                            safety_section = safety_cfg.get('safety', {})
                            leakage_cfg = safety_section.get('leakage_detection', {})
                            auto_fixer_cfg = leakage_cfg.get('auto_fixer', {})
                            symbol_holdout_test_size = float(auto_fixer_cfg.get('symbol_holdout_test_size', 0.2))
                        except Exception:
                            symbol_holdout_test_size = 0.2  # FALLBACK_DEFAULT_OK
                        train_syms, test_syms = sk_train_test_split(
                            unique_symbols, test_size=symbol_holdout_test_size, random_state=holdout_seed
                        )
                        X_train_sym = X_df[symbols.isin(train_syms)]
                        y_train_sym = y_series[symbols.isin(train_syms)]
                        X_test_sym = X_df[symbols.isin(test_syms)]
                        y_test_sym = y_series[symbols.isin(test_syms)]
                        
                        if len(X_train_sym) > 100 and len(X_test_sym) > 100:
                            holdout_model = LogisticRegression(random_state=holdout_seed, solver='liblinear', max_iter=100) if task_type == 'classification' else LinearRegression()
                            holdout_model.fit(X_train_sym, y_train_sym)
                            
                            holdout_result = sentinel.symbol_holdout_test(
                                holdout_model, X_train_sym.values, y_train_sym.values,
                                X_test_sym.values, y_test_sym.values,
                                train_symbols=list(train_syms), test_symbols=list(test_syms)
                            )
                            if not holdout_result.passed:
                                # Large train/test gap = symbol-specific leakage
                                if model_importance:
                                    top_suspicious = sorted(model_importance.items(), key=lambda x: x[1], reverse=True)[:5]
                                    for feat_name, importance in top_suspicious:
                                        if feat_name in candidate_features:
                                            detections.append(LeakageDetection(
                                                feature_name=feat_name,
                                                confidence=0.7,
                                                reason=f"High importance in symbol-holdout test failure (diff={holdout_result.details.get('gap', 0):.3f})",
                                                source="symbol_holdout_test",
                                                suggested_action=self._suggest_action(feat_name)
                                            ))
                    except Exception as e:
                        logger.debug(f"Symbol holdout test skipped: {e}")
                
                # Randomized time test (requires model)
                randomized_result = sentinel.randomized_time_test(
                    simple_model, X_sample.values, y_sample.values
                )
                if not randomized_result.passed and randomized_result.score > 0.7:
                    # High score on randomized time = features encode temporal info incorrectly
                    if model_importance:
                        top_suspicious = sorted(model_importance.items(), key=lambda x: x[1], reverse=True)[:5]
                        for feat_name, importance in top_suspicious:
                            if feat_name in candidate_features:
                                detections.append(LeakageDetection(
                                    feature_name=feat_name,
                                    confidence=0.75,
                                    reason=f"High importance in randomized-time test failure (score={randomized_result.score:.3f})",
                                    source="randomized_time_test",
                                    suggested_action=self._suggest_action(feat_name)
                                ))
            except Exception as e:
                logger.debug(f"Sentinel tests skipped (need model): {e}")
        except Exception as e:
            logger.warning(f"Leakage sentinels failed: {e}")
        
        # Method 3: Pattern-based detection (known leaky patterns)
        # Only run if we haven't already detected leaks from perfect scores
        # (to avoid duplicate detections, but still check patterns if no importance data)
        if not detections or (train_score is None or train_score < 0.99):
            logger.debug("Method 3: Checking for known leaky patterns")
            pattern_detections = []
            for feat_name in candidate_features:
                if self._is_known_leaky_pattern(feat_name):
                    pattern_detections.append(LeakageDetection(
                        feature_name=feat_name,
                        confidence=0.95,  # FALLBACK_DEFAULT_OK: High confidence for known patterns (should load from safety_config.yaml)
                        reason="Matches known leaky pattern",
                        source="pattern_detection",
                        suggested_action=self._suggest_action(feat_name)
                    ))
            detections.extend(pattern_detections)
            logger.debug(f"Method 3: Found {len(pattern_detections)} known leaky patterns")
        
        # Deduplicate and merge confidence scores
        merged = self._merge_detections(detections)
        
        # Enhanced logging for visibility
        logger.info(f"üîç Leakage detection complete: {len(merged)} feature(s) detected "
                   f"(from {len(detections)} raw detections)")
        if merged:
            top_3 = sorted(merged, key=lambda d: d.confidence, reverse=True)[:3]
            logger.info(f"   Top detections: {', '.join([f'{d.feature_name} (conf={d.confidence:.2f}, source={d.source})' for d in top_3])}")
        else:
            # If no detections but we have perfect score, this is suspicious
            if train_score is not None and train_score >= perfect_score_threshold:
                logger.warning(f"   ‚ö†Ô∏è  WARNING: Perfect score ({train_score:.4f}) detected but NO leaks found!")
                logger.warning(f"   This may indicate:")
                logger.warning(f"   1. Leakage is structural (in target construction, not features)")
                logger.warning(f"   2. Leaky features are already excluded (check excluded_features.yaml)")
                logger.warning(f"   3. Detection methods need improvement")
                logger.warning(f"   Candidate features checked: {len(candidate_features)} (total features: {len(feature_names)})")
                if len(candidate_features) < len(feature_names):
                    logger.warning(f"   {len(feature_names) - len(candidate_features)} features already excluded")
        logger.debug(f"Total detections after merge: {len(merged)}")
        
        return merged
    
    def _is_known_leaky_pattern(self, feature_name: str) -> bool:
        """Check if feature matches known leaky patterns."""
        leaky_prefixes = ['p_', 'y_', 'fwd_ret_', 'tth_', 'mfe_', 'mdd_', 'barrier_', 'next_', 'future_']
        leaky_exact = ['ts', 'timestamp', 'symbol', 'date', 'time']
        
        if feature_name in leaky_exact:
            return True
        
        for prefix in leaky_prefixes:
            if feature_name.startswith(prefix):
                return True
        
        return False
    
    def _suggest_action(self, feature_name: str) -> str:
        """Suggest the best action for excluding a feature."""
        # Exact match for common metadata
        if feature_name in ['ts', 'timestamp', 'symbol', 'date', 'time']:
            return 'exact'
        
        # Prefix patterns for known leaky families
        if feature_name.startswith('p_'):
            return 'prefix'  # Add 'p_' to prefix_patterns
        if feature_name.startswith('y_'):
            return 'prefix'  # Add 'y_' to prefix_patterns
        if feature_name.startswith('fwd_ret_'):
            return 'prefix'  # Add 'fwd_ret_' to prefix_patterns
        
        # For others, use exact match (safer)
        return 'exact'
    
    def _merge_detections(self, detections: List[LeakageDetection]) -> List[LeakageDetection]:
        """Merge duplicate detections, taking max confidence."""
        merged_dict: Dict[str, LeakageDetection] = {}
        
        for det in detections:
            if det.feature_name not in merged_dict:
                merged_dict[det.feature_name] = det
            else:
                # Merge: take max confidence, combine reasons
                existing = merged_dict[det.feature_name]
                if det.confidence > existing.confidence:
                    merged_dict[det.feature_name] = LeakageDetection(
                        feature_name=det.feature_name,
                        confidence=det.confidence,
                        reason=f"{existing.reason}; {det.reason}",
                        source=f"{existing.source}+{det.source}",
                        suggested_action=det.suggested_action
                    )
        
        return list(merged_dict.values())
    
    def apply_fixes(
        self,
        detections: List[LeakageDetection],
        min_confidence: Optional[float] = None,  # Load from config if None
        max_features: Optional[int] = None,
        dry_run: bool = False,
        target: Optional[str] = None,
        max_backups_per_target: Optional[int] = None,  # Load from config if None
        target_ctx: Optional[TargetContext] = None,  # NEW: explicit context for per-target patches
        run_id: Optional[str] = None  # NEW: for evidence tracking
    ) -> Tuple[Dict[str, Any], AutoFixInfo]:
        """
        Apply detected fixes to config files.
        
        Args:
            detections: List of detected leaks
            min_confidence: Minimum confidence to auto-fix (default: 0.7)
            max_features: Maximum number of features to fix per run (default: None = no limit)
            dry_run: If True, don't actually modify files, just return what would be done
        
        Returns:
            Tuple of (updates_dict, AutoFixInfo) where:
            - updates_dict: Dict with 'excluded_features_updates' and 'feature_registry_updates'
            - AutoFixInfo: Information about what was modified
        """
        # Load from config if not provided
        if min_confidence is None:
            try:
                from CONFIG.config_loader import get_safety_config
                safety_cfg = get_safety_config()
                # safety_config.yaml has a top-level 'safety' key
                safety_section = safety_cfg.get('safety', {})
                leakage_cfg = safety_section.get('leakage_detection', {})
                auto_fixer_cfg = leakage_cfg.get('auto_fixer', {})
                min_confidence = float(auto_fixer_cfg.get('min_confidence', 0.7))
            except Exception:
                min_confidence = 0.7
        
        if max_backups_per_target is None:
            max_backups_per_target = self.max_backups_per_target
        
        # Filter by confidence
        high_confidence = [d for d in detections if d.confidence >= min_confidence]
        low_confidence = [d for d in detections if d.confidence < min_confidence]
        
        # Log confidence distribution for visibility
        if detections:
            logger.info(f"üìä Detection confidence distribution:")
            logger.info(f"   Total detections: {len(detections)}")
            logger.info(f"   High confidence (>= {min_confidence}): {len(high_confidence)}")
            if low_confidence:
                top_low = sorted(low_confidence, key=lambda x: x.confidence, reverse=True)[:3]
                logger.warning(f"   Low confidence (< {min_confidence}): {len(low_confidence)}")
                logger.warning(f"   Top low-confidence (not fixed): {', '.join([f'{d.feature_name} (conf={d.confidence:.2f})' for d in top_low])}")
            if high_confidence:
                top_high = sorted(high_confidence, key=lambda x: x.confidence, reverse=True)[:3]
                logger.info(f"   Top high-confidence (will fix): {', '.join([f'{d.feature_name} (conf={d.confidence:.2f})' for d in top_high])}")
        
        # Backup configs BEFORE checking if there are leaks to fix
        # This ensures we have a backup even when auto-fix mode is enabled but no leaks detected
        backup_files = []
        if self.backup_configs and not dry_run:
            # Use provided max_backups or fall back to instance config
            backup_max = max_backups_per_target if max_backups_per_target is not None else self.max_backups_per_target
            backup_files = self._backup_configs(
                target=target,
                max_backups_per_target=backup_max
            )
            if not high_confidence:
                logger.info(
                    f"Created backup (no leaks detected with confidence >= {min_confidence}): "
                    f"{len(backup_files)} backup files"
                )
        
        if not high_confidence:
            if detections:
                logger.warning(f"‚ö†Ô∏è  {len(detections)} leaks detected but ALL below confidence threshold ({min_confidence})")
                logger.warning(f"   Consider lowering min_confidence or investigating why confidence is low")
            else:
                logger.info(f"No leaks detected with confidence >= {min_confidence}")
            empty_autofix_info = AutoFixInfo(
                modified_configs=False,
                modified_files=[],
                modified_features=[],
                excluded_features_updates={},
                feature_registry_updates={},
                backup_files=backup_files  # Include backup files even when no leaks
            )
            return {'excluded_features_updates': {}, 'feature_registry_updates': {}}, empty_autofix_info
        
        # Sort by confidence (descending) and limit to max_features
        high_confidence.sort(key=lambda x: x.confidence, reverse=True)
        if max_features is not None and len(high_confidence) > max_features:
            logger.info(
                f"Limiting auto-fix to top {max_features} features (by confidence) "
                f"out of {len(high_confidence)} detected leaks"
            )
            high_confidence = high_confidence[:max_features]
        
        logger.info(
            f"Auto-fixing {len(high_confidence)} leaks "
            f"(confidence >= {min_confidence}, max_features={max_features})"
        )
        
        # Group by action type
        exact_matches = []
        prefix_patterns = set()
        
        for det in high_confidence:
            if det.suggested_action == 'exact':
                exact_matches.append(det.feature_name)
            elif det.suggested_action == 'prefix':
                # Extract prefix
                if det.feature_name.startswith('p_'):
                    prefix_patterns.add('p_')
                elif det.feature_name.startswith('y_'):
                    prefix_patterns.add('y_')
                elif det.feature_name.startswith('fwd_ret_'):
                    prefix_patterns.add('fwd_ret_')
                else:
                    # Fallback to exact
                    exact_matches.append(det.feature_name)
        
        updates = {
            'excluded_features_updates': {
                'exact_patterns': exact_matches,
                'prefix_patterns': list(prefix_patterns)
            },
            'feature_registry_updates': {
                'rejected_features': [d.feature_name for d in high_confidence],
                'detections': high_confidence  # NEW: include detections for evidence
            }
        }
        
        modified_files = []
        
        if not dry_run:
            self._apply_excluded_features_updates(updates['excluded_features_updates'])
            # Pass target_ctx and run_id for per-target patches
            # Note: target_ctx and run_id are passed as parameters to apply_fixes()
            registry_updates = self._apply_feature_registry_updates(
                updates['feature_registry_updates'],
                target_ctx=target_ctx,  # From apply_fixes() parameter
                run_id=run_id  # From apply_fixes() parameter
            )
            # Store written patch paths in updates for caller
            if registry_updates:
                updates['feature_registry_updates']['written_patches'] = registry_updates
            
            # Invalidate cache so next detection reloads excluded features
            self._excluded_features_cache = None
            self._excluded_prefixes_cache = None
            
            # Invalidate leakage_filtering cache so reruns pick up new exclusions immediately
            try:
                from TRAINING.ranking.utils.leakage_filtering import reload_feature_configs
                reload_feature_configs()
                # Get hash for logging (compute from file we just wrote)
                from TRAINING.common.utils.config_hashing import compute_config_hash_from_file
                if self.excluded_features_path.exists():
                    config_hash = compute_config_hash_from_file(self.excluded_features_path, short=False)
                    logger.debug(f"Invalidated leakage_filtering cache after updating excluded_features.yaml (hash: {config_hash[:16]})")
            except ImportError:
                logger.warning("Could not import reload_feature_configs - cache may not be invalidated immediately")
            except Exception as e:
                logger.debug(f"Could not invalidate leakage_filtering cache: {e}")
            
            # Track which files were modified
            if updates['excluded_features_updates'].get('exact_patterns') or updates['excluded_features_updates'].get('prefix_patterns'):
                modified_files.append(str(self.excluded_features_path))
            if updates['feature_registry_updates'].get('rejected_features'):
                modified_files.append(str(self.feature_registry_path))
            
            # Backup files are already created by _backup_configs() above
        
        # Create AutoFixInfo
        autofix_info = AutoFixInfo(
            modified_configs=len(modified_files) > 0,
            modified_files=modified_files,
            modified_features=[d.feature_name for d in high_confidence],
            excluded_features_updates=updates['excluded_features_updates'],
            feature_registry_updates=updates['feature_registry_updates'],
            backup_files=backup_files if backup_files else None
        )
        
        return updates, autofix_info
    
    def _load_backup_config(self) -> int:
        """Load backup configuration from safety_config.yaml."""
        default_max_backups = 20
        try:
            from CONFIG.config_loader import get_safety_config
            safety_cfg = get_safety_config()
            # safety_config.yaml has a top-level 'safety' key
            safety_section = safety_cfg.get('safety', {})
            leakage_cfg = safety_section.get('leakage_detection', {})
            auto_fixer_cfg = leakage_cfg.get('auto_fixer', {})
            max_backups = auto_fixer_cfg.get('max_backups_per_target', default_max_backups)
            return int(max_backups) if max_backups is not None else default_max_backups
        except Exception as e:
            logger.debug(f"Could not load backup config: {e}, using default {default_max_backups}")
            # Fallback to system_config if available
            if _CONFIG_AVAILABLE:
                try:
                    system_cfg = get_system_config()
                    backup_cfg = system_cfg.get('system', {}).get('backup', {})
                    max_backups = backup_cfg.get('max_backups_per_target', default_max_backups)
                    return int(max_backups) if max_backups is not None else default_max_backups
                except Exception:
                    pass
        return default_max_backups
    
    def _get_git_commit_hash(self) -> Optional[str]:
        """Get current git commit hash. Delegates to SST module."""
        from TRAINING.common.utils.git_utils import get_git_commit
        return get_git_commit(short=True)
    
    def _backup_configs(self, target: Optional[str] = None, max_backups_per_target: Optional[int] = None):
        """
        Backup config files before modification.
        
        Uses timestamp subdirectory structure:
        - With target: CONFIG/backups/{target}/{timestamp}/files + manifest.json
        - Without target: CONFIG/backups/{timestamp}/files (legacy flat mode)
        
        Args:
            target: Optional target name to organize backups per-target.
                        If provided, backups are stored in CONFIG/backups/{target}/{timestamp}/
            max_backups_per_target: Maximum number of backups to keep per target 
                                   (None = use config/default, 0 = no limit)
        
        Returns:
            List of backup file paths
        """
        # Safety check: don't create backups if backup_configs is False
        if not self.backup_configs:
            logger.debug("Backups disabled (backup_configs=False), skipping backup creation")
            return []
        
        import shutil
        from datetime import datetime
        import json
        
        # Use configured backup directory (set in __init__)
        base_backup_dir = self.backup_dir
        
        # Use config value if not explicitly provided
        if max_backups_per_target is None:
            max_backups_per_target = self.max_backups_per_target
        
        # Generate high-resolution timestamp to avoid collisions
        now = datetime.now()
        timestamp = now.strftime("%Y%m%d_%H%M%S_%f")  # Include microseconds
        
        # Organize backups
        # If using run-based backups (output_dir provided), organize by target within run directory
        # Otherwise, use CONFIG/backups/ structure
        if self._use_run_backups:
            # Backups are in run directory, so organize by target
            if target:
                # Sanitize target name for filesystem (remove invalid chars)
                safe_target = "".join(c for c in target if c.isalnum() or c in ('_', '-', '.'))[:50]
                target_backup_dir = base_backup_dir / safe_target
                snapshot_dir = target_backup_dir / timestamp
            else:
                # No target name - use flat structure in run directory
                snapshot_dir = base_backup_dir / timestamp
        else:
            # Legacy CONFIG/backups/ structure - organize by target if provided
            if target:
                # Sanitize target name for filesystem (remove invalid chars)
                safe_target = "".join(c for c in target if c.isalnum() or c in ('_', '-', '.'))[:50]
                target_backup_dir = base_backup_dir / safe_target
                snapshot_dir = target_backup_dir / timestamp
            else:
                # Legacy flat mode (warn about this)
                logger.warning(
                    "Backup created with no target; using legacy flat layout. "
                    "Consider passing target for better organization."
                )
                snapshot_dir = base_backup_dir / timestamp
        
        snapshot_dir.mkdir(parents=True, exist_ok=True)
        
        backup_files = []
        
        # Copy config files to snapshot directory
        if self.excluded_features_path.exists():
            backup_path = snapshot_dir / "excluded_features.yaml"
            shutil.copy2(self.excluded_features_path, backup_path)
            backup_files.append(str(backup_path))
            logger.info(f"Backed up excluded_features.yaml to {backup_path}")
        
        if self.feature_registry_path.exists():
            backup_path = snapshot_dir / "feature_registry.yaml"
            shutil.copy2(self.feature_registry_path, backup_path)
            backup_files.append(str(backup_path))
            logger.info(f"Backed up feature_registry.yaml to {backup_path}")
        
        # Log backup creation with full context
        git_commit = self._get_git_commit_hash()
        logger.info(
            f"üì¶ Backup created: target={target or 'N/A'}, "
            f"timestamp={timestamp}, git_commit={git_commit or 'N/A'}, "
            f"source=auto_fix_leakage"
        )
        
        # Create manifest file
        manifest_path = snapshot_dir / "manifest.json"
        try:
            manifest = {
                "backup_version": 1,
                "source": "auto_fix_leakage",
                "target": target,
                "timestamp": timestamp,
                "backup_files": backup_files,
                "excluded_features_path": str(self.excluded_features_path),
                "feature_registry_path": str(self.feature_registry_path),
                "git_commit": self._get_git_commit_hash()
            }
            with open(manifest_path, 'w') as f:
                json.dump(manifest, f, indent=2)
            backup_files.append(str(manifest_path))
        except Exception as e:
            logger.debug(f"Could not create manifest file: {e}")
        
        # Apply retention policy (prune old backups for this target)
        if target and max_backups_per_target > 0:
            pruned_count = self._prune_old_backups(target_backup_dir, max_backups_per_target)
            if pruned_count > 0:
                logger.info(
                    f"üßπ Pruned {pruned_count} old backup(s) for target={target} "
                    f"(kept {max_backups_per_target} most recent)"
                )
        
        return backup_files
    
    def _prune_old_backups(self, target_backup_dir: Path, max_backups: int) -> int:
        """
        Prune old backups for a target, keeping only the most recent N.
        
        Args:
            target_backup_dir: Directory containing backups for a target
            max_backups: Maximum number of backups to keep
        
        Returns:
            Number of backups pruned
        """
        if not target_backup_dir.exists():
            return 0
        
        try:
            # Get all timestamp subdirectories
            # DETERMINISM: Use iterdir_sorted for deterministic iteration order
            backup_dirs = [
                d for d in iterdir_sorted(target_backup_dir)
                if d.is_dir() and d.name.replace('_', '').replace('.', '').isdigit()
            ]
            # Sort by name for consistent ordering (already sorted by iterdir_sorted, but explicit for clarity)
            backup_dirs.sort(key=lambda x: x.name)
            
            if len(backup_dirs) <= max_backups:
                return 0  # No pruning needed
            
            # Sort by timestamp (directory name is timestamp)
            backup_dirs.sort(key=lambda d: d.name, reverse=True)
            
            # Remove oldest backups
            to_remove = backup_dirs[max_backups:]
            pruned_count = 0
            for old_backup in to_remove:
                try:
                    import shutil
                    shutil.rmtree(old_backup)
                    pruned_count += 1
                    logger.debug(f"Pruned old backup: {old_backup.name}")
                except Exception as e:
                    logger.warning(f"Could not prune backup {old_backup}: {e}")
            
            return pruned_count
        except Exception as e:
            logger.debug(f"Could not prune backups: {e}")
            return 0
    
    def _apply_excluded_features_updates(self, updates: Dict[str, Any]):
        """Apply updates to excluded_features.yaml."""
        from TRAINING.common.utils.config_hashing import compute_config_hash_from_file
        
        # Compute hash before writing (if file exists)
        hash_before = None
        if self.excluded_features_path.exists():
            hash_before = compute_config_hash_from_file(self.excluded_features_path, short=False)
            logger.debug(f"excluded_features.yaml hash before update: {hash_before[:16]}...")
        
        if not self.excluded_features_path.exists():
            logger.warning(f"excluded_features.yaml not found at {self.excluded_features_path}, creating new file")
            config = {
                'always_exclude': {
                    'regex_patterns': [],
                    'prefix_patterns': [],
                    'keyword_patterns': [],
                    'exact_patterns': []
                }
            }
        else:
            with open(self.excluded_features_path, 'r') as f:
                config = yaml.safe_load(f) or {}
        
        # Ensure structure exists
        if 'always_exclude' not in config:
            config['always_exclude'] = {}
        
        always_exclude = config['always_exclude']
        
        # Add exact patterns
        existing_exact = set(always_exclude.get('exact_patterns', []))
        new_exact = set(updates.get('exact_patterns', []))
        always_exclude['exact_patterns'] = sorted(list(existing_exact | new_exact))
        
        # Add prefix patterns
        existing_prefix = set(always_exclude.get('prefix_patterns', []))
        new_prefix = set(updates.get('prefix_patterns', []))
        always_exclude['prefix_patterns'] = sorted(list(existing_prefix | new_prefix))
        
        # Write back atomically with canonical serialization for crash safety and determinism
        from TRAINING.common.utils.file_utils import write_atomic_yaml
        write_atomic_yaml(self.excluded_features_path, config)
        
        # Compute hash after writing
        hash_after = compute_config_hash_from_file(self.excluded_features_path, short=False)
        logger.info(
            f"Updated {self.excluded_features_path}: added {len(new_exact)} exact patterns, "
            f"{len(new_prefix)} prefix patterns. Hash: {hash_before[:16] if hash_before else 'new'} -> {hash_after[:16]}"
        )
    
    def _is_structural_leak(self, feature_name: str, detection: Optional[LeakageDetection] = None) -> bool:
        """
        Detect structural leaks (always leaky, not horizon-specific).
        
        Structural leaks are features that are inherently forward-looking or
        otherwise fundamentally leaky (e.g., fwd_ret_*, tth_*).
        
        Args:
            feature_name: Feature name to check
            detection: Optional detection object for enhanced checking
        
        Returns:
            True if feature should be globally rejected, False otherwise
        """
        # Baseline: Regex patterns for known structural leaks
        structural_patterns = [
            r'^fwd_ret_',  # Forward returns
            r'^tth_',      # Time to hit
            r'^mfe_',      # Maximum favorable excursion
            r'^mdd_',     # Maximum drawdown
            r'^barrier_', # Barrier touches
        ]
        
        if any(re.match(pattern, feature_name) for pattern in structural_patterns):
            return True
        
        # Enhanced: Detection metadata (if available)
        if detection:
            # High confidence + perfect score indicates structural leak
            if detection.confidence > 0.95 and detection.source == "perfect_score":
                # Additional checks could be added here (time alignment, forward-window overlap)
                # For now, rely on regex patterns as baseline
                pass
        
        return False
    
    def _write_per_target_patch(
        self,
        target_ctx: TargetContext,
        leaky_features: List[str],
        detections: List[LeakageDetection],
        run_id: Optional[str] = None
    ) -> Optional[Path]:
        """
        Write per-target patch with atomic write and file locking.
        
        Implements monotonic union-only semantics (excluded_horizons_bars only grows).
        Uses shared lockfile with lock spanning entire read‚Üímerge‚Üíwrite‚Üírename cycle.
        
        Args:
            target_ctx: Target context (target, horizon, bar_minutes)
            leaky_features: List of feature names to exclude for this horizon
            detections: List of LeakageDetection objects (for evidence)
            run_id: Optional run ID for evidence tracking
        
        Returns:
            Path to written patch file, or None if failed
        """
        if not self.output_dir:
            logger.warning("No output_dir set, cannot write patch")
            return None
        
        patch_dir = self.output_dir / "registry_patches"
        patch_dir.mkdir(parents=True, exist_ok=True)
        
        # Collision-proof filename
        safe_target = safe_target_filename(target_ctx.target)
        patch_file = patch_dir / safe_target
        lock_file = patch_file.with_suffix('.yaml.lock')
        temp_file = patch_file.with_suffix('.yaml.tmp')
        
        # Acquire exclusive lock on shared lockfile (blocks until available)
        with open(lock_file, 'w') as lock_f:
            fcntl.flock(lock_f.fileno(), fcntl.LOCK_EX)
            try:
                # READ existing patch (inside lock)
                existing_patch = {}
                if patch_file.exists():
                    try:
                        with open(patch_file, 'r') as f:
                            existing_patch = yaml.safe_load(f) or {}
                    except Exception as e:
                        logger.warning(f"Failed to read existing patch {patch_file}: {e}")
                
                # MERGE (union-only, monotonic)
                patch_features = existing_patch.get('features', {})
                for feat_name in leaky_features:
                    # Skip structural leaks (handled in global registry if needed)
                    if self._is_structural_leak(feat_name):
                        continue
                    
                    if feat_name not in patch_features:
                        patch_features[feat_name] = {
                            'excluded_horizons_bars': []
                            # NO evidence, last_run_id, reason, method, confidence in patch file
                            # Patches are policy-only for determinism
                        }
                    
                    excluded = patch_features[feat_name].get('excluded_horizons_bars', [])
                    if target_ctx.horizon_bars not in excluded:
                        excluded.append(target_ctx.horizon_bars)
                        excluded.sort()  # Deterministic sort
                        patch_features[feat_name]['excluded_horizons_bars'] = excluded
                    
                    # Write evidence to audit log instead (not in patch file)
                    if run_id:
                        detection = None
                        if detections:
                            detection = next(
                                (d for d in detections if d.feature_name == feat_name),
                                None
                            )
                        self._write_audit_entry(
                            action='exclude',
                            target=target_ctx.target,
                            details={
                                'feature': feat_name,
                                'horizon_bars': target_ctx.horizon_bars,
                                'run_id': run_id,  # Evidence goes to audit log
                                'method': detection.source if detection else 'unknown',
                                'confidence': detection.confidence if detection else None,
                                'reason': detection.reason if detection else "AUTO: leakage detected"
                            }
                        )
                
                # Update metadata
                existing_patch['target'] = target_ctx.target
                existing_patch['horizon_minutes'] = target_ctx.horizon_minutes
                existing_patch['horizon_bars'] = target_ctx.horizon_bars
                existing_patch['bar_minutes'] = int(target_ctx.bar_minutes)  # Always int
                existing_patch['features'] = dict(sorted(patch_features.items()))  # Deterministic
                
                # WRITE temp file (inside lock) with canonical YAML serialization
                from TRAINING.common.utils.determinism_serialization import canonical_yaml
                yaml_bytes = canonical_yaml(existing_patch)
                with open(temp_file, 'wb') as f:
                    f.write(yaml_bytes)
                    f.flush()
                    os.fsync(f.fileno())
                
                # RENAME (atomic, inside lock)
                temp_file.replace(patch_file)
            finally:
                fcntl.flock(lock_f.fileno(), fcntl.LOCK_UN)
        
        # Keep lockfile (don't delete - lockfiles are cheap)
        logger.info(f"‚úÖ Wrote registry patch: {patch_file} ({len(leaky_features)} features)")
        return patch_file
    
    def _write_audit_entry(
        self,
        action: str,
        target: str,
        details: Dict[str, Any],
        audit_dir: Optional[Path] = None
    ) -> None:
        """
        Write audit entry with atomic append and file locking.
        
        Uses JSONL format (one JSON object per line).
        Lockfile ensures concurrent writes are safe.
        
        Args:
            action: Action type (e.g., 'exclude', 'promote', 'unblock')
            target: Target column name
            details: Additional details dict
            audit_dir: Optional audit directory (default: RESULTS/audit/)
        """
        if audit_dir is None:
            # Use RESULTS/audit/ (not CONFIG/)
            repo_root = Path(__file__).resolve().parents[2]
            audit_dir = repo_root / "RESULTS" / "audit"
        
        audit_dir.mkdir(parents=True, exist_ok=True)
        audit_file = audit_dir / "registry_patch_ops.jsonl"
        lock_file = audit_file.with_suffix('.jsonl.lock')
        
        entry = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',  # ISO 8601
            'action': action,
            'target': target,
            **details
        }
        
        # Atomic append with lock
        with open(lock_file, 'w') as lock_f:
            fcntl.flock(lock_f.fileno(), fcntl.LOCK_EX)
            try:
                with open(audit_file, 'a') as f:
                    # DETERMINISM: Use sort_keys for reproducible JSONL
                    f.write(json.dumps(entry, sort_keys=True) + '\n')
                    f.flush()
                    os.fsync(f.fileno())
            finally:
                fcntl.flock(lock_f.fileno(), fcntl.LOCK_UN)
        
        # Keep lockfile (don't delete)
    
    def promote_patches_from_config(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Promote patches from output_dir to persistent CONFIG/data/feature_registry_per_target/.
        
        Reads config['promotion'] section:
        - enabled: bool
        - output_dirs: List[str] - directories to scan for patches
        - targets: Optional[List[str]] - specific targets to promote (None = all)
        - review_mode: bool - if True, log but don't write
        
        Args:
            config: Full registry_patches.yaml config dict
        
        Returns:
            List of promotion results (one per promoted patch)
        """
        promotion_cfg = config.get('promotion', {})
        if not promotion_cfg.get('enabled', False):
            return []
        
        output_dirs = promotion_cfg.get('output_dirs', [])
        target_filter = promotion_cfg.get('targets')  # None = all targets
        review_mode = promotion_cfg.get('review_mode', False)
        
        promotions = []
        repo_root = Path(__file__).resolve().parents[2]
        persistent_dir = repo_root / "CONFIG" / "data" / "feature_registry_per_target"
        persistent_dir.mkdir(parents=True, exist_ok=True)
        
        for output_dir_str in output_dirs:
            output_dir = Path(output_dir_str)
            if not output_dir.is_absolute():
                output_dir = repo_root / output_dir
            
            patch_dir = output_dir / "registry_patches"
            if not patch_dir.exists():
                logger.warning(f"Patch directory not found: {patch_dir}")
                continue
            
            # Find all patch files
            # DETERMINISTIC: Use glob_sorted for deterministic iteration with relative paths
            from TRAINING.common.utils.determinism_ordering import glob_sorted
            patch_files = glob_sorted(patch_dir, "*__*.yaml", filter_fn=lambda p: p.suffix == ".yaml" and not p.name.endswith(".unblock.yaml"))
            for patch_file in patch_files:
                
                try:
                    with open(patch_file, 'r') as f:
                        patch_data = yaml.safe_load(f) or {}
                    
                    target = patch_data.get('target')
                    if not target:
                        continue
                    
                    # Filter by target if specified
                    if target_filter and target not in target_filter:
                        continue
                    
                    # Generate persistent filename
                    persistent_file = persistent_dir / safe_target_filename(target)
                    
                    # Merge with existing persistent patch (union-only)
                    existing_data = {}
                    if persistent_file.exists():
                        with open(persistent_file, 'r') as f:
                            existing_data = yaml.safe_load(f) or {}
                    
                    # Merge features (union-only, monotonic)
                    existing_features = existing_data.get('features', {})
                    patch_features = patch_data.get('features', {})
                    
                    for feat_name, feat_data in patch_features.items():
                        if feat_name not in existing_features:
                            existing_features[feat_name] = {
                                'excluded_horizons_bars': []
                            }
                        
                        # Merge excluded_horizons_bars (union)
                        existing_horizons = set(existing_features[feat_name].get('excluded_horizons_bars', []))
                        new_horizons = set(feat_data.get('excluded_horizons_bars', []))
                        existing_features[feat_name]['excluded_horizons_bars'] = sorted(existing_horizons | new_horizons)
                    
                    # Update metadata
                    merged_patch = {
                        'target': target,
                        'bar_minutes': patch_data.get('bar_minutes'),
                        'horizon_minutes': patch_data.get('horizon_minutes'),
                        'horizon_bars': patch_data.get('horizon_bars'),
                        'features': dict(sorted(existing_features.items()))  # Deterministic
                    }
                    
                    if not review_mode:
                        # Write persistent patch (atomic)
                        temp_file = persistent_file.with_suffix('.yaml.tmp')
                        with open(temp_file, 'w') as f:
                            yaml.dump(merged_patch, f, default_flow_style=False, sort_keys=True)
                            f.flush()
                            os.fsync(f.fileno())
                        temp_file.replace(persistent_file)
                        
                        # Write audit entry
                        self._write_audit_entry(
                            action='promote',
                            target=target,
                            details={
                                'source_patch': str(patch_file),
                                'persistent_file': str(persistent_file),
                                'features_count': len(existing_features),
                                'promoted_by': 'config_driven'
                            }
                        )
                    
                    promotions.append({
                        'target': target,
                        'source_patch': str(patch_file),
                        'persistent_file': str(persistent_file),
                        'features_count': len(existing_features),
                        'review_mode': review_mode
                    })
                    
                    logger.info(f"{'[REVIEW] ' if review_mode else ''}Promoted patch for {target}: {len(existing_features)} features")
                
                except Exception as e:
                    logger.error(f"Failed to promote patch {patch_file}: {e}")
                    continue
        
        return promotions
    
    def apply_unblocks_from_config(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Apply unblocks from config to persistent unblock files.
        
        Reads config['unblocking'] section:
        - enabled: bool
        - unblocks: List[Dict] with:
          - target: str
          - features: Dict[str, List[int]] - feature -> unblocked_horizons_bars
          - reason: Optional[str]
        
        Args:
            config: Full registry_patches.yaml config dict
        
        Returns:
            List of unblock results (one per unblocked target)
        """
        unblock_cfg = config.get('unblocking', {})
        if not unblock_cfg.get('enabled', False):
            return []
        
        unblocks_list = unblock_cfg.get('unblocks', [])
        unblock_results = []
        
        repo_root = Path(__file__).resolve().parents[2]
        persistent_dir = repo_root / "CONFIG" / "data" / "feature_registry_per_target"
        persistent_dir.mkdir(parents=True, exist_ok=True)
        
        for unblock_spec in unblocks_list:
            target = unblock_spec.get('target')
            if not target:
                logger.warning("Unblock spec missing target, skipping")
                continue
            
            unblock_features = unblock_spec.get('features', {})
            reason = unblock_spec.get('reason', 'Manual unblock')
            
            # Generate unblock filename
            unblock_file = persistent_dir / safe_target_filename(target, suffix=".unblock.yaml")
            
            # Load existing unblock (if any)
            existing_data = {}
            if unblock_file.exists():
                with open(unblock_file, 'r') as f:
                    existing_data = yaml.safe_load(f) or {}
            
            # Merge unblocks (union-only, monotonic)
            existing_unblocks = existing_data.get('features', {})
            
            for feat_name, unblocked_horizons in unblock_features.items():
                if feat_name not in existing_unblocks:
                    existing_unblocks[feat_name] = {
                        'unblocked_horizons_bars': []
                    }
                
                # Merge unblocked_horizons_bars (union)
                existing_horizons = set(existing_unblocks[feat_name].get('unblocked_horizons_bars', []))
                new_horizons = set(unblocked_horizons)
                existing_unblocks[feat_name]['unblocked_horizons_bars'] = sorted(existing_horizons | new_horizons)
            
            # Write unblock file (policy-only, atomic)
            unblock_patch = {
                'target': target,
                'bar_minutes': unblock_spec.get('bar_minutes'),  # Optional compatibility metadata
                'features': dict(sorted(existing_unblocks.items()))  # Deterministic
            }
            
            temp_file = unblock_file.with_suffix('.unblock.yaml.tmp')
            with open(temp_file, 'w') as f:
                yaml.dump(unblock_patch, f, default_flow_style=False, sort_keys=True)
                f.flush()
                os.fsync(f.fileno())
            temp_file.replace(unblock_file)
            
            # Write audit entry
            self._write_audit_entry(
                action='unblock',
                target=target,
                details={
                    'unblock_file': str(unblock_file),
                    'features_count': len(existing_unblocks),
                    'reason': reason
                }
            )
            
            unblock_results.append({
                'target': target,
                'unblock_file': str(unblock_file),
                'features_count': len(existing_unblocks),
                'reason': reason
            })
            
            logger.info(f"Applied unblock for {target}: {len(existing_unblocks)} features")
        
        return unblock_results
    
    def _apply_feature_registry_updates(
        self,
        updates: Dict[str, Any],
        target_ctx: Optional[TargetContext] = None,
        run_id: Optional[str] = None
    ) -> Dict[str, Path]:
        """
        Apply registry updates as patches to output_dir.
        
        Writes per-target patches (always to output_dir).
        Only writes to global registry for structural leaks (opt-in, behind flag).
        
        Args:
            updates: Dict with 'rejected_features' and 'detections' keys
            target_ctx: Optional TargetContext for per-target patches
            run_id: Optional run ID for evidence tracking
        
        Returns:
            Dict mapping target -> patch_file_path
        """
        written_patches = {}
        
        # Separate structural leaks from horizon-specific leaks
        rejected_features = updates.get('rejected_features', [])
        detections = updates.get('detections', [])
        
        structural_leaks = [
            feat for feat in rejected_features
            if self._is_structural_leak(feat, next((d for d in detections if d.feature_name == feat), None))
        ]
        
        # Only write global registry for structural leaks (and only if flag set)
        # For now, skip global registry writes (patches are the primary mechanism)
        # TODO: Add opt-in flag for global registry writes if needed
        
        # Write per-target patches (always, to output_dir)
        if target_ctx:
            patch_file = self._write_per_target_patch(
                target_ctx=target_ctx,
                leaky_features=rejected_features,
                detections=detections,
                run_id=run_id
            )
            if patch_file:
                written_patches[target_ctx.target] = patch_file
        
        return written_patches
    
    def run_auto_fix_loop(
        self,
        training_function,  # Function that runs training and returns (X, y, feature_names, model_importance, scores)
        max_iterations: int = 5,
        min_confidence: Optional[float] = None,  # Load from config if None
        target_column: str = None,
        **training_kwargs
    ) -> Dict[str, Any]:
        """
        Run training in a loop, detecting and fixing leaks until clean.
        
        Args:
            training_function: Function that runs training and returns results
            max_iterations: Maximum number of fix iterations
            min_confidence: Minimum confidence to auto-fix (loads from config if None)
            target_column: Target column name
            **training_kwargs: Additional arguments to pass to training function
        
        Returns:
            Dict with final results and fix history
        """
        # Load min_confidence from config if not provided
        if min_confidence is None:
            try:
                from CONFIG.config_loader import get_safety_config
                safety_cfg = get_safety_config()
                # safety_config.yaml has a top-level 'safety' key
                safety_section = safety_cfg.get('safety', {})
                leakage_cfg = safety_section.get('leakage_detection', {})
                auto_fixer_cfg = leakage_cfg.get('auto_fixer', {})
                min_confidence = float(auto_fixer_cfg.get('min_confidence', 0.7))
            except Exception:
                min_confidence = 0.7
        
        self.iteration_count = 0
        fix_history = []
        
        for iteration in range(max_iterations):
            self.iteration_count = iteration + 1
            logger.info(f"\n{'='*70}")
            logger.info(f"Auto-Fix Iteration {self.iteration_count}/{max_iterations}")
            logger.info(f"{'='*70}")
            
            # Run training
            logger.info("Running training...")
            training_results = training_function(**training_kwargs)
            
            # Extract results
            X = training_results.get('X')
            y = training_results.get('y')
            feature_names = training_results.get('feature_names', [])
            model_importance = training_results.get('model_importance', {})
            train_score = training_results.get('train_score')
            test_score = training_results.get('test_score')
            symbols = training_results.get('symbols')
            task_type = training_results.get('task_type', 'classification')
            data_interval_minutes = training_results.get('data_interval_minutes', 5)
            
            # Detect leaks
            logger.info("Detecting leaking features...")
            detections = self.detect_leaking_features(
                X=X, y=y, feature_names=feature_names,
                target_column=target_column or training_results.get('target_column', 'unknown'),
                symbols=symbols, task_type=task_type,
                data_interval_minutes=data_interval_minutes,
                model_importance=model_importance,
                train_score=train_score, test_score=test_score
            )
            
            if not detections:
                logger.info("‚úÖ No leaks detected! Training is clean.")
                return {
                    'success': True,
                    'iterations': self.iteration_count,
                    'fix_history': fix_history,
                    'final_results': training_results
                }
            
            # Check if we've seen these leaks before (avoid infinite loop)
            leak_names = {d.feature_name for d in detections}
            if leak_names.issubset(set(self.detected_leaks.keys())):
                logger.warning(f"‚ö†Ô∏è  Same leaks detected again - may need manual intervention")
                logger.warning(f"   Detected: {leak_names}")
                break
            
            # Record detected leaks
            for det in detections:
                self.detected_leaks[det.feature_name] = det
            
            # Apply fixes
            logger.info(f"Applying fixes for {len(detections)} leaks...")
            updates = self.apply_fixes(
                detections, 
                min_confidence=min_confidence, 
                dry_run=False,
                target=target_column,  # Use target_column from training_kwargs if available
                max_backups_per_target=None  # Use instance config
            )
            
            fix_history.append({
                'iteration': self.iteration_count,
                'detections': [d.__dict__ for d in detections],
                'updates': updates
            })
            
            logger.info(f"‚úÖ Applied fixes. Re-running training in next iteration...")
        
        logger.warning(f"‚ö†Ô∏è  Reached max iterations ({max_iterations}). Some leaks may remain.")
        return {
            'success': False,
            'iterations': self.iteration_count,
            'fix_history': fix_history,
            'remaining_leaks': list(self.detected_leaks.keys())
        }
    
    @staticmethod
    def list_backups(target: Optional[str] = None, backup_dir: Optional[Path] = None) -> List[Dict[str, Any]]:
        """
        List available backups for a target (or all targets if None).
        
        Args:
            target: Target name to list backups for (None = all targets)
            backup_dir: Backup directory (default: CONFIG/backups)
        
        Returns:
            List of backup info dicts with: target, timestamp, manifest_path, snapshot_dir
        """
        import json
        
        if backup_dir is None:
            backup_dir = _REPO_ROOT / "CONFIG" / "backups"
        
        if not backup_dir.exists():
            return []
        
        backups = []
        
        if target:
            # List backups for specific target
            safe_target = "".join(c for c in target if c.isalnum() or c in ('_', '-', '.'))[:50]
            target_dir = backup_dir / safe_target
            if target_dir.exists():
                # DETERMINISTIC: Sort iterdir() results for consistent iteration order
                for snapshot_dir in sorted(target_dir.iterdir(), key=lambda x: x.name):
                    if snapshot_dir.is_dir():
                        manifest_path = snapshot_dir / "manifest.json"
                        if manifest_path.exists():
                            try:
                                with open(manifest_path, 'r') as f:
                                    manifest = json.load(f)
                                backups.append({
                                    'target': manifest.get('target'),
                                    'timestamp': manifest.get('timestamp'),
                                    'manifest_path': str(manifest_path),
                                    'snapshot_dir': str(snapshot_dir),
                                    'git_commit': manifest.get('git_commit'),
                                    'source': manifest.get('source')
                                })
                            except Exception:
                                pass
        else:
            # List all backups across all targets
            # DETERMINISTIC: Sort iterdir() results for consistent iteration order
            for target_dir in sorted(backup_dir.iterdir(), key=lambda x: x.name):
                if target_dir.is_dir():
                    # DETERMINISTIC: Sort nested iterdir() results
                    for snapshot_dir in sorted(target_dir.iterdir(), key=lambda x: x.name):
                        if snapshot_dir.is_dir():
                            manifest_path = snapshot_dir / "manifest.json"
                            if manifest_path.exists():
                                try:
                                    with open(manifest_path, 'r') as f:
                                        manifest = json.load(f)
                                    backups.append({
                                        'target': manifest.get('target'),
                                        'timestamp': manifest.get('timestamp'),
                                        'manifest_path': str(manifest_path),
                                        'snapshot_dir': str(snapshot_dir),
                                        'git_commit': manifest.get('git_commit'),
                                        'source': manifest.get('source')
                                    })
                                except Exception:
                                    pass
        
        # Sort by timestamp (newest first)
        backups.sort(key=lambda b: b.get('timestamp', ''), reverse=True)
        return backups
    
    @staticmethod
    def restore_backup(
        target: str,
        timestamp: Optional[str] = None,
        backup_dir: Optional[Path] = None,
        dry_run: bool = False
    ) -> bool:
        """
        Restore config files from a backup.
        
        Args:
            target: Target name
            timestamp: Timestamp of backup to restore (None = most recent)
            backup_dir: Backup directory (default: CONFIG/backups)
            dry_run: If True, only show what would be restored without actually restoring
        
        Returns:
            True if restore succeeded, False otherwise
        """
        import shutil
        import json
        
        if backup_dir is None:
            backup_dir = _REPO_ROOT / "CONFIG" / "backups"
        
        safe_target = "".join(c for c in target if c.isalnum() or c in ('_', '-', '.'))[:50]
        target_backup_dir = backup_dir / safe_target
        
        if not target_backup_dir.exists():
            logger.error(f"‚ùå No backups found for target: {target}")
            logger.error(f"   Backup directory does not exist: {target_backup_dir}")
            return False
        
        # Find backup to restore
        if timestamp:
            snapshot_dir = target_backup_dir / timestamp
            if not snapshot_dir.exists():
                # List available timestamps for better error message
                available = LeakageAutoFixer.list_backups(target=target, backup_dir=backup_dir)
                available_timestamps = [b['timestamp'] for b in available]
                logger.error(f"‚ùå Backup not found: {target}/{timestamp}")
                if available_timestamps:
                    logger.error(f"   Available timestamps for {target}:")
                    for ts in available_timestamps[:10]:  # Show first 10
                        logger.error(f"     - {ts}")
                    if len(available_timestamps) > 10:
                        logger.error(f"     ... and {len(available_timestamps) - 10} more")
                else:
                    logger.error(f"   No backups found for target: {target}")
                return False
        else:
            # Find most recent backup
            backups = LeakageAutoFixer.list_backups(target=target, backup_dir=backup_dir)
            if not backups:
                logger.error(f"‚ùå No backups found for target: {target}")
                logger.error(f"   Backup directory exists but contains no valid backups: {target_backup_dir}")
                return False
            snapshot_dir = Path(backups[0]['snapshot_dir'])
            timestamp = backups[0]['timestamp']
            logger.info(f"üì¶ Using most recent backup: {timestamp} (git: {backups[0].get('git_commit', 'N/A')})")
        
        # Load manifest
        manifest_path = snapshot_dir / "manifest.json"
        if not manifest_path.exists():
            logger.error(f"‚ùå Manifest not found in backup: {snapshot_dir}")
            logger.error(f"   Expected manifest at: {manifest_path}")
            logger.error(f"   This backup may be corrupted or incomplete")
            return False
        
        try:
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Manifest is malformed (invalid JSON): {manifest_path}")
            logger.error(f"   Error: {e}")
            logger.error(f"   Cannot restore from corrupted backup")
            return False
        except Exception as e:
            logger.error(f"‚ùå Could not load manifest: {e}")
            logger.error(f"   Manifest path: {manifest_path}")
            return False
        
        # Validate manifest structure
        required_fields = ['excluded_features_path', 'feature_registry_path']
        missing_fields = [f for f in required_fields if f not in manifest]
        if missing_fields:
            logger.error(f"‚ùå Manifest missing required fields: {missing_fields}")
            logger.error(f"   Manifest version: {manifest.get('backup_version', 'unknown')}")
            logger.error(f"   Cannot restore from incomplete backup")
            return False
        
        # Restore files with atomic writes
        excluded_features_backup = snapshot_dir / "excluded_features.yaml"
        feature_registry_backup = snapshot_dir / "feature_registry.yaml"
        
        excluded_features_path = Path(manifest['excluded_features_path'])
        feature_registry_path = Path(manifest['feature_registry_path'])
        
        restored = []
        import os
        import tempfile
        
        # Atomic restore helper
        def atomic_restore(backup_file: Path, target_path: Path, file_name: str) -> bool:
            """Restore a file atomically (write to temp, then atomic rename)."""
            if not backup_file.exists():
                logger.warning(f"‚ö†Ô∏è  Backup file not found: {backup_file}")
                return False
            
            if dry_run:
                logger.info(f"[DRY RUN] Would restore: {backup_file} -> {target_path}")
                return True
            
            try:
                # Create target directory if needed
                target_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Write to temporary file first
                temp_suffix = f".tmp-{os.getpid()}-{os.urandom(4).hex()}"
                temp_path = target_path.parent / f"{target_path.name}{temp_suffix}"
                
                # Copy backup to temp file
                shutil.copy2(backup_file, temp_path)
                
                # Atomic rename (POSIX: rename is atomic)
                os.replace(temp_path, target_path)
                
                restored.append(file_name)
                logger.info(f"‚úÖ Restored: {target_path}")
                return True
            except Exception as e:
                logger.error(f"‚ùå Failed to restore {file_name}: {e}")
                # Clean up temp file if it exists
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except Exception:
                        pass
                return False
        
        # Restore both files
        atomic_restore(excluded_features_backup, excluded_features_path, 'excluded_features.yaml')
        atomic_restore(feature_registry_backup, feature_registry_path, 'feature_registry.yaml')
        
        if restored:
            logger.info(
                f"‚úÖ Restored {len(restored)} config file(s) from backup "
                f"(target={target}, timestamp={timestamp}, "
                f"git_commit={manifest.get('git_commit', 'N/A')})"
            )
            return True
        else:
            logger.warning("‚ö†Ô∏è  No files were restored")
            return False


# ============================================================================
# Registry Patch Operations (Explicit Ops Stage)
# ============================================================================

class RegistryPatchOps:
    """
    Explicit ops stage for registry patch operations.
    
    NEVER runs during normal pipeline execution.
    Must be explicitly invoked via separate command or pipeline stage.
    
    Throws if called from non-REGISTRY_PATCH_OPS stage context.
    """
    
    def __init__(self, config_path: Optional[Path] = None, run_context: Optional[Any] = None):
        """
        Initialize ops from config.
        
        Args:
            config_path: Optional path to registry_patches.yaml
            run_context: Optional RunContext to validate stage
        """
        # Validate stage if run_context provided
        if run_context and hasattr(run_context, 'stage'):
            from TRAINING.orchestration.utils.scope_resolution import Stage
            if run_context.stage != Stage.REGISTRY_PATCH_OPS:
                raise ValueError(
                    f"RegistryPatchOps can only run in REGISTRY_PATCH_OPS stage, "
                    f"got {run_context.stage}"
                )
        
        if config_path is None:
            repo_root = Path(__file__).resolve().parents[2]
            config_path = repo_root / "CONFIG" / "data" / "registry_patches.yaml"
        self.config_path = Path(config_path)
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """Load config from YAML."""
        if not self.config_path.exists():
            return {}
        with open(self.config_path, 'r') as f:
            return yaml.safe_load(f) or {}
    
    def run_ops(self) -> Dict[str, Any]:
        """
        Execute ops from config (promotion, unblocking).
        
        Returns:
            Dict with ops results
        """
        results = {
            'promotions': [],
            'unblocks': [],
            'errors': []
        }
        
        # 1. Promotion ops
        promotion_cfg = self.config.get('promotion', {})
        if promotion_cfg.get('enabled', False):
            try:
                # Load fixer for promotion
                fixer = LeakageAutoFixer(output_dir=None)  # No output_dir for ops
                promotions = fixer.promote_patches_from_config(self.config)
                results['promotions'] = promotions
            except Exception as e:
                logger.error(f"Promotion ops failed: {e}")
                results['errors'].append({'operation': 'promotion', 'error': str(e)})
        
        # 2. Unblock ops
        unblock_cfg = self.config.get('unblocking', {})
        if unblock_cfg.get('enabled', False):
            try:
                fixer = LeakageAutoFixer(output_dir=None)  # No output_dir for ops
                unblocks = fixer.apply_unblocks_from_config(self.config)
                results['unblocks'] = unblocks
            except Exception as e:
                logger.error(f"Unblock ops failed: {e}")
                results['errors'].append({'operation': 'unblocking', 'error': str(e)})
        
        return results


def auto_fix_leakage(
    training_function,
    target_column: str,
    max_iterations: int = 5,
    min_confidence: float = 0.7,
    **training_kwargs
) -> Dict[str, Any]:
    """
    Convenience function to run auto-fix loop.
    
    Example:
        def my_training():
            # ... run training ...
            return {
                'X': X, 'y': y, 'feature_names': feature_names,
                'model_importance': importance_dict,
                'train_score': 0.99, 'test_score': 0.85,
                'symbols': symbols_series, 'task_type': 'classification'
            }
        
        results = auto_fix_leakage(my_training, target_column='y_will_peak_60m_0.8')
    """
    fixer = LeakageAutoFixer()
    return fixer.run_auto_fix_loop(
        training_function=training_function,
        max_iterations=max_iterations,
        min_confidence=min_confidence,
        target_column=target_column,
        **training_kwargs
    )


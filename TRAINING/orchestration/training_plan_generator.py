# SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial
# Copyright (c) 2025-2026 Fox ML Infrastructure LLC

"""
Training Plan Generator

Converts routing decisions into actionable training jobs/plan.
This is the bridge between routing decisions and actual model training.
"""

import json
import logging
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import yaml

from TRAINING.orchestration.routing_integration import load_routing_plan
# DETERMINISM_CRITICAL: Training plan generation order must be deterministic
from TRAINING.common.utils.determinism_ordering import sorted_items

logger = logging.getLogger(__name__)


@dataclass
class TrainingJob:
    """A single training job specification."""
    job_id: str
    target: str
    symbol: Optional[str]  # None for cross-sectional
    route: str  # ROUTE_CROSS_SECTIONAL, ROUTE_SYMBOL_SPECIFIC, etc.
    training_type: str  # "cross_sectional" or "symbol_specific"
    model_families: List[str] = field(default_factory=list)  # Families to train
    priority: int = 0  # Higher = more important
    estimated_samples: Optional[int] = None
    reason: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)


class TrainingPlanGenerator:
    """
    Generates training plan from routing decisions.
    """
    
    def __init__(
        self,
        routing_plan: Dict[str, Any],
        model_families: Optional[List[str]] = None,
        default_families: Optional[List[str]] = None
    ):
        """
        Initialize generator.
        
        Args:
            routing_plan: Routing plan dict (from routing_router)
            model_families: Optional list of model families to train (if None, uses default)
            default_families: Default families if model_families not provided
        """
        self.routing_plan = routing_plan
        
        # Use centralized filter_trainers for normalization and filtering (SST)
        from TRAINING.common.utils.sst_contract import filter_trainers, FEATURE_SELECTORS
        
        # Store raw families for metadata (before filtering)
        self._model_families_requested = model_families
        
        # Respect empty list from config (SST) - only use defaults if None
        if model_families is not None:
            # Use centralized filter_trainers (handles normalization, deduplication, alias mapping)
            filtered_families = filter_trainers(model_families)
            
            removed = len(model_families) - len(filtered_families)
            if removed > 0:
                logger.debug(f"ðŸ“‹ TrainingPlanGenerator: Filtered {removed} families (selectors/duplicates)")
            
            self.model_families = filtered_families
            logger.info(f"ðŸ“‹ TrainingPlanGenerator: Using provided model_families={self.model_families} (after normalization and filtering, original had {len(model_families)})")
        elif default_families is not None:
            # Filter defaults too
            self.model_families = filter_trainers(default_families)
            logger.debug(f"ðŸ“‹ TrainingPlanGenerator: Using default_families={self.model_families} (after filtering)")
        else:
            self.model_families = ["lightgbm", "xgboost"]
            logger.debug(f"ðŸ“‹ TrainingPlanGenerator: Using hardcoded defaults={self.model_families}")
    
        # Invariant: model_families must not contain feature selectors (defensive check)
        selector_violations = set(self.model_families) & FEATURE_SELECTORS
        if selector_violations:
            raise RuntimeError(
                f"ðŸš¨ TrainingPlanGenerator INVARIANT VIOLATION: model_families contains feature selectors: {selector_violations}. "
                f"Full list: {self.model_families}"
            )
    
    def generate_training_plan(
        self,
        output_dir: Path,
        include_blocked: bool = False,
        git_commit: Optional[str] = None,
        config_hash: Optional[str] = None,
        metrics_snapshot: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate training plan from routing decisions.
        
        Args:
            output_dir: Output directory for plan artifacts
            include_blocked: If True, include blocked jobs (for analysis)
            git_commit: Optional git commit hash
            config_hash: Optional config hash
            metrics_snapshot: Optional path to metrics snapshot
        
        Returns:
            Training plan dict
        
        Raises:
            ValueError: If routing_plan is invalid or output_dir cannot be created
        """
        # Validate inputs
        if not isinstance(self.routing_plan, dict):
            raise ValueError(f"routing_plan must be a dict, got {type(self.routing_plan)}")
        
        if not isinstance(output_dir, (Path, str)):
            raise ValueError(f"output_dir must be Path or str, got {type(output_dir)}")
        
        output_dir = Path(output_dir)
        
        # Validate model_families
        if not isinstance(self.model_families, list):
            logger.warning(f"model_families is not a list, using default")
            self.model_families = ["lightgbm", "xgboost"]
        
        # Respect empty list from config (SST) - empty list means "no families to train"
        # Only warn if it's empty (config explicitly set it to empty)
        if not self.model_families:
            logger.warning("âš ï¸ model_families is empty - no families will be trained (this may be intentional from config)")
        
        jobs = []
        
        # Safely get targets with validation
        targets = self.routing_plan.get("targets", {})
        if not isinstance(targets, dict):
            logger.warning(f"routing_plan['targets'] is not a dict, got {type(targets)}, using empty dict")
            targets = {}
        
        # FIX ISSUE-023: Sort for determinism - ensures consistent plan generation order regardless of dict construction order
        for target, target_data in sorted(targets.items()):
            # Validate target and target_data
            if not isinstance(target, str) or not target:
                logger.warning(f"Skipping invalid target: {target} (not a non-empty string)")
                continue
            
            if not isinstance(target_data, dict):
                logger.warning(f"Skipping target {target}: target_data is not a dict")
                continue
            
            try:
                # Cross-sectional jobs
                cs_info = target_data.get("cross_sectional", {})
                if not isinstance(cs_info, dict):
                    logger.warning(f"Target {target}: cross_sectional is not a dict, skipping CS job")
                    cs_info = {}
                
                if cs_info.get("route") == "ENABLED":
                    # Validate reason field
                    reason = cs_info.get("reason", "CS training enabled")
                    if not isinstance(reason, str):
                        reason = str(reason) if reason is not None else "CS training enabled"
                    
                    # Safely get sample_size
                    cs_metrics = target_data.get("cs_metrics", {})
                    if not isinstance(cs_metrics, dict):
                        cs_metrics = {}
                    sample_size = cs_metrics.get("sample_size")
                    
                    job = TrainingJob(
                        job_id=f"cs_{target}",
                        target=target,
                        symbol=None,
                        route="ROUTE_CROSS_SECTIONAL",
                        training_type="cross_sectional",
                        model_families=self.model_families.copy(),
                        priority=2,  # CS jobs are high priority
                        reason=reason,
                        metadata={
                            "cs_state": cs_info.get("state", "UNKNOWN"),
                            "sample_size": sample_size
                        }
                    )
                    jobs.append(job)
                
                # Symbol-specific jobs
                symbols = target_data.get("symbols", {})
                if not isinstance(symbols, dict):
                    logger.warning(f"Target {target}: symbols is not a dict, skipping symbol jobs")
                    symbols = {}
                
                # DETERMINISM: Use sorted_items for deterministic iteration order
                for symbol, sym_data in sorted_items(symbols):
                    # Validate symbol and sym_data
                    if not isinstance(symbol, str) or not symbol:
                        logger.warning(f"Target {target}: Skipping invalid symbol: {symbol}")
                        continue
                    
                    if not isinstance(sym_data, dict):
                        logger.warning(f"Target {target}, Symbol {symbol}: sym_data is not a dict, skipping")
                        continue
                    
                    route = sym_data.get("route", "ROUTE_BLOCKED")
                    if not isinstance(route, str):
                        logger.warning(f"Target {target}, Symbol {symbol}: route is not a string, defaulting to ROUTE_BLOCKED")
                        route = "ROUTE_BLOCKED"
                    
                    if route == "ROUTE_BLOCKED":
                        if include_blocked:
                            # Safely handle reason field
                            reason_list = sym_data.get("reason", [])
                            if not isinstance(reason_list, list):
                                reason_list = [str(reason_list)] if reason_list is not None else []
                            
                            try:
                                reason_str = "; ".join(str(r) for r in reason_list)
                            except Exception as e:
                                logger.warning(f"Failed to join reason list: {e}, using default")
                                reason_str = "Blocked"
                            
                            job = TrainingJob(
                                job_id=f"blocked_{target}_{symbol}",
                                target=target,
                                symbol=symbol,
                                route=route,
                                training_type="blocked",
                                model_families=[],
                                priority=0,
                                reason=reason_str,
                                metadata={
                                    "cs_state": sym_data.get("cs_state", "UNKNOWN"),
                                    "local_state": sym_data.get("local_state", "UNKNOWN")
                                }
                            )
                            jobs.append(job)
                        continue
                    
                    # Determine training type based on route
                    if route == "ROUTE_CROSS_SECTIONAL":
                        # Should use CS model (already handled above)
                        continue
                    elif route in ["ROUTE_SYMBOL_SPECIFIC", "ROUTE_BOTH", "ROUTE_EXPERIMENTAL_ONLY"]:
                        training_type = "symbol_specific"
                        priority = 3 if route == "ROUTE_BOTH" else (1 if route == "ROUTE_EXPERIMENTAL_ONLY" else 2)
                        
                        # Safely handle reason field
                        reason_list = sym_data.get("reason", [])
                        if not isinstance(reason_list, list):
                            reason_list = [str(reason_list)] if reason_list is not None else []
                        
                        try:
                            reason_str = "; ".join(str(r) for r in reason_list)
                        except Exception as e:
                            logger.warning(f"Failed to join reason list: {e}, using default")
                            reason_str = f"{route} training enabled"
                        
                        job = TrainingJob(
                            job_id=f"sym_{target}_{symbol}",
                            target=target,
                            symbol=symbol,
                            route=route,
                            training_type=training_type,
                            model_families=self.model_families.copy(),
                            priority=priority,
                            reason=reason_str,
                            metadata={
                                "cs_state": sym_data.get("cs_state", "UNKNOWN"),
                                "local_state": sym_data.get("local_state", "UNKNOWN"),
                                "needs_cs_ensemble": route == "ROUTE_BOTH"
                            }
                        )
                        jobs.append(job)
                    else:
                        logger.warning(f"Target {target}, Symbol {symbol}: Unknown route '{route}', skipping")
            except Exception as e:
                logger.error(f"Error processing target {target}: {e}", exc_info=True)
                continue
        
        # Dev mode fallback: Generate jobs if router produced 0 jobs
        # FIX ISSUE-004: Use centralized dev_mode helper instead of direct get_cfg
        dev_mode = False
        try:
            from CONFIG.dev_mode import get_dev_mode
            dev_mode = get_dev_mode()
        except Exception:
            pass
        
        if dev_mode and len(jobs) == 0:
            # Dev mode: generate fallback jobs (at least 1 per target Ã— trainer)
            logger.warning(
                f"âš ï¸  Dev mode: Router produced 0 jobs. Generating fallback jobs: "
                f"1 CS job per target Ã— trainer (ignoring thresholds)."
            )
            # Get trainers from model_families (already filtered to trainers only)
            trainers = self.model_families if self.model_families else ["lightgbm", "xgboost"]
            
            # Generate minimal jobs for each target
            targets = self.routing_plan.get("targets", {})
            # FIX ISSUE-023: Sort for determinism - ensures consistent job generation order regardless of dict construction order
            for target in sorted(targets.keys()):
                for trainer in trainers:
                    job = TrainingJob(
                        job_id=f"dev_fallback_cs_{target}_{trainer}",
                        target=target,
                        symbol=None,
                        route="ROUTE_CROSS_SECTIONAL",
                        training_type="cross_sectional",
                        model_families=[trainer],  # One trainer per job for dev fallback
                        priority=1,  # Lower priority than normal jobs
                        reason="Dev mode fallback: router produced 0 jobs",
                        metadata={
                            "dev_mode_fallback": True,  # Mark as fallback
                            "cs_state": "UNKNOWN"
                        }
                    )
                    jobs.append(job)
            logger.info(f"âœ… Dev mode: Generated {len(jobs)} fallback jobs")
        
        # Sort by priority (higher first) - with error handling
        try:
            jobs.sort(key=lambda j: -j.priority)
        except Exception as e:
            logger.warning(f"Failed to sort jobs by priority: {e}, keeping original order")
        
        # Generate run_id - use deterministic derivation from RunIdentity if available
        # Check if run_identity is in routing_plan metadata
        run_identity = None
        if isinstance(self.routing_plan, dict):
            routing_metadata = self.routing_plan.get("metadata", {})
            if isinstance(routing_metadata, dict):
                run_identity = routing_metadata.get("run_identity")
        
        # SST: Use derive_run_id_from_identity for deterministic run_id
        # In strict mode: fails closed if run_identity not available (correct behavior)
        # In best-effort mode: falls back to unstable run_id
        from TRAINING.orchestration.utils.manifest import derive_run_id_from_identity, derive_unstable_run_id, generate_run_instance_id
        try:
            run_id = derive_run_id_from_identity(
                run_identity=run_identity
            )
        except ValueError:
            # Fallback to unstable run_id if identity not available
            run_id = derive_unstable_run_id(generate_run_instance_id())
        
        # Get routing plan path from metadata - safely
        # Use globals/ (primary) with backward compatibility for METRICS/
        routing_plan_path = "globals/routing_plan/routing_plan.json"
        try:
            routing_metadata = self.routing_plan.get("metadata", {})
            if isinstance(routing_metadata, dict):
                metrics_snapshot_val = routing_metadata.get("metrics_snapshot")
                if metrics_snapshot_val and isinstance(metrics_snapshot_val, str):
                    routing_plan_path = metrics_snapshot_val
                    # Update legacy METRICS paths to globals
                    if routing_plan_path.startswith("METRICS/"):
                        routing_plan_path = routing_plan_path.replace("METRICS/", "globals/", 1)
        except Exception as e:
            logger.debug(f"Could not extract routing_plan_path from metadata: {e}")
        
        # Build plan structure with full metadata - with validation
        try:
            routing_metadata = self.routing_plan.get("metadata", {})
            if not isinstance(routing_metadata, dict):
                routing_metadata = {}
            
            plan = {
                "metadata": {
                    "generated_at": datetime.utcnow().isoformat() + "Z",
                    "run_id": run_id,
                    "git_commit": str(git_commit) if git_commit else str(routing_metadata.get("git_commit", "unknown")),
                    "config_hash": str(config_hash) if config_hash else str(routing_metadata.get("config_hash", "unknown")),
                    "routing_plan_path": str(routing_plan_path),
                    "metrics_snapshot": str(metrics_snapshot) if metrics_snapshot else "globals/routing_candidates.parquet",
                    "total_jobs": len(jobs),
                    # Store both raw and normalized families for debugging and fallback
                    "model_families_requested": list(self._model_families_requested) if self._model_families_requested else None,
                    "model_families_normalized": list(self.model_families),  # Already filtered/normalized
                    "model_families": list(self.model_families)  # Backward compatibility
                },
                "jobs": [],
                "summary": {}
            }
            
            # Convert jobs to dicts safely
            for job in jobs:
                try:
                    plan["jobs"].append(asdict(job))
                except Exception as e:
                    logger.warning(f"Failed to convert job {job.job_id} to dict: {e}, skipping")
            
            # Generate summary safely
            try:
                plan["summary"] = self._generate_summary(jobs)
            except Exception as e:
                logger.warning(f"Failed to generate summary: {e}, using empty summary")
                plan["summary"] = {
                    "by_route": {},
                    "by_type": {},
                    "by_priority": {},
                    "total_cs_jobs": 0,
                    "total_symbol_jobs": 0,
                    "total_blocked": 0
                }
        except Exception as e:
            logger.error(f"Failed to build plan structure: {e}", exc_info=True)
            raise ValueError(f"Failed to build training plan: {e}")
        
        # Save plan - with error handling
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            raise ValueError(f"Failed to create output directory {output_dir}: {e}")
        
        # Save master plan (canonical source of truth) - with error handling
        master_path = output_dir / "master_training_plan.json"
        # SST: Sanitize plan data to normalize enums to strings before JSON serialization
        from TRAINING.orchestration.utils.diff_telemetry import _sanitize_for_json
        sanitized_plan = _sanitize_for_json(plan)
        
        try:
            # SST: Use write_atomic_json for atomic write with canonical serialization
            from TRAINING.common.utils.file_utils import write_atomic_json
            write_atomic_json(master_path, sanitized_plan)
            logger.info(f"âœ… Saved master training plan: {master_path}")
        except Exception as e:
            logger.error(f"Failed to save master training plan: {e}", exc_info=True)
            raise ValueError(f"Failed to save master training plan: {e}")
        
        # Save convenience mirror - with error handling
        json_path = output_dir / "training_plan.json"
        try:
            # CRITICAL: Use atomic write for crash safety (fsync + dir fsync)
            from TRAINING.common.utils.file_utils import write_atomic_json
            write_atomic_json(json_path, plan)
            logger.info(f"âœ… Saved training plan (convenience mirror): {json_path}")
        except Exception as e:
            logger.warning(f"Failed to save convenience mirror: {e}, continuing...")
        
        # Save YAML - with error handling
        yaml_path = output_dir / "training_plan.yaml"
        try:
            # DETERMINISM: Use canonical_yaml() for deterministic YAML output
            from TRAINING.common.utils.determinism_serialization import write_canonical_yaml
            write_canonical_yaml(yaml_path, plan)
            logger.info(f"âœ… Saved training plan YAML: {yaml_path}")
        except Exception as e:
            logger.warning(f"Failed to save YAML: {e}, continuing...")
        
        # Save Markdown report - with error handling
        md_path = output_dir / "training_plan.md"
        try:
            self._write_markdown_report(plan, md_path)
            logger.info(f"âœ… Saved training plan Markdown: {md_path}")
        except Exception as e:
            logger.warning(f"Failed to save Markdown report: {e}, continuing...")
        
        # Generate derived views - with error handling
        try:
            self._generate_derived_views(plan, output_dir)
        except Exception as e:
            logger.warning(f"Failed to generate derived views: {e}, continuing...")
        
        return plan
    
    def _generate_summary(self, jobs: List[TrainingJob]) -> Dict[str, Any]:
        """Generate summary statistics with error handling."""
        summary = {
            "by_route": {},
            "by_type": {},
            "by_priority": {},
            "total_cs_jobs": 0,
            "total_symbol_jobs": 0,
            "total_blocked": 0
        }
        
        if not jobs:
            return summary
        
        for job in jobs:
            try:
                if not isinstance(job, TrainingJob):
                    logger.warning(f"Skipping invalid job in summary: {type(job)}")
                    continue
                
                # Count by route - safely
                route = getattr(job, 'route', 'UNKNOWN')
                if not isinstance(route, str):
                    route = str(route) if route is not None else 'UNKNOWN'
                summary["by_route"][route] = summary["by_route"].get(route, 0) + 1
                
                # Count by type - safely
                job_type = getattr(job, 'training_type', 'UNKNOWN')
                if not isinstance(job_type, str):
                    job_type = str(job_type) if job_type is not None else 'UNKNOWN'
                summary["by_type"][job_type] = summary["by_type"].get(job_type, 0) + 1
                
                # Count by priority - safely
                priority = getattr(job, 'priority', 0)
                try:
                    priority = int(priority) if priority is not None else 0
                except (ValueError, TypeError):
                    priority = 0
                summary["by_priority"][priority] = summary["by_priority"].get(priority, 0) + 1
                
                # Count totals - safely
                if job_type == "cross_sectional":
                    summary["total_cs_jobs"] += 1
                elif job_type == "symbol_specific":
                    summary["total_symbol_jobs"] += 1
                elif job_type == "blocked":
                    summary["total_blocked"] += 1
            except Exception as e:
                logger.warning(f"Error processing job in summary: {e}, skipping")
                continue
        
        return summary
    
    def _write_markdown_report(self, plan: Dict[str, Any], output_path: Path):
        """Write human-readable Markdown report with error handling."""
        if not isinstance(plan, dict):
            logger.warning(f"Plan is not a dict, cannot write markdown report")
            return
        
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logger.warning(f"Failed to create directory for markdown report: {e}")
            return
        
        try:
            with open(output_path, "w") as f:
                # Safely get metadata
                metadata = plan.get("metadata", {})
                if not isinstance(metadata, dict):
                    metadata = {}
                
                f.write("# Training Plan\n\n")
                
                # Safely write metadata fields
                generated_at = metadata.get("generated_at", "unknown")
                f.write(f"**Generated:** {generated_at}\n")
                
                total_jobs = metadata.get("total_jobs", 0)
                f.write(f"**Total Jobs:** {total_jobs}\n")
                
                model_families = metadata.get("model_families", [])
                if isinstance(model_families, list):
                    families_str = ", ".join(str(f) for f in model_families)
                else:
                    families_str = str(model_families)
                f.write(f"**Model Families:** {families_str}\n\n")
            
                # Safely get summary
                summary = plan.get("summary", {})
                if not isinstance(summary, dict):
                    summary = {}
                
                f.write("## Summary\n\n")
                f.write(f"- **Cross-Sectional Jobs:** {summary.get('total_cs_jobs', 0)}\n")
                f.write(f"- **Symbol-Specific Jobs:** {summary.get('total_symbol_jobs', 0)}\n")
                f.write(f"- **Blocked:** {summary.get('total_blocked', 0)}\n\n")
                
                # By route - safely
                by_route = summary.get("by_route", {})
                if isinstance(by_route, dict) and by_route:
                    f.write("### By Route\n\n")
                    try:
                        for route, count in sorted(by_route.items()):
                            f.write(f"- {route}: {count}\n")
                    except Exception as e:
                        logger.warning(f"Failed to write by_route section: {e}")
                    f.write("\n")
                
                # By priority - safely
                by_priority = summary.get("by_priority", {})
                if isinstance(by_priority, dict) and by_priority:
                    f.write("### By Priority\n\n")
                    try:
                        for priority in sorted(by_priority.keys(), reverse=True):
                            count = by_priority[priority]
                            f.write(f"- Priority {priority}: {count} jobs\n")
                    except Exception as e:
                        logger.warning(f"Failed to write by_priority section: {e}")
                    f.write("\n")
                
                f.write("---\n\n")
                f.write("## Training Jobs\n\n")
                
                # Group by target - safely
                jobs = plan.get("jobs", [])
                if not isinstance(jobs, list):
                    jobs = []
                
                jobs_by_target = {}
                for job_data in jobs:
                    if not isinstance(job_data, dict):
                        continue
                    try:
                        target = job_data.get("target")
                        if target and isinstance(target, str):
                            if target not in jobs_by_target:
                                jobs_by_target[target] = []
                            jobs_by_target[target].append(job_data)
                    except Exception as e:
                        logger.warning(f"Error grouping job by target: {e}")
                        continue
                
                for target, target_jobs in sorted(jobs_by_target.items()):
                    try:
                        f.write(f"### {target}\n\n")
                        
                        # CS job - safely
                        cs_jobs = [j for j in target_jobs if isinstance(j, dict) and j.get("training_type") == "cross_sectional"]
                        if cs_jobs:
                            cs = cs_jobs[0]
                            f.write(f"**Cross-Sectional:** âœ… Enabled\n")
                            f.write(f"- Job ID: `{cs.get('job_id', 'unknown')}`\n")
                            
                            families = cs.get("model_families", [])
                            if isinstance(families, list):
                                families_str = ", ".join(str(f) for f in families)
                            else:
                                families_str = str(families)
                            f.write(f"- Families: {families_str}\n")
                            f.write(f"- Priority: {cs.get('priority', 0)}\n")
                            f.write(f"- Reason: {cs.get('reason', 'N/A')}\n\n")
                        
                        # Symbol jobs - safely
                        sym_jobs = [j for j in target_jobs if isinstance(j, dict) and j.get("training_type") == "symbol_specific"]
                        if sym_jobs:
                            f.write("**Symbol-Specific Jobs:**\n\n")
                            f.write("| Symbol | Route | Priority | Families | Reason |\n")
                            f.write("|--------|-------|----------|----------|--------|\n")
                            try:
                                for job in sorted(sym_jobs, key=lambda j: -j.get("priority", 0)):
                                    families = job.get("model_families", [])
                                    if isinstance(families, list):
                                        families_str = ", ".join(str(f) for f in families[:3])
                                        if len(families) > 3:
                                            families_str += f" (+{len(families) - 3} more)"
                                    else:
                                        families_str = str(families)
                                    
                                    reason = job.get("reason", "N/A")
                                    if isinstance(reason, str) and len(reason) > 80:
                                        reason_short = reason[:80] + "..."
                                    else:
                                        reason_short = str(reason) if reason else "N/A"
                                    
                                    f.write(f"| {job.get('symbol', 'N/A')} | {job.get('route', 'N/A')} | {job.get('priority', 0)} | {families_str} | {reason_short} |\n")
                            except Exception as e:
                                logger.warning(f"Failed to write symbol jobs table: {e}")
                            f.write("\n")
                        
                        f.write("\n")
                    except Exception as e:
                        logger.warning(f"Error writing target section for {target}: {e}")
                        continue
        except PermissionError as e:
            logger.warning(f"Permission denied writing markdown report to {output_path}: {e}")
        except Exception as e:
            logger.warning(f"Failed to write markdown report: {e}", exc_info=True)
    
    def _generate_derived_views(self, plan: Dict[str, Any], output_dir: Path):
        """Generate derived view artifacts (by_target, by_symbol, by_type, by_route)."""
        jobs = plan.get("jobs", [])
        
        if not jobs:
            logger.warning("No jobs in plan, skipping derived view generation")
            return
        
        # Group by target
        by_target = {}
        for job in jobs:
            if not isinstance(job, dict):
                logger.warning(f"Skipping invalid job (not a dict): {job}")
                continue
            target = job.get("target")
            if target is None:
                logger.warning(f"Skipping job with missing target: {job.get('job_id', 'unknown')}")
                continue
            if target not in by_target:
                by_target[target] = []
            by_target[target].append(job)
        
        # Group by symbol
        by_symbol = {}
        for job in jobs:
            if not isinstance(job, dict):
                continue
            symbol = job.get("symbol")
            if symbol is not None:
                if symbol not in by_symbol:
                    by_symbol[symbol] = []
                by_symbol[symbol].append(job)
        
        # Group by training type
        by_type = {}
        for job in jobs:
            if not isinstance(job, dict):
                continue
            job_type = job.get("training_type")
            if job_type is None:
                logger.warning(f"Skipping job with missing training_type: {job.get('job_id', 'unknown')}")
                continue
            if job_type not in by_type:
                by_type[job_type] = []
            by_type[job_type].append(job)
        
        # Group by route
        by_route = {}
        for job in jobs:
            if not isinstance(job, dict):
                continue
            route = job.get("route")
            if route is None:
                logger.warning(f"Skipping job with missing route: {job.get('job_id', 'unknown')}")
                continue
            if route not in by_route:
                by_route[route] = []
            by_route[route].append(job)
        
        # Save by_target views
        try:
            by_target_dir = output_dir / "by_target"
            by_target_dir.mkdir(exist_ok=True)
            # DETERMINISM_CRITICAL: Training plan generation order must be deterministic
            for target, target_jobs in sorted_items(by_target):
                view = {
                    "target": target,
                    "jobs": target_jobs
                }
                view_path = by_target_dir / f"{target}.json"
                # SST: Use write_atomic_json for atomic write with canonical serialization
                from TRAINING.common.utils.file_utils import write_atomic_json
                write_atomic_json(view_path, view)
            logger.info(f"âœ… Generated {len(by_target)} by_target views")
        except Exception as e:
            logger.warning(f"Failed to generate by_target views: {e}")
        
        # Save by_symbol views
        try:
            by_symbol_dir = output_dir / "by_symbol"
            by_symbol_dir.mkdir(exist_ok=True)
            # DETERMINISM_CRITICAL: Training plan generation order must be deterministic
            for symbol, symbol_jobs in sorted_items(by_symbol):
                view = {
                    "symbol": symbol,
                    "jobs": symbol_jobs
                }
                view_path = by_symbol_dir / f"{symbol}.json"
                # SST: Use write_atomic_json for atomic write with canonical serialization
                from TRAINING.common.utils.file_utils import write_atomic_json
                write_atomic_json(view_path, view)
            logger.info(f"âœ… Generated {len(by_symbol)} by_symbol views")
        except Exception as e:
            logger.warning(f"Failed to generate by_symbol views: {e}")
        
        # Save by_type views
        try:
            by_type_dir = output_dir / "by_type"
            by_type_dir.mkdir(exist_ok=True)
            # DETERMINISM_CRITICAL: Training plan generation order must be deterministic
            for job_type, type_jobs in sorted_items(by_type):
                view = {
                    "training_type": job_type,
                    "jobs": type_jobs
                }
                view_path = by_type_dir / f"{job_type}.json"
                # SST: Use write_atomic_json for atomic write with canonical serialization
                from TRAINING.common.utils.file_utils import write_atomic_json
                write_atomic_json(view_path, view)
            logger.info(f"âœ… Generated {len(by_type)} by_type views")
        except Exception as e:
            logger.warning(f"Failed to generate by_type views: {e}")
        
        # Save by_route views
        try:
            by_route_dir = output_dir / "by_route"
            by_route_dir.mkdir(exist_ok=True)
            # DETERMINISM_CRITICAL: Training plan generation order must be deterministic
            for route, route_jobs in sorted_items(by_route):
                view = {
                    "route": route,
                    "jobs": route_jobs
                }
                # Sanitize route name for filename
                route_safe = route.replace("ROUTE_", "").lower()
                view_path = by_route_dir / f"{route_safe}.json"
                # SST: Use write_atomic_json for atomic write with canonical serialization
                from TRAINING.common.utils.file_utils import write_atomic_json
                write_atomic_json(view_path, view)
            logger.info(f"âœ… Generated {len(by_route)} by_route views")
        except Exception as e:
            logger.warning(f"Failed to generate by_route views: {e}")


def generate_training_plan_from_routing(
    routing_plan_path: Path,
    output_dir: Path,
    model_families: Optional[List[str]] = None,
    include_blocked: bool = False,
    git_commit: Optional[str] = None,
    config_hash: Optional[str] = None,
    metrics_snapshot: Optional[str] = None
) -> Dict[str, Any]:
    """
    Convenience function to generate training plan from routing plan file.
    
    Args:
        routing_plan_path: Path to routing_plan.json
        output_dir: Output directory for training plan
        model_families: Optional list of model families
        include_blocked: Include blocked jobs in plan
        git_commit: Optional git commit hash
        config_hash: Optional config hash
        metrics_snapshot: Optional path to metrics snapshot
    
    Returns:
        Training plan dict
    """
    routing_plan = load_routing_plan(routing_plan_path.parent)
    if routing_plan is None:
        raise ValueError(f"Failed to load routing plan from {routing_plan_path}")
    
    generator = TrainingPlanGenerator(
        routing_plan=routing_plan,
        model_families=model_families
    )
    
    return generator.generate_training_plan(
        output_dir=output_dir,
        include_blocked=include_blocked,
        git_commit=git_commit,
        config_hash=config_hash,
        metrics_snapshot=metrics_snapshot
    )

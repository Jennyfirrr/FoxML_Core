# SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial
# Copyright (c) 2025-2026 Fox ML Infrastructure LLC

"""Simple working stub for Neuralnetwork trainer."""

import TRAINING.common.repro_bootstrap  # noqa: F401 - must be first

import os
import numpy as np
from TRAINING.common.determinism import get_deterministic_params, seed_for, is_strict_mode
import logging
from typing import Dict, Any, Optional
from .base_trainer import BaseModelTrainer

logger = logging.getLogger(__name__)

class NeuralNetworkTrainer(BaseModelTrainer):
    """Simple working stub for Neuralnetwork trainer."""


    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)

    def fit(self, X_tr: np.ndarray, y_tr: np.ndarray, seed: Optional[int] = None, **kwargs) -> Any:
        """Fit method for compatibility with sklearn-style interface."""
        return self.train(X_tr, y_tr, seed, **kwargs)
    
    def train(self, X_tr: np.ndarray, y_tr: np.ndarray, X_va=None, y_va=None, cpu_only=False, num_threads=12, feat_cols=None, seed: Optional[int] = None, **kwargs) -> Any:
        """Train Neural Network model using self-contained logic."""
        return self._train_neural_network(X_tr, y_tr, X_va, y_va, cpu_only, num_threads, feat_cols, seed, **kwargs)
    
    def _train_neural_network(self, X_tr, y_tr, X_va=None, y_va=None, cpu_only=False, num_threads=12, feat_cols=None, seed=None, epochs=50, **kwargs):
        """Train Neural Network model."""
        # STRICT MODE: Force CPU for determinism
        if is_strict_mode():
            logger.info("[NeuralNetwork] Strict mode: forcing CPU (GPU disabled for determinism)")
            cpu_only = True
        
        # Load batch_size and max_epochs from training profile
        try:
            from CONFIG.config_loader import get_cfg, get_optimizer_config
            # FIX: active_profile is at top level of optimizer config (not training.active_profile)
            profile = get_cfg("active_profile", default="default", config_name="optimizer")
            optimizer_cfg = get_optimizer_config()
            profile_cfg = optimizer_cfg.get("training_profiles", {}).get(profile, {})
            batch_size = profile_cfg.get("batch_size", 256)  # FALLBACK_DEFAULT_OK
            max_epochs = profile_cfg.get("max_epochs", 50)  # FALLBACK_DEFAULT_OK
            # Use max_epochs from profile if epochs not explicitly provided
            if epochs == 50:  # Default value
                epochs = max_epochs
        except Exception:
            batch_size = 256  # FALLBACK_DEFAULT_OK
            max_epochs = 50  # FALLBACK_DEFAULT_OK
            if epochs == 50:
                epochs = max_epochs
        
        # CRITICAL: Check if TensorFlow is disabled in this child process
        # This prevents tensorflow from being imported when TRAINER_CHILD_NO_TF=1
        if os.getenv("TRAINER_CHILD_NO_TF", "0") == "1":
            logger.error("Neural Network training failed: tensorflow is disabled in this child process (TRAINER_CHILD_NO_TORCH/NO_TF)")
            return None
        
        try:
            import tensorflow as tf
            from tensorflow.keras import layers, Model
            from tensorflow.keras.optimizers import Adam
            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
            from sklearn.impute import SimpleImputer
            from sklearn.preprocessing import StandardScaler
            import numpy as np
            
            # Set device
            TF_DEVICE = '/GPU:0' if not cpu_only else '/CPU:0'
            
            # Preprocess data
            imputer = SimpleImputer(strategy='median')
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(imputer.fit_transform(X_tr))
            
            if X_va is not None:
                X_va_scaled = scaler.transform(imputer.transform(X_va))
            else:
                X_va_scaled = None
            
            n_features = X_scaled.shape[1]

            logger.info(f"ðŸ§  Neural Network training on {TF_DEVICE}")

            # Set TF seed for determinism (TF_DETERMINISTIC_OPS=1 requires explicit seed)
            tf_seed = int(os.environ.get("PYTHONHASHSEED", "42"))
            tf.random.set_seed(tf_seed)

            with tf.device(TF_DEVICE):
                inputs = layers.Input(shape=(n_features,))
                x = layers.Dense(256, activation='relu')(inputs)
                x = layers.BatchNormalization()(x)
                x = layers.Dropout(0.3)(x)
                x = layers.Dense(128, activation='relu')(x)
                x = layers.BatchNormalization()(x)
                x = layers.Dropout(0.3)(x)
                x = layers.Dense(64, activation='relu')(x)
                x = layers.BatchNormalization()(x)
                x = layers.Dropout(0.2)(x)
                outputs = layers.Dense(1, dtype="float32")(x)
                
                model = Model(inputs, outputs)
                model.compile(
                    optimizer=Adam(learning_rate=self._get_learning_rate()),
                    loss='mse',
                    metrics=['mae']
                )
            
            # Callbacks
            callbacks = [
                EarlyStopping(patience=20, restore_best_weights=True),
                ReduceLROnPlateau(patience=10, factor=0.5, min_lr=1e-6)
            ]
            
            # Train model
            if X_va_scaled is not None and y_va is not None:
                model.fit(
                    X_scaled, y_tr,
                    validation_data=(X_va_scaled, y_va),
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=callbacks,
                    verbose=0
                )
            else:
                model.fit(
                    X_scaled, y_tr,
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=callbacks,
                    verbose=0
                )
            
            # Store preprocessors for inference
            self.scaler = scaler
            self.imputer = imputer
            self.model = model
            self.is_trained = True
            return model
            
        except ImportError:
            logger.error("TensorFlow not available for Neural Network")
            return None
        except Exception as e:
            logger.error(f"Neural Network training failed: {e}")
            return None
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions using the trained neural network model."""
        if not hasattr(self, 'model') or not hasattr(self, 'is_trained') or not self.is_trained:
            raise ValueError("Model not trained yet. Call fit() first.")
        
        try:
            # Clean input data using stored preprocessors
            X_float = X.astype(np.float64)
            X_clean = self.imputer.transform(X_float)
            X_scaled = self.scaler.transform(X_clean)
            
            # Make predictions
            predictions = self.model.predict(X_scaled, verbose=0)
            
            # Flatten if needed
            if predictions.ndim > 1:
                predictions = predictions.flatten()
            
            # Ensure predictions are finite
            predictions = np.nan_to_num(predictions, nan=0.0)
            
            return predictions.astype(np.float32)
            
        except Exception as e:
            logger.error(f"Neural Network prediction failed: {e}")
            # Return zeros as fallback
            return np.zeros(X.shape[0], dtype=np.float32)
# Changelog ‚Äî 2025-12-11

**Reproducibility Tracking, Leakage Fixes, Interval Detection, Param Sanitization, Cross-Sectional Stability, Cohort-Aware Reproducibility & RESULTS Organization**

For a quick overview, see the [root changelog](../../../CHANGELOG.md).  
For other dates, see the [changelog index](README.md).

---

## Added

### Per-Model Reproducibility Tracking

**Feature Selection Per-Model Tracking**
- **Enhancement**: Added structured, thresholded per-model reproducibility tracking to feature selection
- **Metrics**: Computes delta_score, Jaccard@K, and importance_corr for each model family
- **Storage**: Results stored in `model_metadata.json` for audit trails
- **Logging**: Compact logging approach - stable models = one line (INFO), unstable = WARNING
- **Summary**: Symbol-level summary shows reproducibility status across all families
- **Thresholds**: Different thresholds for high-variance vs stable model families
- **Non-spammy**: Only logs when it matters (unstable/borderline)
- **Files**: 
  - `TRAINING/ranking/multi_model_feature_selection.py` - Added `compute_per_model_reproducibility()`, `load_previous_model_results()`, `save_model_metadata()`
  - Integrated into `_process_single_symbol()` with compact logging

### Model Parameter Sanitization System

**Config Cleaner Utility**
- **New Module**: `TRAINING/utils/config_cleaner.py` - Shared utility for systematic parameter validation
- **Method**: Uses `inspect.signature()` to validate parameters against actual estimator signatures
- **Features**: 
  - Automatically removes duplicate and unknown parameters
  - Prevents "got multiple values for keyword argument" errors
  - Prevents "unexpected keyword argument" errors
  - Special handling for known parameter conflicts (MLPRegressor, CatBoost, etc.)
- **Integration**: Integrated into all model instantiation paths (multi-model feature selection, task types, cross-sectional ranking)
- **SST Compliance**: Maintains Single Source of Truth while ensuring only valid parameters reach constructors
- **Files**: 

**MLPRegressor Verbose Parameter Fix**
- **Issue**: `verbose=-1` was being passed to MLPRegressor, but sklearn requires `verbose >= 0`
- **Impact**: `ValueError: The 'verbose' parameter of MLPRegressor must be an int in the range [0, inf)`
- **Fix**: Added sanitization in `config_cleaner.py` that converts negative verbose values to `0` for neural_network family
- **Files**: `TRAINING/utils/config_cleaner.py`

**CatBoost Iteration Synonyms Conflict Fix**
- **Issue**: `CatBoostError: only one of the parameters iterations, n_estimators, num_boost_round, num_trees should be initialized`
- **Root Cause**: Global defaults injection was adding `n_estimators: 1000` while config already had `iterations: 300`
- **Fix**: 
  - Added aggressive sanitization in `multi_model_feature_selection.py` that removes all synonyms before model construction
  - Prefers `iterations` (CatBoost's native param)
  - Double-check after `_clean_config_for_estimator` ensures no synonyms remain
  - Also added sanitization in `config_cleaner.py` for global application
- **Files**: 
  - `TRAINING/utils/config_cleaner.py` - Global sanitization
  - `TRAINING/ranking/multi_model_feature_selection.py` - Aggressive removal before model construction

**CatBoost Random State/Random Seed Conflict Fix**
- **Issue**: `CatBoostError: only one of the parameters random_seed, random_state should be initialized`
- **Root Cause**: CatBoost accepts only one RNG parameter, but both were being passed
- **Fix**: Added sanitization in `config_cleaner.py` that:
  - Converts `random_state` to `random_seed` (CatBoost's preferred name)
  - Or removes it if `random_seed` is already in extra_kwargs
  - Prevents conflicts from defaults injection
- **Files**: `TRAINING/utils/config_cleaner.py`

**Univariate Selection Signed F-Statistics Handling**
- **Issue**: Negative F-statistics caused negative importance sums, triggering uniform fallback (no real signal)
- **Impact**: Univariate selection was losing signal when scores were negative
- **Fix**: 
  - Modified to use absolute values for ranking
  - Treats negative correlations as weaker but still informative
  - Preserves signal instead of falling back to uniform distribution
  - Logs when negative scores are detected for debugging
  - Also fixed `AssertionError: Importance sum should be positive` by adding multiple defensive fallback layers in `normalize_importance()`
- **Files**: `TRAINING/ranking/multi_model_feature_selection.py`

### Interval Detection Improvements

**Large Gap Filtering**
- **Issue**: Large gaps (overnight/weekend, e.g., 270m, 1210m) were contaminating interval detection
- **Impact**: False "unclear interval" warnings on clean 5m data
- **Fix**: 
  - Added median-based gap filtering that excludes gaps > 10x median
  - Configurable via `pipeline.data_interval.max_gap_factor`
  - Two-stage filtering: first removes gaps > 1 day, then removes gaps > 10x median
  - Prevents 270m/1210m gaps from contaminating detection when base cadence is 5m
- **Files**: `TRAINING/utils/data_interval.py`

**Fixed Interval Mode**
- **Enhancement**: Added `interval_detection.mode=fixed` config option to skip auto-detection entirely when interval is known
- **Usage**: When `mode=fixed`, uses `data.bar_interval` directly without auto-detection
- **Benefit**: Eliminates all warnings for known intervals
- **Files**: `TRAINING/utils/data_interval.py`

**Warning Noise Reduction**
- **Enhancement**: Downgraded "unclear interval" warnings from WARNING to INFO level
- **Rationale**: These messages are expected when large gaps are filtered out and the default is being used correctly
- **Result**: All interval detection messages now at INFO/DEBUG level (not WARNING)
- **Files**: `TRAINING/utils/data_interval.py`

### Leakage Detection Critical Bug Fixes

**CRITICAL BUG FIX: Detection Confidence Calculation**
- **Issue**: Detection confidence was using raw importance values (0-1) directly as confidence scores
  - Example: Feature with 15% importance ‚Üí 15% confidence
  - min_confidence threshold = 80% (0.8)
  - Result: ALL detections were filtered out, so auto-fixer reported 0 leaks even when perfect scores were detected
- **Impact**: Valid leak detections were silently filtered out, making it appear that detection wasn't working
- **Root Cause**: Misunderstanding of confidence vs importance - confidence should reflect suspicion level (perfect score = high suspicion), not just raw importance
- **Fix**: Changed confidence calculation to account for perfect-score context:
  - Base confidence: 0.85 (perfect score = high suspicion)
  - Importance boost: up to +0.1 based on importance value
  - Final confidence: 0.85-0.95 (always above 80% threshold)
- **Result**: Detections now get appropriate confidence scores and are properly applied
- **Files**: `TRAINING/common/leakage_auto_fixer.py`

**Enhanced Detection When Importances Missing**
- **Issue**: When `model_importance` was not provided, detection only checked known patterns (p_, y_, fwd_ret_, etc.), missing subtle leaks
- **Impact**: Detection couldn't identify leaky features when importances weren't passed from upstream
- **Fix**: 
  - Compute feature importances on-the-fly using quick RandomForest when missing
  - Use computed importances to find suspicious features
  - Ensures detection works even when importances aren't passed
- **Files**: `TRAINING/common/leakage_auto_fixer.py`

**Improved Detection Visibility and Diagnostics**
- **Enhancement**: Added comprehensive logging for detection process
- **Features**:
  - Confidence distribution logging (high vs low confidence detections)
  - Lists top detections that will be fixed vs filtered out
  - Warnings when perfect score detected but no leaks found
  - Explains possible reasons: structural leakage, already excluded, detection methods need improvement
  - INFO-level logging for auto-fixer inputs and top features by importance
- **Files**: 
  - `TRAINING/common/leakage_auto_fixer.py`
  - `TRAINING/ranking/predictability/model_evaluation.py`

### Reproducibility Tracking Tolerance Bands & Enhancements

**Tolerance Bands with STABLE/DRIFTING/DIVERGED Classification**
- **Enhancement**: Replaced binary SAME/DIFFERENT classification with three-tier system
- **Problem**: Binary system flagged tiny differences (0.08% shifts) as DIFFERENT, causing alert fatigue
- **Solution**: Three-tier classification:
  - **STABLE**: Differences within noise (passes both abs and rel thresholds, optional z-score) ‚Üí INFO level
  - **DRIFTING**: Small but noticeable changes (within 2x thresholds) ‚Üí INFO level
  - **DIVERGED**: Real reproducibility issues (exceeds 2x thresholds) ‚Üí WARNING level
- **Features**:
  - Configurable thresholds per metric (roc_auc, composite, importance) in `safety_config.yaml`
  - Supports absolute, relative, and z-score thresholds
  - Uses reported œÉ (std_score) when available for statistical significance
  - Example: 0.08% ROC-AUC shift with z=0.06 is now STABLE (INFO) instead of DIFFERENT (WARNING)
- **Config**: Added `safety.reproducibility` section with thresholds and `use_z_score` option
- **Files**: 
  - `TRAINING/utils/reproducibility_tracker.py` - Complete rewrite of classification logic
  - `CONFIG/training_config/safety_config.yaml` - Added reproducibility thresholds config

**Reproducibility Tracking Error Handling Fixes**
- **Issue**: Missing `List` and `Tuple` imports causing `NameError: name 'List' is not defined`
- **Impact**: Reproducibility tracking failed silently, breaking output generation
- **Fix**: 
  - Added `List` and `Tuple` to typing imports
  - Added comprehensive error handling in `_find_previous_log_files()` to prevent crashes
  - Changed exception logging from DEBUG to WARNING level for visibility
  - Added traceback logging for debugging
- **Files**: `TRAINING/utils/reproducibility_tracker.py`

### Reproducibility Tracking & Auto-Fixer Fixes

**Reproducibility Tracking Directory Structure Fix**
- **Issue**: Reproducibility logs were stored in shared location, mixing different modules (target ranking, feature selection, model training). Also couldn't find previous runs because each run uses timestamped directory (e.g., `test_e2e_ranking_unified_20251211_133358`).
- **Impact**: 
  - Modules couldn't be properly separated (all logs in one place)
  - Previous runs weren't found (stored in different timestamped directories)
  - Always showed "First run" even when previous runs existed
- **Fix**: 
  - Each module now has its own reproducibility log in module-specific subdirectory:
    - Target ranking: `{output_dir}/target_rankings/reproducibility_log.json`
    - Feature selection: `{output_dir}/feature_selections/reproducibility_log.json`
    - Model training: `{output_dir}/training_results/reproducibility_log.json`
  - Added `search_previous_runs` option (enabled for all modules)
  - `_find_previous_log_files()` searches parent directories (up to 3 levels) for previous runs from same module
  - Merges runs from current log + previous logs, sorts by timestamp, uses most recent
- **Result**: Modules are properly separated, and previous runs are found across different timestamped output directories
- **Files**: 
  - `TRAINING/utils/reproducibility_tracker.py` - Added module-specific directory support and previous run search
  - `TRAINING/ranking/predictability/model_evaluation.py` - Updated to use target_rankings/ subdirectory
  - `TRAINING/ranking/feature_selector.py` - Updated to use feature_selections/ subdirectory
  - `TRAINING/training_strategies/training.py` - Updated to use training_results/ subdirectory

**Auto-Fixer Logging Format Error Fix**
- **Issue**: `ValueError: Invalid format specifier` when logging auto-fixer inputs. Code tried to use conditional expression (`actual_train_score:.4f if actual_train_score else None`) directly in format specifier, which is invalid Python syntax.
- **Impact**: Auto-fixer logging crashed with format error, preventing visibility into what was being passed to detection
- **Fix**: Format the value first, then use in f-string:
  ```python
  train_score_str = f"{actual_train_score:.4f}" if actual_train_score is not None else "None"
  logger.info(f"üîß Auto-fixer inputs: train_score={train_score_str}, ...")
  ```
- **Result**: Auto-fixer logging now works correctly, showing inputs for debugging
- **Files**: `TRAINING/ranking/predictability/model_evaluation.py`

### Cross-Sectional Sampling & Config Parameter Fixes

**Critical Bug Fix: max_cs_samples Filtering**
- **Issue**: `max_cs_samples` filtering code was in wrong code block (`else:` when `time_col is None`), so it never executed when timestamps exist
- **Impact**: Large dataframes (25,000+ rows) were built despite `max_cs_samples=1000` setting, causing unnecessary memory usage and slower processing
- **Fix**: Moved filtering code into `if time_col is not None:` block, right after `min_cs` filter
- **Result**: Now properly limits cross-sectional samples per timestamp, dramatically reducing dataframe size
- **File**: `TRAINING/utils/cross_sectional_data.py`

**Config Parameter Passing Fix**
- **Issue**: `min_cs`, `max_cs_samples`, and `max_rows_per_symbol` from test config were not being passed to `rank_targets()` function
- **Impact**: Test config settings (e.g., `min_cs=3`) were ignored, default values (e.g., `min_cs=10`) were used instead
- **Fix**: 
  - Extract these values from `train_kwargs` at start of `train_with_intelligence()` method
  - Pass them to `rank_targets_auto()` and then to `rank_targets()`
  - Ensures config-driven settings are actually used throughout pipeline
- **Files**: `TRAINING/orchestration/intelligent_trainer.py`

**Comprehensive Reproducibility Tracking**
- **Enhancement**: Added reproducibility tracking to all deterministic pipeline stages
- **Architectural Improvement**: Moved tracking from entry points to computation modules
  - **Target Ranking**: Tracking in `evaluate_target_predictability()` in `model_evaluation.py` (computation module)
  - **Feature Selection**: Tracking in `select_features_for_target()` in `feature_selector.py` (computation module)
  - **Model Training**: Tracking in training loop in `training.py` (computation module)
- **Benefits**:
  - Works regardless of entry point (intelligent_trainer, standalone scripts, programmatic calls)
  - Single source of tracking logic (no duplication)
  - Better architecture: computation functions handle their own tracking
  - Easier maintenance: update tracking in one place
- **Visibility**: Fixed issue where logs weren't appearing - now logs to both internal and main loggers
- **Files Modified**: 
  - `TRAINING/ranking/predictability/model_evaluation.py` - Added tracking to `evaluate_target_predictability()`
  - `TRAINING/ranking/feature_selector.py` - Added tracking to `select_features_for_target()`
  - `TRAINING/training_strategies/training.py` - Added tracking after model training
  - `TRAINING/ranking/target_ranker.py` - Removed duplicate tracking (now in computation module)
  - `TRAINING/ranking/predictability/main.py` - Removed duplicate tracking (now in computation module)
  - `TRAINING/ranking/multi_model_feature_selection.py` - Removed duplicate tracking (now in computation module)
  - `TRAINING/utils/reproducibility_tracker.py` - Enhanced with visibility fixes

### Config Parameter Validation & Silent Error Visibility

**Config Cleaner Utility**
- **New module**: `TRAINING/utils/config_cleaner.py` providing systematic parameter validation
- **Function**: `clean_config_for_estimator()` uses `inspect.signature()` to validate parameters against actual estimator constructors
- **Features**:
  - Automatically removes duplicate parameters (if also passed explicitly via `extra_kwargs`)
  - Automatically removes unknown parameters (not in estimator's `__init__` signature)
  - Logs what was stripped for visibility (DEBUG level, can be raised to WARNING for auditing)
  - Handles edge cases: None configs, non-dict configs, None extra_kwargs
  - Maintains SST (Single Source of Truth) - values still come from config/defaults, but only valid keys are passed
  - Prevents entire class of parameter passing errors that would occur with config drift
- **Integration**: Applied to `multi_model_feature_selection.py`, `task_types.py`, `cross_sectional_feature_ranker.py`
- **Future-proof**: Makes codebase resilient to future changes in `inject_defaults` or model library updates

**Reproducibility Tracking Module**
- **New module**: `TRAINING/utils/reproducibility_tracker.py` - reusable `ReproducibilityTracker` class
- **Features**:
  - Generic module usable across all pipeline stages
  - Integrated into target ranking and feature selection pipelines
  - Compares current run to previous runs with configurable tolerances (0.1% for scores, 1% for importance)
  - Stores run history in JSON format (keeps last 10 runs per item by default)
  - Logs differences with percentage changes and absolute deltas
  - Flags reproducible runs (‚úÖ) vs different runs (‚ö†Ô∏è)
- **Documentation**: `DOCS/03_technical/implementation/REPRODUCIBILITY_TRACKING.md` with API reference, integration examples, best practices, and troubleshooting guide
- **Reproducibility comparison logging**: Automatic comparison of target ranking results to previous runs
  - Stores run summaries in `reproducibility_log.json` (keeps last 10 runs per target)
  - Compares current vs previous: mean score, std score, importance, composite score
  - Integrated into `rank_target_predictability.py` - logs after each target evaluation summary

**Silent Error Visibility Improvements**
- **ImportError fallback in target validation**: Added DEBUG logging when `validate_target` module is unavailable and fallback validation is used
- **Importance extraction validation**: Added comprehensive validation and logging:
  - Validates importance is not None before use
  - Validates importance is correct type (pd.Series) and converts if needed
  - Validates importance length matches feature count and pads/truncates with warnings
  - Wraps entire extraction in try/except with ERROR-level logging on failures
- **Bare except clauses**: Replaced all bare `except:` clauses with `except Exception as e:` and added DEBUG-level logging:
  - Stability selection bootstrap loop now logs when iterations fail
  - RFE score computation now logs when scoring fails
- **Reproducibility tracker visibility**: Fixed issue where reproducibility comparison logs were not visible in main script output by ensuring logger propagation and fallback to main logger
- **Config loading failures**: Added DEBUG-level logging to all config loading exception handlers (SHAP config, RFE config, Boruta config, max_samples, etc.) that previously silently fell back to defaults
- **Empty results aggregation**: Added WARNING-level log when all model families fail and empty results are returned
- **Logging levels**: All silent failures now have appropriate logging levels (DEBUG for expected fallbacks, WARNING for unexpected conditions, ERROR for actual failures)

**Files Modified**
- `TRAINING/utils/config_cleaner.py` ‚Äî New module
- `TRAINING/utils/reproducibility_tracker.py` ‚Äî Enhanced with visibility fixes
- `TRAINING/ranking/multi_model_feature_selection.py` ‚Äî Integrated config cleaner, added importance validation, fixed bare excepts
- `TRAINING/utils/task_types.py` ‚Äî Integrated config cleaner with proper closure handling
- `TRAINING/ranking/cross_sectional_feature_ranker.py` ‚Äî Integrated config cleaner, added stability tracking
- `CONFIG/config_loader.py` ‚Äî Hardened `inject_defaults` to handle None configs

### Training Routing & Planning System

**Config-Driven Routing Decisions**
- **New System**: Comprehensive routing system that makes reproducible, config-driven decisions about where to train models for each `(target, symbol)` pair
- **Routing States**: `ROUTE_CROSS_SECTIONAL`, `ROUTE_SYMBOL_SPECIFIC`, `ROUTE_BOTH`, `ROUTE_EXPERIMENTAL_ONLY`, `ROUTE_BLOCKED`
- **Decision Factors**: Based on feature selection scores, stability analysis, leakage detection, and sample sizes
- **Configuration**: Full routing policy in `CONFIG/training_config/routing_config.yaml` with thresholds, stability requirements, experimental lane settings
- **Files**:
  - `CONFIG/training_config/routing_config.yaml` - Routing policy configuration
  - `TRAINING/orchestration/training_router.py` - Core routing decision engine
  - `TRAINING/orchestration/metrics_aggregator.py` - Collects metrics from pipeline outputs
  - `TRAINING/orchestration/routing_integration.py` - Integration hooks
  - `TRAINING/orchestration/training_plan_generator.py` - Converts routing ‚Üí training jobs
  - `TRAINING/orchestration/training_plan_consumer.py` - Filters training based on plan
  - `TRAINING/orchestration/generate_routing_plan.py` - CLI entry point
  - `DOCS/02_reference/training_routing/` - Complete documentation

**Automatic Routing Plan Generation**
- **Integration**: Automatically generates routing plan after feature selection completes
- **Artifacts**: 
  - `METRICS/routing_candidates.parquet` (or CSV fallback) - All metrics
  - `METRICS/routing_plan/routing_plan.json` - Machine-readable plan
  - `METRICS/routing_plan/routing_plan.md` - Human-readable report
- **Metrics Aggregation**: Collects metrics from feature selection outputs, stability snapshots, leakage detection
- **Stability Classification**: Classifies stability from metrics (STABLE/DRIFTING/DIVERGED/UNKNOWN) using configurable thresholds

**Training Plan Generation & Consumption**
- **Training Plan**: Converts routing decisions into actionable training jobs with priorities, model families, reasons
- **Automatic Filtering**: Training phase automatically filters targets based on training plan
- **Current Status**:
  - ‚úÖ Cross-sectional training filtering: Fully implemented - only targets with CS jobs in plan are trained
  - ‚úÖ Training plan generation: Fully implemented for both CS and symbol-specific routes
  - ‚ö†Ô∏è Symbol-specific training execution: Plan generated but per-symbol model execution requires future pipeline integration
- **Artifacts**:
  - `globals/training_plan/training_plan.json` - Machine-readable job specifications (primary location)
  - `globals/training_plan/training_plan.md` - Human-readable report (primary location)
  - Note: `METRICS/training_plan/` is supported as legacy fallback for backward compatibility
- **Integration**: Automatically integrated into `IntelligentTrainer.train_with_intelligence()`
- **Backward Compatible**: If plan doesn't exist, all targets trained (no breaking changes)

**What Gets Trained**
- ‚úÖ **Cross-sectional models**: Trained for targets with CS jobs in plan (pooled across all symbols)
- ‚ö†Ô∏è **Symbol-specific models**: Plan generated but execution not yet integrated (future enhancement)

**2-Stage Training Pipeline & One-Command Flow**
- **Enhancement**: `--model-types sequential` now trains all 20 models (both sequential and cross-sectional) using 2-stage approach
- **Stage 1 (CPU)**: 10 CPU-only models run first (LightGBM, XGBoost, QuantileLightGBM, RewardBased, NGBoost, GMMRegime, ChangePoint, FTRLProximal, Ensemble, MetaLearning)
- **Stage 2 (GPU)**: 10 GPU models run second (4 TensorFlow: MLP, VAE, GAN, MultiTask + 6 PyTorch: CNN1D, LSTM, Transformer, TabCNN, TabLSTM, TabTransformer)
- **Benefits**: Prevents thread pollution between CPU and GPU libraries, efficient resource usage
- **One-Command Flow**: Complete end-to-end pipeline in single command:
  - Target ranking ‚Üí Feature selection ‚Üí Training plan generation ‚Üí Training execution
  - Training plan automatically passed to training phase (auto-detected from common locations)
  - No manual intervention required
- **Auto-Detection**: Training phase automatically finds training plan in common locations (`results/globals/training_plan/` primary, `results/METRICS/training_plan/` legacy fallback, etc.)
- **Convenience Module**: `TRAINING/training_strategies/train_sequential.py` - Python module for one-command sequential training
- **Files**:
  - `TRAINING/training_strategies/main.py` - Added 2-stage ordering, auto-detection, model family filtering
  - `TRAINING/training_strategies/train_sequential.py` - New convenience module
  - `DOCS/02_reference/training_routing/END_TO_END_FLOW.md` - Complete flow documentation
  - `DOCS/02_reference/training_routing/TWO_STAGE_TRAINING.md` - 2-stage approach details
  - `DOCS/02_reference/training_routing/ONE_COMMAND_TRAINING.md` - Usage examples
- **Status**: Currently being tested

### Cross-Sectional Stability Tracking

**Factor Robustness Analysis for Cross-Sectional Features**
- **Enhancement**: Added stability tracking to cross-sectional feature selection using the feature importance stability system
- **Purpose**: Tracks factor robustness across runs to identify persistent global features vs. symbol-specific noise
- **Metrics**: 
  - Top-K overlap (Jaccard similarity) between runs
  - Kendall tau (rank correlation) for feature ordering consistency
  - Snapshot-based analysis across multiple runs
- **Classification**: Three-tier system (STABLE/DRIFTING/DIVERGED) with stricter thresholds than per-symbol:
  - STABLE: overlap ‚â•0.75, tau ‚â•0.65 (vs. 0.7/0.6 for per-symbol)
  - DRIFTING: Moderate stability (within 2x thresholds)
  - DIVERGED: Low stability (exceeds 2x thresholds)
- **Storage**: 
  - Snapshots saved to `artifacts/feature_importance/{target}/cross_sectional_panel/`
  - Metadata stored in `cross_sectional_stability_metadata.json`
  - Results included in main feature selection metadata
- **Logging**: Compact one-line logs showing stability status, metrics, and snapshot count. Summary log at end of feature selection.
- **Integration**: Automatically runs after cross-sectional importance computation when CS ranking is enabled
- **Benefits**:
  - Identifies persistent global factors vs. symbol-specific quirks
  - Provides institutional-grade factor robustness analysis
  - Enables feature routing (stable global ‚Üí production, unstable ‚Üí experimental)
  - Matches workflows used by major quant infrastructure systems
- **Files**: 
  - `TRAINING/ranking/cross_sectional_feature_ranker.py` - Added `compute_cross_sectional_stability()` function
  - `TRAINING/ranking/feature_selector.py` - Integrated stability tracking after CS importance computation
  - Uses existing `TRAINING/stability/feature_importance/` system for snapshot storage and analysis

---

## Fixed

### Parameter Passing Errors & Silent Failures

**Systematic Parameter Passing Error Prevention**
- **Root cause**: `inject_defaults` was injecting parameters (like `random_seed`, `random_state`, `n_jobs`, `num_threads`, `threads`) into model configs, but model instantiation code was also passing these (or renamed versions) explicitly, leading to duplicate arguments or unknown parameters.
- **Solution**: Implemented systematic fix using shared `clean_config_for_estimator()` utility from `TRAINING/utils/config_cleaner.py`:
  - **All model families in multi_model_feature_selection.py**: LightGBM, XGBoost, RandomForest, MLPRegressor, CatBoost, Lasso now use `clean_config_for_estimator()` helper
  - **All model constructors in task_types.py**: Fixed with proper closure handling (explicit copies in lambda closures to prevent reference issues)
  - **Cross-sectional feature ranker**: LightGBM and XGBoost now use config cleaner
  - **inject_defaults hardening**: Now handles None configs gracefully, prevents "argument of type 'NoneType' is not iterable" errors
  - Prevents "got multiple values for keyword argument" errors (e.g., `random_seed` passed both in config and explicitly)
  - Prevents "unexpected keyword argument" errors (e.g., `num_threads` for RandomForest, `n_jobs` for MLPRegressor/Lasso)
  - Lambda closure fix: All lambda functions in `task_types.py` now capture explicit copies (`config_final`, `extra_final`) to prevent reference issues
  - Future-proof: Any new parameters added by `inject_defaults` will be automatically filtered if they're duplicates or unknown

**Model Config Parameter Sanitization**
- **Fixed**: Critical TypeError and ValueError errors when global config defaults (`random_seed`, `n_jobs`, `early_stopping_rounds`) were injected into model constructors
- **sklearn models** (RandomForest, MLPRegressor, Lasso): Remove `random_seed` (use `random_state` instead)
- **CatBoost**: Remove `n_jobs` (uses `thread_count` instead)
- **XGBoost/LightGBM**: Remove all early stopping params (`early_stopping_rounds`, `callbacks`, `eval_set`, `eval_metric`) in feature selection mode (requires `eval_set` which isn't available)
- Determinism preserved: All models explicitly set `random_state`/`random_seed` using deterministic `model_seed` per symbol/target combination
- Uses `.copy()` and `.pop()` for explicit parameter sanitization to prevent incompatible parameters from reaching model constructors

**Silent Error Visibility Improvements**
- **ImportError fallback in target validation**: Added DEBUG logging when `validate_target` module is unavailable and fallback validation is used
- **Importance extraction validation**: Added comprehensive validation and logging:
  - Validates importance is not None before use
  - Validates importance is correct type (pd.Series) and converts if needed
  - Validates importance length matches feature count and pads/truncates with warnings
  - Wraps entire extraction in try/except with ERROR-level logging on failures
- **Bare except clauses**: Replaced all bare `except:` clauses with `except Exception as e:` and added DEBUG-level logging:
  - Stability selection bootstrap loop now logs when iterations fail
  - RFE score computation now logs when scoring fails
- **Reproducibility tracker visibility**: Fixed issue where reproducibility comparison logs were not visible in main script output by ensuring logger propagation and fallback to main logger
- **Config loading failures**: Added DEBUG-level logging to all config loading exception handlers (SHAP config, RFE config, Boruta config, max_samples, etc.) that previously silently fell back to defaults
- **Empty results aggregation**: Added WARNING-level log when all model families fail and empty results are returned
- **Logging levels**: All silent failures now have appropriate logging levels (DEBUG for expected fallbacks, WARNING for unexpected conditions, ERROR for actual failures)

**Files Modified**
- `TRAINING/utils/config_cleaner.py` ‚Äî New module
- `TRAINING/ranking/multi_model_feature_selection.py` ‚Äî Integrated config cleaner, added importance validation, fixed bare excepts
- `TRAINING/utils/task_types.py` ‚Äî Integrated config cleaner with proper closure handling
- `TRAINING/ranking/cross_sectional_feature_ranker.py` ‚Äî Integrated config cleaner
- `CONFIG/config_loader.py` ‚Äî Hardened `inject_defaults` to handle None configs
- `TRAINING/utils/reproducibility_tracker.py` ‚Äî Enhanced with visibility fixes

---

## Cohort-Aware Reproducibility & RESULTS Organization (2025-12-11, Late)

### Added

- **Cohort-Aware Reproducibility System**
  - Runs organized by data cohort (sample size, symbols, date range, config)
  - Sample-adjusted drift detection using z-scores
  - Only compares runs within the same cohort for statistically meaningful comparisons
  - Automatic cohort identification and matching
  - `INCOMPARABLE` classification when cohorts differ significantly

- **RESULTS Directory Organization**
  - All runs (test and production) stored in `RESULTS/` directory
  - Automatic organization by cohort: `RESULTS/{cohort_id}/{run_name}/`
  - Runs start in `RESULTS/_pending/{run_name}/` and move to cohort directory after first target
  - Keeps root directory clean and organized

- **Integrated Config Backups**
  - Config backups now stored in run directory: `RESULTS/{cohort_id}/{run_name}/backups/`
  - Organized by target: `backups/{target}/{timestamp}/`
  - Moves with run directory when organized by cohort
  - Replaces separate `CONFIG/backups/` structure (backward compatible)

- **Enhanced Metadata**
  - `metadata.json` now includes full `symbols` list (sorted, deduplicated)
  - Essential for debugging cohort behavior ("why did this cohort behave weirdly?")
  - Stable git diffs (sorted list)
  - Complete cohort manifest

- **Unified Metadata Extractor**
  - New `TRAINING/utils/cohort_metadata_extractor.py` utility
  - Centralized logic for extracting cohort metadata from various data sources
  - Used across target ranking, feature selection, and training modules
  - Reduces code duplication and ensures consistency
  - Handles numpy arrays, pandas DataFrames, lists, dicts, etc.

- **Immediate File Writes**
  - All reproducibility files flushed to disk immediately with `fsync()`
  - Real-time visibility during runs (not waiting until end)
  - INFO-level logging for file operations

### Changed

- **ReproducibilityTracker**
  - Default mode is now cohort-aware (`cohort_aware=True` by default)
  - Enhanced logging with INFO-level messages for mode selection
  - Better error classification (`IO_ERROR`, `SERIALIZATION_ERROR`, `UNKNOWN_ERROR`)
  - Operational metrics in `stats.json` for visibility
  - Immediate file writes with `flush()` and `os.fsync()`

- **IntelligentTrainer**
  - Output directories automatically go to `RESULTS/` for all runs
  - Automatic cohort-based organization after first target
  - Cache attributes initialized early to prevent AttributeError
  - Runs start in `RESULTS/_pending/` and move to `RESULTS/{cohort_id}/` after cohort identified

- **LeakageAutoFixer**
  - Accepts `output_dir` parameter to store backups in run directory
  - Backups integrated into RESULTS structure
  - Maintains backward compatibility with `CONFIG/backups/` if `output_dir` not provided

- **All Pipeline Modules**
  - Target ranking, feature selection, and training now use unified `cohort_metadata_extractor`
  - Consistent cohort metadata extraction across all stages
  - All modules store `cohort_context` after data preparation for later use

### Files Changed

- [Root Changelog](../../../CHANGELOG.md)
- [2025-12-10](2025-12-10.md)

# Changelog â€” 2025-12-12

**Sample Size Binning System, Trend Analysis System Extension (Feature Selection), Cohort-Aware Reproducibility System, RESULTS Directory Organization, Integrated Backups, Enhanced Metadata**

For a quick overview, see the [root changelog](../../../CHANGELOG.md).  
For other dates, see the [changelog index](README.md).

---

## Added

### Sample Size Binning System

**Directory Organization by Sample Size Bins**
- **Enhancement**: Implemented audit-grade sample size binning for RESULTS directory organization
- **Structure**: `RESULTS/sample_25k-50k/{run_name}/` instead of exact `N_effective`
- **Bins**: 9 bins (0-5k, 5k-10k, 10k-25k, 25k-50k, 50k-100k, 100k-250k, 250k-500k, 500k-1M, 1M+)
- **Boundaries**: EXCLUSIVE upper bounds (`bin_min <= N_effective < bin_max`) - unambiguous binning
- **Versioning**: `sample_bin_v1` stored in metadata for backward compatibility
- **Early Estimation**: Automatically estimates `N_effective` from data files or existing metadata during initialization
- **Metadata**: Bin info stored in `metadata.json`: `bin_name`, `bin_min`, `bin_max`, `binning_scheme_version`
- **Files**:
  - `TRAINING/orchestration/intelligent_trainer.py` - Added `_get_sample_size_bin()` and `_estimate_n_effective_early()`
  - `TRAINING/utils/reproducibility_tracker.py` - Added `_compute_sample_size_bin()` static method

**Benefits**:
- Easy comparison of runs with similar sample sizes (e.g., all ~25k runs in `sample_25k-50k/`)
- Trend analysis friendly: Series won't fragment when `N_effective` jitters slightly
- Cleaner structure: Only 9 top-level directories instead of hundreds
- Audit-grade: Boundaries are deterministic, versioned, and stored in metadata
- Trend series stability: Bin NOT included in series keys (uses stable identity: cohort_id, stage, target)

### Trend Analysis System Extension

**Feature Selection Integration**
- **Enhancement**: Extended trend analysis system to feature selection (single symbol + aggregated) and cross-sectional feature ranking
- **Coverage**: Trend analysis now integrated into:
  - Target Ranking (`TRAINING/ranking/predictability/model_evaluation.py`)
  - Feature Selection - Single symbol + aggregated (`TRAINING/ranking/feature_selector.py`)
  - Cross-Sectional Feature Ranking (`TRAINING/ranking/cross_sectional_feature_ranker.py`)
- **API**: All stages use the same `log_run()` API with `RunContext` and include trend analysis automatically
- **Metrics**: Tracks `n_selected`, `n_features_selected`, `mean_consensus`, `std_consensus` for feature selection
- **CS Metrics**: Tracks `cs_importance_score`, `n_features_evaluated` for cross-sectional ranking
- **Metadata**: Trend metadata stored in `metadata.json` (same format as target ranking)
- **Fallback**: Gracefully falls back to legacy `log_comparison()` API if `RunContext` unavailable
- **Files**: 
  - `TRAINING/ranking/feature_selector.py` - Updated to use `log_run()` API with trend analysis
  - `TRAINING/ranking/cross_sectional_feature_ranker.py` - Added reproducibility tracking with trend analysis

### Files Modified

#### Feature Selection
- `TRAINING/ranking/feature_selector.py`
  - Updated reproducibility tracking to use new `log_run()` API with `RunContext`
  - Integrated trend analysis (same infrastructure as target ranking)
  - Falls back to legacy `log_comparison()` API if `RunContext` unavailable
  - Tracks metrics: `n_selected`, `n_features_selected`, `mean_consensus`, `std_consensus`

#### Cross-Sectional Feature Ranking
- `TRAINING/ranking/cross_sectional_feature_ranker.py`
  - Added `output_dir` parameter to `compute_cross_sectional_importance()`
  - Integrated reproducibility tracking with `log_run()` API and trend analysis
  - Tracks CS-specific metrics: `cs_importance_score`, `n_features_evaluated`
  - Passes `output_dir` from `feature_selector.py` for tracking

### Features

- **Trend Analysis**: Both single-symbol and cross-sectional feature selection now compute trends automatically
- **Metadata Storage**: Trend metadata stored in `metadata.json` (same format as target ranking)
- **Skip Logging**: Explicit skip reasons when insufficient runs (no silent failures)
- **Audit Reports**: Audit violations/warnings logged for feature selection runs
- **Graceful Fallback**: Falls back to legacy API if `RunContext` unavailable

### Coverage

Trend analysis is now integrated into:
- âœ… Target Ranking (`TRAINING/ranking/predictability/model_evaluation.py`)
- âœ… Feature Selection - Single symbol + aggregated (`TRAINING/ranking/feature_selector.py`)
- âœ… Cross-Sectional Feature Ranking (`TRAINING/ranking/cross_sectional_feature_ranker.py`)

All stages use the same `log_run()` API with `RunContext` and include trend analysis automatically.

### Documentation

- Updated `CHANGELOG.md` with Trend Analysis System highlight
- Updated `DOCS/03_technical/implementation/TREND_ANALYZER_VERIFICATION.md` to include feature selection coverage
- Updated `DOCS/INDEX.md` with trend analyzer verification guide reference

### Testing

To verify:
1. Run feature selection multiple times (same target, same cohort)
2. Check logs for trend messages: `ðŸ“ˆ Trend (n_selected): slope=.../day`
3. Check `metadata.json` for `trend` sections
4. Verify `TREND_REPORT.json` includes feature selection series

## Fixed (2025-12-12)

### Documentation Link Fixes
- **Issue**: 404 errors when navigating between root and lower-level documentation files
- **Cause**: Relative paths like `../ROADMAP.md` worked from `DOCS/` but broke when viewing from root, and vice versa
- **Fix**: Calculated correct relative paths based on each file's depth from `DOCS/` root
  - Root-level files (`ROADMAP.md`, `CHANGELOG.md`) now use proper `../` paths
  - Testing notice moved to `DOCS/02_reference/testing/TESTING_NOTICE.md`
  - Root-level directories (`LEGAL/`, `DATA_PROCESSING/`, `CONFIG/`, `TRAINING/`) use proper `../` paths
  - Links work correctly when viewing from any location in the repository
- **Files Fixed**: 26 documentation files updated with correct relative paths
- **Impact**: All documentation links now work correctly regardless of viewing location

### XGBoost 3.1+ GPU Compatibility
- **Issue**: XGBoost 3.1+ removed `gpu_id` parameter, causing `gpu_id has been removed since 3.1. Use device instead` errors
- **Fix**: Removed all `gpu_id` references from XGBoost GPU detection code. Now uses `device='cuda'` with `tree_method='hist'` for XGBoost 3.1+, with automatic fallback to legacy `tree_method='gpu_hist'` for older versions
- **Files**:
  - `TRAINING/ranking/predictability/model_evaluation.py` (target ranking)
  - `TRAINING/ranking/multi_model_feature_selection.py` (feature selection)
  - `CONFIG/training_config/gpu_config.yaml` (updated comments)
- **Impact**: XGBoost GPU acceleration now works with XGBoost 3.1+ without errors

### CatBoost GPU Verification
- **Issue**: CatBoost requires `task_type='GPU'` to actually use GPU (devices alone is ignored), but verification was insufficient
- **Fix**: Added explicit pre-instantiation and post-instantiation verification that `task_type='GPU'` is set. Enhanced logging to show exactly what params are being passed. Added educational hints about CatBoost quantization behavior (CPU first, then GPU)
- **Files**:
  - `TRAINING/ranking/predictability/model_evaluation.py` (target ranking)
  - `TRAINING/ranking/multi_model_feature_selection.py` (feature selection)
- **Impact**: CatBoost GPU usage is now explicitly verified and logged, making it easier to debug if GPU isn't being used

### Process Deadlock Fix (readline library conflict)
- **Issue**: Process hangs for 10+ minutes on small datasets, CPU at 100%, error: `sh: symbol lookup error: sh: undefined symbol: rl_print_keybinding`
- **Cause**: Conda environment's `readline` library conflicts with system's `readline` library, causing subprocess calls (like `nvidia-smi` checks) to fail and retry indefinitely
- **Fix**: Created `TRAINING/common/subprocess_utils.py` with `safe_subprocess_run()` function that automatically sets safe environment variables (`TERM=dumb`, `SHELL=/usr/bin/bash`, `INPUTRC=/dev/null`) to prevent readline conflicts. Updated all subprocess calls to use this utility.
- **Files**:
  - `TRAINING/common/subprocess_utils.py` (new utility module)
  - `TRAINING/model_fun/xgboost_trainer.py` (nvidia-smi check)
  - `TRAINING/common/leakage_auto_fixer.py` (git calls)
  - `TRAINING/orchestration/routing_integration.py` (git calls)
  - `TRAINING/orchestration/generate_routing_plan.py` (git calls)
  - `TRAINING/utils/reproducibility_tracker.py` (git calls)
- **Impact**: Prevents process deadlocks from readline library conflicts. All subprocess calls now protected with safe environment variables. Better error messages with troubleshooting hints.
- **Prevention**: The system already sets these environment variables in `isolation_runner.py`, but this fix ensures all subprocess calls (including those outside isolation) are protected.

### Mutual Information Random State (SST Compliance)
- **Issue**: `random_state` was accessed directly from config dict, causing `KeyError` when missing
- **Fix**: Changed to `.get('random_state')` with fallback to SST determinism system (`stable_seed_from()`)
- **Files**:
  - `TRAINING/ranking/predictability/model_evaluation.py` (lines 1883-1889)
  - `TRAINING/ranking/predictability/leakage_detection.py` (lines 1531-1537)
- **Impact**: Mutual Information feature selection now works reliably with SST determinism system
- **No hardcoded defaults**: All `random_state` values now come from config or SST

### Audit Violation Fix (Final Gatekeeper Alignment)
- **Issue**: Audit system was checking `resolved_config.feature_lookback_max_minutes` computed BEFORE Final Gatekeeper dropped features, causing false violations
- **Fix**: Recompute `resolved_config.feature_lookback_max_minutes` AFTER Final Gatekeeper runs, so audit sees actual remaining features
- **Files**:
  - `TRAINING/ranking/predictability/model_evaluation.py` (lines 3110-3124)
  - `TRAINING/utils/resolved_config.py` (added `feature_lookback_max_minutes` field to `ResolvedConfig` dataclass)
- **Impact**: Eliminates false audit violations when Final Gatekeeper drops 1440m features
- **Result**: Audit now correctly validates against features that actually remain after safety checks

### CatBoost Feature Importance Fix
- **Issue**: `CatBoost.get_feature_importance()` requires training dataset to be passed explicitly
- **Error**: "Model has no meta information needed to calculate feature importances. Pass training dataset to this function."
- **Fix**: Pass `data=X` and `type='PredictionValuesChange'` to `get_feature_importance()`
- **Files**:
  - `TRAINING/ranking/predictability/model_evaluation.py` (lines 1795-1798)
  - `TRAINING/ranking/predictability/leakage_detection.py` (lines 1444-1447)
- **Impact**: CatBoost feature importance calculation now works correctly in target ranking

## Added (2025-12-12)

### License & Commercial Use Banner
- **Enhancement**: Added professional startup banner that prints licensing and commercial use information
- **Purpose**: Ensures users see licensing requirements even when using automated systems that clone the repo ("headless" clones)
- **30-Day Evaluation Period**: Added 30-day evaluation period notice to banner and commercial license
- **Implementation**:
  - Created `TRAINING/common/license_banner.py` with `print_license_banner()` and `print_license_banner_once()` functions
  - Banner prints on startup of all main entry points
  - Uses logger if available, otherwise prints to stderr
  - Can be suppressed via `FOXML_SUPPRESS_BANNER` environment variable
- **Content**:
  - License type (AGPL-3.0 for personal/academic use)
  - Commercial use requirements (business use requires commercial license)
  - **30-Day Evaluation Period**: Commercial organizations may evaluate for 30 days for testing/evaluation purposes
  - Links to legal documentation (`LEGAL/SUBSCRIPTIONS.md`, `LEGAL/LICENSING.md`, `LEGAL/COMMERCIAL_USE.md`, `COMMERCIAL_LICENSE.md`)
  - Contact email for enterprise licensing (`jenn.lewis5789@gmail.com`)
- **Legal Updates**:
  - `COMMERCIAL_LICENSE.md` Section 2: Added "Evaluation License (30-Day Trial Period)" clause
  - `LEGAL/SUBSCRIPTIONS.md`: Updated to reflect 30-day commercial evaluation period
  - Clarified that evaluation period is for testing/evaluation only, production use requires paid license
- **Integration Points**:
  - `TRAINING/orchestration/intelligent_trainer.py` (main entry point)
  - `TRAINING/ranking/multi_model_feature_selection.py` (feature selection CLI)
  - `TRAINING/training_strategies/main.py` (training strategies main)
- **Benefits**:
  - **Removes friction**: Allows compliance officers to approve evaluation without immediate license commitment
  - Ensures compliance teams see licensing information in logs (indexed in Splunk/Datadog)
  - Helps capture value from automated systems (1,250+ clones vs 43 visitors)
  - Professional, non-intrusive banner with clear links to legal docs
  - Appears in terminal output and logs, ensuring visibility even in headless environments
  - **Sales funnel**: Organizations can "get hooked" legally before committing to paid license
- **Files**:
  - `TRAINING/common/license_banner.py` (new utility module)
  - `COMMERCIAL_LICENSE.md` (added evaluation period clause)
  - `LEGAL/SUBSCRIPTIONS.md` (updated evaluation period language)

### GPU Acceleration for Target Ranking and Feature Selection
- **Enhancement**: Added GPU acceleration support for XGBoost, CatBoost, and LightGBM in target ranking and feature selection
- **SST Compliance**: All GPU detection parameters now use `get_cfg()` directly (not `.get()` with hardcoded defaults)
- **Configuration**: All GPU settings in `CONFIG/training_config/gpu_config.yaml` (Single Source of Truth)
- **Detection**: Automatic GPU detection with test models to verify availability
- **Fallback**: Graceful fallback to CPU if GPU unavailable (with clear error messages)
- **Logging**: Always logs GPU status (not just when `gpu_detail` enabled) for easier debugging
- **XGBoost 3.1+ Compatibility** (Fixed 2025-12-12): Removed `gpu_id` parameter (removed in XGBoost 3.1+), now uses `device='cuda'` with `tree_method='hist'`. Automatic fallback to legacy API for older XGBoost versions
- **CatBoost GPU Verification** (Fixed 2025-12-12): Added explicit verification that `task_type='GPU'` is set (CatBoost requires this to use GPU). Added pre-instantiation and post-instantiation checks, enhanced logging with educational hints about CatBoost quantization behavior
- **Target Ranking** (`TRAINING/ranking/predictability/model_evaluation.py`):
  - **XGBoost**: Reads `gpu_config.yaml`, sets `tree_method='hist'` and `device='cuda'`
  - **CatBoost**: Reads `gpu_config.yaml`, sets `task_type='GPU'` and `devices` from config
  - **LightGBM**: Already had GPU support (try CUDA, fallback to OpenCL, then CPU)
- **Feature Selection** (`TRAINING/ranking/multi_model_feature_selection.py`):
  - **LightGBM**: Try CUDA first, fallback to OpenCL, then CPU
  - **XGBoost**: Reads `gpu_config.yaml`, sets `tree_method='hist'` and `device='cuda'`
  - **CatBoost**: Reads `gpu_config.yaml`, sets `task_type='GPU'` and `devices` from config
- **Implementation**:
  - GPU detection with test models to verify availability
  - Graceful fallback to CPU if GPU not available
  - All GPU settings read from `CONFIG/training_config/gpu_config.yaml` (SST)
  - GPU params override model config values when GPU is available
  - Logging controlled by `log_cfg.gpu_detail` and `log_cfg.edu_hints`
- **Configuration**:
  - `gpu.xgboost.device`: "cuda" or "cpu"
  - `gpu.xgboost.tree_method`: "hist" (for GPU)
  - `gpu.xgboost.gpu_id`: GPU device ID
  - `gpu.cuda_visible_devices`: For CatBoost devices parameter
- **Benefits**: Significantly faster target ranking and feature selection on large datasets when GPU is available
- **Files**:
  - `TRAINING/ranking/predictability/model_evaluation.py` (XGBoost: lines 1613-1650, CatBoost: lines 1745-1800)
  - `TRAINING/ranking/multi_model_feature_selection.py` (LightGBM: lines 926-961, XGBoost: lines 974-1028, CatBoost: lines 1080-1150)

### Active Sanitization (Ghost Buster)

**Proactive Feature Quarantine System**
- **Enhancement**: Implemented active sanitization to automatically quarantine features with excessive lookback before training starts
- **Purpose**: Prevents "ghost feature" discrepancies where audit and auto-fix see different lookback values
- **Problem Solved**: Previously, daily/24h features (1440m lookback) caused conflicts:
  - Audit enforcer detected 1440m (from daily feature)
  - Auto-fix resolver calculated 1000m (from sma_200)
  - Result: `AUDIT VIOLATION: purge_minutes (1010) < feature_lookback_max_minutes (1440)`
- **Solution**: Active sanitization quarantines problematic features BEFORE lookback computation, ensuring both audit and auto-fix see the same values

**Implementation:**
- **New Module**: `TRAINING/utils/feature_sanitizer.py`
  - `auto_quarantine_long_lookback_features()`: Scans features and quarantines those with excessive lookback
  - `quarantine_by_pattern()`: Optional pattern-based quarantine (more aggressive)
- **Integration**: Integrated into `filter_features_for_target()` in `TRAINING/utils/leakage_filtering.py`
  - Runs after all other filtering (registry, patterns, etc.)
  - Automatically quarantines problematic features before training starts
- **Configuration**: `CONFIG/training_config/safety_config.yaml`
  ```yaml
  active_sanitization:
    enabled: true  # Enable active sanitization
    max_safe_lookback_minutes: 240.0  # Default: 4 hours
    pattern_quarantine:
      enabled: false  # Optional pattern-based quarantine
      patterns: []
  ```

**How It Works:**
1. After all other filtering, sanitizer scans remaining features
2. Computes lookback for each feature using same logic as auto-fix
3. Quarantines features with lookback > `max_safe_lookback_minutes` (default: 240m)
4. Logs what was quarantined and why
5. Returns only safe features for training

**Expected Behavior:**
- Daily/24h features (1440m lookback) quarantined before lookback computation
- Auto-fix sees 1000m (from sma_200) instead of 1440m
- Audit sees 1000m (same as auto-fix)
- Purge auto-adjusts to 1010m (1000 * 1.01)
- No more "ghost feature" discrepancies

**Files:**

## Fixed (2025-12-12)

### GPU Acceleration SST Compliance
- **Enhancement**: Made all GPU detection parameters fully config-driven (SST)
- **Before**: Used `.get()` with hardcoded defaults (e.g., `gpu_cfg.get('device', 'cpu')`)
- **After**: Use `get_cfg()` directly (e.g., `get_cfg('gpu.xgboost.device', default='cpu', config_name='gpu_config')`)
- **Impact**: All GPU settings now come from `gpu_config.yaml` - no hardcoded values in code
- **Files**:
  - `TRAINING/ranking/predictability/model_evaluation.py` (LightGBM, XGBoost, CatBoost)
  - `TRAINING/ranking/multi_model_feature_selection.py` (LightGBM, XGBoost, CatBoost)
- **Configuration**: All values in `CONFIG/training_config/gpu_config.yaml`

### Reproducibility Tracking Bug Fixes

**Critical Bug Fixes**
- **Fixed `ctx` NameError in reproducibility tracker** (`TRAINING/utils/reproducibility_tracker.py:862`)
  - **Issue**: `NameError: name 'ctx' is not defined` when saving metadata for TARGET_RANKING
  - **Impact**: Prevented `metadata.json` and `metrics.json` from being written (86 failures in `stats.json`)
  - **Fix**: Changed to use `additional_data.get('view')` instead of `getattr(ctx, 'view', None)`
  - **Result**: Metadata files now write correctly for all runs

- **Fixed feature importances save path** (`TRAINING/ranking/predictability/model_evaluation.py`)
  - **Issue**: Feature importances saved under `CROSS_SECTIONAL` even for `SYMBOL_SPECIFIC` runs
  - **Fix**: Added `view` parameter to `_save_feature_importances()` and updated directory structure
  - **Result**: Feature importances now saved under correct view directory: `target_rankings/feature_importances/{target}/{view}/{symbol?}/`

- **Fixed perfect CV detection false positive** (`TRAINING/ranking/predictability/model_evaluation.py`)
  - **Issue**: Triggered on training scores (0.9999) instead of actual CV scores (0.687)
  - **Fix**: Changed to use `primary_scores` (from `cross_val_score`) as primary source, with explicit overwrite of training metrics with CV scores
  - **Result**: Perfect CV detection now only triggers on actual validation CV scores â‰¥ 99%

- **Added missing metadata diagnostic** (`TRAINING/utils/reproducibility_tracker.py`)
  - **Enhancement**: Warns when `metadata.json` or `metrics.json` are missing but `audit_report.json` exists
  - **Purpose**: Detects partial writes from previous bugs
  - **Result**: Better visibility into metadata completeness

**Files Modified**:
- `TRAINING/utils/reproducibility_tracker.py` - Fixed `ctx` NameError, added metadata diagnostic
- `TRAINING/ranking/predictability/model_evaluation.py` - Fixed feature importances path, fixed CV score detection, updated `_compute_and_store_metrics()` to store CV scores correctly

**Impact**:
- All new runs will have complete metadata files
- Feature importances saved under correct view directories
- Perfect CV detection now accurate (no false positives)
- Existing runs from before fix are missing metadata (documented in `METADATA_MISSING_README.md`)

### Related

- See Trend Analyzer Verification Guide (if exists)
- See [Cohort-Aware Reproducibility Guide](../../03_technical/implementation/COHORT_AWARE_REPRODUCIBILITY.md)

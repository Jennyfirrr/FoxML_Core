# Changelog — 2025-12-10

**SST Enforcement, Determinism System, Config Centralization**

For a quick overview, see the [root changelog](../../../CHANGELOG.md).  
For other dates, see the [changelog index](README.md).

---

## Added

### SST Enforcement & Configuration Hardening

**SST Enforcement Test**
- **Automated SST compliance test** — `TRAINING/tests/test_no_hardcoded_hparams.py` scans all Python files in TRAINING/ for hardcoded hyperparameters, thresholds, and seeds. Only explicitly marked values (`FALLBACK_DEFAULT_OK`, `DESIGN_CONSTANT_OK`) are allowed. Prevents accidental introduction of hardcoded configuration values.

**Configuration Improvements**
- **Training profiles** — Added `training_profiles` to `CONFIG/training_config/optimizer_config.yaml` with `default`, `debug`, and `throughput_optimized` profiles. Neural network trainers now load `batch_size` and `max_epochs` from active profile.
- **Top fraction configurable** — Added `importance_top_fraction: 0.10` to `CONFIG/feature_selection/multi_model.yaml`. All "top 10%" importance patterns in `model_evaluation.py` and `leakage_detection.py` now use configurable fraction instead of hardcoded 0.1.

**Documentation**
- **Public deterministic training guide** — `DOCS/00_executive/DETERMINISTIC_TRAINING.md` (public-facing guide for buyers/quants).

**Code Changes**
- **All seed fallbacks marked** — Added `FALLBACK_DEFAULT_OK` markers to all seed fallback defaults throughout codebase.
- **Diagnostic models marked** — `n_estimators=1` in diagnostic leakage detection models marked with `DESIGN_CONSTANT_OK`.
- **Batch size configurable** — `neural_network_trainer.py` and `comprehensive_trainer.py` load batch_size from training profiles.
- **Top fraction helper** — Created `_get_importance_top_fraction()` helper in `model_evaluation.py` and `leakage_detection.py`.
- **Fixed indentation errors** — Corrected indentation in top fraction pattern updates.

**Files Modified**
- `CONFIG/feature_selection/multi_model.yaml` — Added `importance_top_fraction`
- `CONFIG/training_config/optimizer_config.yaml` — Added `training_profiles`
- `TRAINING/tests/test_no_hardcoded_hparams.py` — New SST enforcement test
- `TRAINING/model_fun/neural_network_trainer.py` — Load batch_size from config
- `TRAINING/model_fun/comprehensive_trainer.py` — Load batch_size from config
- `TRAINING/ranking/predictability/model_evaluation.py` — Top fraction configurable (11 instances)
- `TRAINING/ranking/predictability/leakage_detection.py` — Top fraction configurable (11 instances)
- All seed fallbacks marked with `FALLBACK_DEFAULT_OK` across 15+ files

---

## Fixed

### Complete Single Source of Truth (SST) Implementation

Replaced ALL hardcoded values across entire TRAINING pipeline for full reproducibility:

- **Model trainers** (`TRAINING/model_fun/` - 34 files): All hardcoded hyperparameters replaced with config loading:
  - `comprehensive_trainer.py`: LightGBM/XGBoost `n_estimators`, `max_depth`, `learning_rate` now load from `models.{family}.{param}` config paths
  - `neural_network_trainer.py`: Adam `learning_rate` now uses `_get_learning_rate()` helper method
  - `ensemble_trainer.py`: Ridge `alpha` now loads from `models.ridge.alpha` config
  - `change_point_trainer.py`: Ridge `alpha` and KMeans `random_state` now load from config
  - `ngboost_trainer.py`: HistGradientBoosting `max_depth` and `learning_rate` now load from config
  - All other trainers already using `_get_test_split_params()` and `_get_random_state()` from base class
- **Specialized models** (`TRAINING/models/specialized/` - 2 files): All hardcoded values replaced:
  - `trainers.py`: All `train_test_split`, `random_state`, `learning_rate`, `alpha`, DecisionTree/RandomForest hyperparameters now load from config
  - `trainers_extended.py`: LightGBM `learning_rate` and `seed`, GradientBoosting hyperparameters, all Adam `learning_rate`, all `train_test_split` calls now use config
- **Base trainer enhancements**: Added `_get_learning_rate()` helper method to `base_trainer.py` for consistent optimizer config access. Method loads from `optimizer.learning_rate` config with model-family-specific fallback support.
- **Strategies**: RandomForest fallback `n_estimators` in `cascade.py` and `single_task.py` now loads from `models.random_forest.n_estimators` config
- **Config sources used**:
  - `preprocessing.validation.test_size` - For all train/test splits
  - `BASE_SEED` (determinism system) - For all random_state values (with config fallback)
  - `models.{family}.{param}` - For model-specific hyperparameters (n_estimators, max_depth, learning_rate, alpha, etc.)
  - `optimizer.learning_rate` - For neural network optimizers
- **Result**: Same config file → identical results across all pipeline stages. Full reproducibility guaranteed. Zero hardcoded config values remain in the TRAINING pipeline.

### Feature Selection Pipeline Fixes

- **Boruta `X_clean` error** — Fixed `NameError: name 'X_clean' is not defined` in Boruta feature selection. Now correctly uses `X_dense` and `y` from `make_sklearn_dense_X()` sanitization.
- **Interval detection "Nonem" warning** — Fixed logging issue where interval detection fallback showed "Using default: Nonem" instead of actual default value. Now properly passes default parameter through call chain.
- **Boruta double-counting** — Fixed issue where Boruta was contributing to both base consensus (as a model family) and gatekeeper modifier. Now excluded from base consensus, only applied as gatekeeper modifier.
- **Boruta feature count mismatch** — Fixed `ValueError: X has N features, but ExtraTreesClassifier is expecting M features` error. Boruta now uses `train_score = math.nan` (not `0.0`) to indicate "not applicable" since it's a selector, not a predictor. Added NaN handling in logging and checkpoint serialization.
- **Config hardcoded values** — Moved all hardcoded config values (RFE estimator params, Boruta hyperparams, Stability Selection params, Boruta gatekeeper thresholds) to YAML config files for full configurability.

### Logging Configuration

- Fixed method name mismatch in `logging_config_utils.py` (`get_backend_logging_config` now correctly calls `get_backend_config`)
- Fixed logger initialization order in `rank_target_predictability.py` (config import before logger usage)

### Ranking Pipeline

- Fixed `_perfect_correlation_models` NameError in target ranking
- Fixed insufficient features handling (now properly filters targets with <2 features)
- Fixed early exit logic when leakage detected (removed false positive triggers)
- Improved error messages when no targets selected after ranking
- **Fixed progress logging denominator** - Now correctly shows `[1/23]` instead of `[1/63]` when using `--max-targets-to-evaluate`
- **Interval detection warnings in ranking** — Fixed spurious interval auto-detection warnings by wiring `explicit_interval` through entire ranking call chain. All `detect_interval_from_dataframe()` calls now respect `data.bar_interval` from experiment config.
- **Interval detection warnings in feature selection** — Fixed interval auto-detection warnings in feature selection path by extracting `explicit_interval` from `experiment_config.data.bar_interval` and passing it through `intelligent_trainer.py` → `feature_selector.py` → `multi_model_feature_selection.py` → `detect_interval_from_dataframe()`. Eliminates spurious "Timestamp delta doesn't map to reasonable interval" warnings when `data.bar_interval` is configured in experiment config. See `TRAINING/orchestration/intelligent_trainer.py` lines 454-477.
- **Ranking cache JSON serialization** — Fixed `TypeError: Object of type Timestamp is not JSON serializable` when saving ranking cache. Added `_json_default()` serializer function that handles pandas Timestamp (via `isoformat()`), numpy scalars (int/float conversion), numpy arrays (tolist()), and datetime objects. Updated `_save_cached_rankings()` to use `default=_json_default` in `json.dump()` call. See `TRAINING/orchestration/intelligent_trainer.py` lines 107-137, 240.
- **CatBoost loss function for classification** — Fixed CatBoost using `RMSE` loss for binary classification targets. Now auto-detects target type and sets `loss_function` appropriately (`Logloss` for binary, `MultiClass` for multiclass, `RMSE` for regression).
- **Sklearn NaN/dtype handling in ranking** — Replaced ad-hoc `SimpleImputer` usage with shared `make_sklearn_dense_X()` helper for all sklearn-based models. Ensures consistent preprocessing across ranking and feature selection pipelines.
- **Shared target type detection** — Created `TRAINING/utils/target_utils.py` with reusable helpers used consistently across ranking and feature selection.

### Path Resolution & Imports

- **Fixed inconsistent repo root calculations** - `feature_selector.py` and `target_ranker.py` now use `parents[2]` consistently
- **Auto-fixer import path** — fixed `parents[3]` to `parents[2]` in `leakage_auto_fixer.py` for correct repo root detection
- **Leakage filtering path resolution** — fixed config path lookup in `leakage_filtering.py` when moved to `TRAINING/utils/`
- **Path resolution in moved files** — corrected `parents[2]` vs `parents[3]` for repo root detection
- **Import paths after module migration** — all `scripts.utils.*` imports updated to `TRAINING.utils.*`

### Auto-Fixer

- **Auto-fixer training accuracy detection** — now passes actual training accuracy (from `model_metrics`) instead of CV scores to auto-fixer
- **Auto-fixer pattern-based fallback** — added fallback detection when `model_importance` is missing
- **Auto-fixer pre-excluded feature check** — now filters out already-excluded features before detection to prevent redundant exclusions
- **Hardcoded safety net in leakage filtering** — added fallback patterns to exclude known leaky features even when config fails to load

### GPU & Model Issues

- VAE serialization issues — custom Keras layers now properly imported before deserialization
- Sequential models 3D preprocessing issues — input shape handling corrected
- XGBoost source-build stability — persistent build directory and non-editable install
- Readline symbol lookup errors — environment variable fixes
- TensorFlow GPU initialization — CUDA library path resolution
- Type conversion issues in callback configs (min_lr, factor, patience, etc.)
- LSTM timeout issues — dynamic batch and epoch scaling implemented
- Transformer OOM errors — reduced batch size and attention heads, dynamic scaling
- CNN1D, LSTM, Transformer input shape mismatches — 3D to 2D reshape fixes
- **LightGBM GPU verbose parameter** — moved `verbose` from `fit()` to model constructor (LightGBM API requirement)

### Documentation

- Comprehensive documentation updates — Updated all documentation with cross-links and new content
- Added `RANKING_SELECTION_CONSISTENCY.md` guide explaining unified pipeline behavior
- Updated all configuration docs to include `logging_config.yaml` references
- Added comprehensive cross-links throughout all documentation files
- Updated main `INDEX.md` with new files and references
- Fixed broken references to non-existent files
- Created `CROSS_REFERENCES.md` tracking document
- All API references now include utility modules (`target_utils.py`, `sklearn_safe.py`)
- All training tutorials now reference unified pipeline behavior
- Configuration examples updated with interval config and CatBoost examples

---

## Related

- [Changelog Index](README.md)
- [Root Changelog](../../../CHANGELOG.md)
- [2025-12-11](2025-12-11.md)
- [General](general.md)
